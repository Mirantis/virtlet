{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Virtlet is a Kubernetes runtime server which allows you to run VM workloads on your cluster. It can run arbitrary QCOW2 images, while making the VMs appear as pod-like as possible. This means the possibility of using most of standard kubectl commands, building higher-level Kubernetes objects such as StatefulSets or Deployments out of the VMs and so on. On the other hand, it's also suitable for running the traditional long-lived \"pet\" VMs, too. Virtlet has full support for Kubernetes networking and multiple CNI implementations, such as Calico, Weave and Flannel. In addition to this, more advanced CNI setups are supported, too, such as SR-IOV and using multiple CNI implementations at the same time. You can a Virtlet usage demo by following this link .","title":"Home"},{"location":"#introduction","text":"Virtlet is a Kubernetes runtime server which allows you to run VM workloads on your cluster. It can run arbitrary QCOW2 images, while making the VMs appear as pod-like as possible. This means the possibility of using most of standard kubectl commands, building higher-level Kubernetes objects such as StatefulSets or Deployments out of the VMs and so on. On the other hand, it's also suitable for running the traditional long-lived \"pet\" VMs, too. Virtlet has full support for Kubernetes networking and multiple CNI implementations, such as Calico, Weave and Flannel. In addition to this, more advanced CNI setups are supported, too, such as SR-IOV and using multiple CNI implementations at the same time. You can a Virtlet usage demo by following this link .","title":"Introduction"},{"location":"101/architecture/","text":"Virtlet architecture See the detailed architecture description in Virtlet docs","title":"Virtlet architecture"},{"location":"101/architecture/#virtlet-architecture","text":"See the detailed architecture description in Virtlet docs","title":"Virtlet architecture"},{"location":"101/cloud-init/","text":"Cloud-Init support Virtlet is using Cloud-Init to configure VMs. As always you can check Virtlet Cloud-Init documentation to see exactly how Virtlet uses it. Cloud-Init data is passed to the VM using Pod's annotations. The most common use case is to pass SSH public key to the VM: VirtletSSHKeys: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost or it can be pulled from Secret : VirtletSSHKeySource: secret/mysecret It's also possible to create a new user by defining user data: VirtletCloudInitUserData: | ssh_pwauth: True users: - name: testuser gecos: User primary-group: testuser groups: users lock_passwd: false shell: /bin/bash # the password is testuser passwd: $6$rounds=4096$wPs4Hz4tfs$a8ssMnlvH.3GX88yxXKF2cKMlVULsnydoOKgkuStTErTq2dzKZiIx9R/pPWWh5JLxzoZEx7lsSX5T2jW5WISi1 sudo: ALL=(ALL) NOPASSWD:ALL ssh-authorized-keys: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost It's also possible to use ConfigMap as a source for user data: VirtletCloudInitUserDataSource: configmap/vm-user-data When you are passing environment variables to a Pod Virtlet uses Cloud-Init to pass it to a VM and store them in a /etc/cloud/environment file. When you are using ConfigMap or Secret in a Pod then they are passed to the VM using Cloud-Init by creating new files there. Pod's volumes are also converted to mounts and mounted in VM using cloud-init when listed in the volumeMounts field. See examples/k8s.yaml where VirtletCloudInitUserData is used to do some advanced scripting there.","title":"Cloud-Init support"},{"location":"101/cloud-init/#cloud-init-support","text":"Virtlet is using Cloud-Init to configure VMs. As always you can check Virtlet Cloud-Init documentation to see exactly how Virtlet uses it. Cloud-Init data is passed to the VM using Pod's annotations. The most common use case is to pass SSH public key to the VM: VirtletSSHKeys: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost or it can be pulled from Secret : VirtletSSHKeySource: secret/mysecret It's also possible to create a new user by defining user data: VirtletCloudInitUserData: | ssh_pwauth: True users: - name: testuser gecos: User primary-group: testuser groups: users lock_passwd: false shell: /bin/bash # the password is testuser passwd: $6$rounds=4096$wPs4Hz4tfs$a8ssMnlvH.3GX88yxXKF2cKMlVULsnydoOKgkuStTErTq2dzKZiIx9R/pPWWh5JLxzoZEx7lsSX5T2jW5WISi1 sudo: ALL=(ALL) NOPASSWD:ALL ssh-authorized-keys: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost It's also possible to use ConfigMap as a source for user data: VirtletCloudInitUserDataSource: configmap/vm-user-data When you are passing environment variables to a Pod Virtlet uses Cloud-Init to pass it to a VM and store them in a /etc/cloud/environment file. When you are using ConfigMap or Secret in a Pod then they are passed to the VM using Cloud-Init by creating new files there. Pod's volumes are also converted to mounts and mounted in VM using cloud-init when listed in the volumeMounts field. See examples/k8s.yaml where VirtletCloudInitUserData is used to do some advanced scripting there.","title":"Cloud-Init support"},{"location":"101/creating-and-managing-pods/","text":"Creating and managing virtual machines The demo.sh script which you've run above has set up Virtlet, started virtual machine with cirros image, started a nginx pod which is exposed via Service. Let's check how it all works. The script will ssh into a cirros-vm before finishing. Switch to another console and list the pods: kubectl get pod cirros-vm and nginx pod are both listed. Now list the services: kubectl get svc Now go back to the first console where you are logged in the cirros-vm . If you disconnected from it you can reconnect with virtletctl ssh cirros@cirros-vm -- -i virtlet/examples/vmkey . Check that you have internet access: ping 8.8.8.8 ping mirantis.com Make sure that the cluster network is also accessible from within the pod. Retrieve some data: curl nginx Disconnect from the VM using Ctrl-D. View Pod details Use the kubectl get and kubectl describe commands to view details for the cirros-vm pod: View the logs of a Pod Use the kubectl logs command to view the logs for the cirros-vm pod: kubectl logs cirros-vm Attach to the VM Use the kubectl attach command to attach to the VM console: kubectl attach -it cirros-vm To exit use CTRL-] key combination. Create new Ubuntu VM Now you can create another virtual machine. Let's create Ubuntu VM from Virtlet examples: cat examples/ubuntu-vm-with-testuser.yaml kubectl apply -f examples/ubuntu-vm-with-testuser.yaml This example also shows how cloud-init script can be used. Here new user: testuser is created and also an ssh key is injected. Wait for ubuntu VM: kubectl get pod -w Now attach to the VM using testuser/testuser credentials: kubectl attach -it ubuntu-vm-with-testuser Check if it also has internet connection using the same method which was used for cirros-vm above. Use in Deployment Because Virtlet VMs are just normal pods you can create Deployment, DaemonSet or even StatefulSet from Virtual Machines: cat examples/deploy-cirros-vm.yaml kubectl apply -f examples/cirros-vm-deployment.yaml When it's ready you can scale it: kubectl scale --replicas=2 deploy/cirros-deployment","title":"Creating and managing virtual machines"},{"location":"101/creating-and-managing-pods/#creating-and-managing-virtual-machines","text":"The demo.sh script which you've run above has set up Virtlet, started virtual machine with cirros image, started a nginx pod which is exposed via Service. Let's check how it all works. The script will ssh into a cirros-vm before finishing. Switch to another console and list the pods: kubectl get pod cirros-vm and nginx pod are both listed. Now list the services: kubectl get svc Now go back to the first console where you are logged in the cirros-vm . If you disconnected from it you can reconnect with virtletctl ssh cirros@cirros-vm -- -i virtlet/examples/vmkey . Check that you have internet access: ping 8.8.8.8 ping mirantis.com Make sure that the cluster network is also accessible from within the pod. Retrieve some data: curl nginx Disconnect from the VM using Ctrl-D.","title":"Creating and managing virtual machines"},{"location":"101/creating-and-managing-pods/#view-pod-details","text":"Use the kubectl get and kubectl describe commands to view details for the cirros-vm pod:","title":"View Pod details"},{"location":"101/creating-and-managing-pods/#view-the-logs-of-a-pod","text":"Use the kubectl logs command to view the logs for the cirros-vm pod: kubectl logs cirros-vm","title":"View the logs of a Pod"},{"location":"101/creating-and-managing-pods/#attach-to-the-vm","text":"Use the kubectl attach command to attach to the VM console: kubectl attach -it cirros-vm To exit use CTRL-] key combination.","title":"Attach to the VM"},{"location":"101/creating-and-managing-pods/#create-new-ubuntu-vm","text":"Now you can create another virtual machine. Let's create Ubuntu VM from Virtlet examples: cat examples/ubuntu-vm-with-testuser.yaml kubectl apply -f examples/ubuntu-vm-with-testuser.yaml This example also shows how cloud-init script can be used. Here new user: testuser is created and also an ssh key is injected. Wait for ubuntu VM: kubectl get pod -w Now attach to the VM using testuser/testuser credentials: kubectl attach -it ubuntu-vm-with-testuser Check if it also has internet connection using the same method which was used for cirros-vm above.","title":"Create new Ubuntu VM"},{"location":"101/creating-and-managing-pods/#use-in-deployment","text":"Because Virtlet VMs are just normal pods you can create Deployment, DaemonSet or even StatefulSet from Virtual Machines: cat examples/deploy-cirros-vm.yaml kubectl apply -f examples/cirros-vm-deployment.yaml When it's ready you can scale it: kubectl scale --replicas=2 deploy/cirros-deployment","title":"Use in Deployment"},{"location":"101/network/","text":"Virtlet networking Virtlet Networking docs One interface Virtlet, as other CRI implementations, uses CNI (Container Network Interface). When only one interface is used then no additional configuration is required. As you saw in the previous chapter it just works. Multiple interfaces For multiple interfaces Virtlet uses CNI-Genie . When running demo.sh script you have set MULTI_CNI=1 which tells kubeadm-dind-cluster to install CNI-Genie and two network providers: flannel and calico. Check that all is already installed: kubectl -n kube-system get pod You should see genie, flannel and calico-related pods in the output. See Virtlet docs for more details about multiple interfaces support. demo.sh script configured CNI-Genie to create two network interfaces for each newly created pod. That includes docker pods and Virtlet pods. Check the ubuntu-vm to see that it has already configured two interfaces. Attach to the VM (log in as testuser/testuser): kubectl attach -it ubuntu-vm-with-testuser and list addresses on interfaces: ip address See how to specify which networks to use: cat examples/ubuntu-multi-cni.yaml See cni: \"calico,flannel\" annotation. It tells CNI-Genie which networks to use. If no cni annotation is specified then default plugin will be used. In demo.sh script default is set also to \"calico,flannel\": docker exec kube-node-1 cat /etc/cni/net.d/00-genie.conf","title":"Virtlet networking"},{"location":"101/network/#virtlet-networking","text":"Virtlet Networking docs","title":"Virtlet networking"},{"location":"101/network/#one-interface","text":"Virtlet, as other CRI implementations, uses CNI (Container Network Interface). When only one interface is used then no additional configuration is required. As you saw in the previous chapter it just works.","title":"One interface"},{"location":"101/network/#multiple-interfaces","text":"For multiple interfaces Virtlet uses CNI-Genie . When running demo.sh script you have set MULTI_CNI=1 which tells kubeadm-dind-cluster to install CNI-Genie and two network providers: flannel and calico. Check that all is already installed: kubectl -n kube-system get pod You should see genie, flannel and calico-related pods in the output. See Virtlet docs for more details about multiple interfaces support. demo.sh script configured CNI-Genie to create two network interfaces for each newly created pod. That includes docker pods and Virtlet pods. Check the ubuntu-vm to see that it has already configured two interfaces. Attach to the VM (log in as testuser/testuser): kubectl attach -it ubuntu-vm-with-testuser and list addresses on interfaces: ip address See how to specify which networks to use: cat examples/ubuntu-multi-cni.yaml See cni: \"calico,flannel\" annotation. It tells CNI-Genie which networks to use. If no cni annotation is specified then default plugin will be used. In demo.sh script default is set also to \"calico,flannel\": docker exec kube-node-1 cat /etc/cni/net.d/00-genie.conf","title":"Multiple interfaces"},{"location":"101/provision-virtlet/","text":"Virtlet provisioning Using MCP Follow the latest instructions how to deploy Kubernetes cluster and then deploy Virtlet Deploy Virtlet on MCP Using kubeadm-dind-cluster integration For this workshop you will use kubeadm-dind-cluster cluster. Use demo.sh script which will deploy Kubernetes and provision Virtlet there. MULTI_CNI=1 ./deploy/demo.sh Answer y to the script\u2019s questions and wait until the script completes. The script will create a CirrOS VM for you and display its shell prompt. Other deployment options To see how to provision Virtlet on non MCP clusters see the deployment docs","title":"Virtlet provisioning"},{"location":"101/provision-virtlet/#virtlet-provisioning","text":"","title":"Virtlet provisioning"},{"location":"101/provision-virtlet/#using-mcp","text":"Follow the latest instructions how to deploy Kubernetes cluster and then deploy Virtlet Deploy Virtlet on MCP","title":"Using MCP"},{"location":"101/provision-virtlet/#using-kubeadm-dind-cluster-integration","text":"For this workshop you will use kubeadm-dind-cluster cluster. Use demo.sh script which will deploy Kubernetes and provision Virtlet there. MULTI_CNI=1 ./deploy/demo.sh Answer y to the script\u2019s questions and wait until the script completes. The script will create a CirrOS VM for you and display its shell prompt.","title":"Using kubeadm-dind-cluster integration"},{"location":"101/provision-virtlet/#other-deployment-options","text":"To see how to provision Virtlet on non MCP clusters see the deployment docs","title":"Other deployment options"},{"location":"101/rolling-out-updates/","text":"Rolling out updates Virtlet Updates To update Virtlet to the newer or older version just change its image name/tag in DaemonSet definition: kubectl -n kube-system edit ds virtlet You should change it in four places there, for each container definition. During the upgrade state of VM pods will change to ContainerCreating but there is nothing to worry about. The VMs are still running. Kubelet just can't get the state of the VMs because Virtlet is not available during the update. When Virtlet update is done you can check that the VMs are still running. Attach to cirros-vm : kubectl attach -it cirros-vm and run uptime command. Virtlet updates are done using standard DaemonSet update mechanism","title":"Rolling out updates"},{"location":"101/rolling-out-updates/#rolling-out-updates","text":"","title":"Rolling out updates"},{"location":"101/rolling-out-updates/#virtlet-updates","text":"To update Virtlet to the newer or older version just change its image name/tag in DaemonSet definition: kubectl -n kube-system edit ds virtlet You should change it in four places there, for each container definition. During the upgrade state of VM pods will change to ContainerCreating but there is nothing to worry about. The VMs are still running. Kubelet just can't get the state of the VMs because Virtlet is not available during the update. When Virtlet update is done you can check that the VMs are still running. Attach to cirros-vm : kubectl attach -it cirros-vm and run uptime command. Virtlet updates are done using standard DaemonSet update mechanism","title":"Virtlet Updates"},{"location":"101/troubleshooting/","text":"Troubleshooting When having problems with deploying Virtual Machine the firs step is to troubleshoot it as normal Pods. If it doesn't help then you need to check Virtlet logs. You can do it manually or by using virtletctl diag command: mkdir dump_data virtletctl diag dump dump_data/ It will download all required logs and statuses from all nodes where Virtlet is installed: ls dump_data/nodes For more info, see Diagnostics in Virtlet Reference.","title":"Troubleshooting"},{"location":"101/troubleshooting/#troubleshooting","text":"When having problems with deploying Virtual Machine the firs step is to troubleshoot it as normal Pods. If it doesn't help then you need to check Virtlet logs. You can do it manually or by using virtletctl diag command: mkdir dump_data virtletctl diag dump dump_data/ It will download all required logs and statuses from all nodes where Virtlet is installed: ls dump_data/nodes For more info, see Diagnostics in Virtlet Reference.","title":"Troubleshooting"},{"location":"101/volumes/","text":"Volumes Volumes Documentation Virtlet supports Kubernetes Volumes in a several ways: Directory volume with 9pfs It allows Virtlet to use for example emptyDir or hostPath . It is a network protocol used over a virtual pci device (does not use networking stack) so when comparing to other options it may have worse performance. Persistent Block Volumes Virtlet can attach local block volume. Ceph based volumes can also be used. Persistent Rootfs Virtlet also supports booting VM from a Persisten Block Volume. FlexVolumes It\u2019s possible to use Virtlet\u2019s flexvolume driver to specify mounting of local block devices, \u201cephemeral volumes\u201d with their lifetime bound to the one of the pod, and Ceph volumes that are specified as block devices. See how to use flexvolume with VM Pod: cat examples/ubuntu-vm-with-volume.yaml kubectl create -f examples/ubuntu-vm-with-volume.yaml There are also plans to work on CSI","title":"Virtual machine volumes"},{"location":"101/volumes/#volumes","text":"Volumes Documentation Virtlet supports Kubernetes Volumes in a several ways:","title":"Volumes"},{"location":"101/volumes/#directory-volume-with-9pfs","text":"It allows Virtlet to use for example emptyDir or hostPath . It is a network protocol used over a virtual pci device (does not use networking stack) so when comparing to other options it may have worse performance.","title":"Directory volume with 9pfs"},{"location":"101/volumes/#persistent-block-volumes","text":"Virtlet can attach local block volume. Ceph based volumes can also be used.","title":"Persistent Block Volumes"},{"location":"101/volumes/#persistent-rootfs","text":"Virtlet also supports booting VM from a Persisten Block Volume.","title":"Persistent Rootfs"},{"location":"101/volumes/#flexvolumes","text":"It\u2019s possible to use Virtlet\u2019s flexvolume driver to specify mounting of local block devices, \u201cephemeral volumes\u201d with their lifetime bound to the one of the pod, and Ceph volumes that are specified as block devices. See how to use flexvolume with VM Pod: cat examples/ubuntu-vm-with-volume.yaml kubectl create -f examples/ubuntu-vm-with-volume.yaml There are also plans to work on CSI","title":"FlexVolumes"},{"location":"101/workshop-setup/","text":"Workshop setup Clone repository In your shell clone the Virtlet repository and download virtletctl binary: git clone https://github.com/Mirantis/virtlet.git chmod 600 virtlet/examples/vmkey wget https://github.com/Mirantis/virtlet/releases/download/v1.4.1/virtletctl chmod +x virtletctl wget https://storage.googleapis.com/kubernetes-release/release/v1.12.3/bin/linux/amd64/kubectl chmod +x kubectl mkdir -p ~/bin mv virtletctl ~/bin mv kubectl ~/bin cd virtlet","title":"Workshop setup"},{"location":"101/workshop-setup/#workshop-setup","text":"","title":"Workshop setup"},{"location":"101/workshop-setup/#clone-repository","text":"In your shell clone the Virtlet repository and download virtletctl binary: git clone https://github.com/Mirantis/virtlet.git chmod 600 virtlet/examples/vmkey wget https://github.com/Mirantis/virtlet/releases/download/v1.4.1/virtletctl chmod +x virtletctl wget https://storage.googleapis.com/kubernetes-release/release/v1.12.3/bin/linux/amd64/kubectl chmod +x kubectl mkdir -p ~/bin mv virtletctl ~/bin mv kubectl ~/bin cd virtlet","title":"Clone repository"},{"location":"dev/architecture/","text":"Overview The following figure provides a general view of Virtlet architecture: Virtlet consists of the following components: Virtlet manager which implements CRI interface for virtualization and image handling libvirt instance vmwrapper which is responsible for setting up the environment for emulator the emulator, currently qemu with KVM support (with a possibility to disable KVM) In addition to the above, our example setup uses CRI proxy which provides the possibility to mix dockershim and VM based workloads on the same k8s node. Virtlet manager The main binary is responsible for providing API fullfiling CRI specification . It serves the requests from kubelet by doing the following: setting up libvirt VM environment (virtual drives, network interfaces, trimming resources like RAM, CPU) performing CNI setup telling libvirt to call vmwrapper instead of using emulator directly querying libvirt for VM statuses instructing libvirt to stop VMs performing CNI teardown and finally calling libvirt to tear down VM environment. vmwrapper vmrapper is run by libvirt and wraps the emulator (QEMU/KVM). It requests tap file descriptor from Virtlet, adds command line arguments needed by the emulator to use the tap device and then exec s the emulator. CRI Proxy CRI Proxy is an external project provides a way to run multiple CRI implementations on the same node, e.g. Virtlet and dockershim. This is handy for running infrastructure pods such as kube-proxy. It's possible to reuse dockershim component from kubelet to make it possible to have Docker as one of CRI implementations on the multi-runtime nodes.","title":"Virtlet architecture"},{"location":"dev/architecture/#overview","text":"The following figure provides a general view of Virtlet architecture: Virtlet consists of the following components: Virtlet manager which implements CRI interface for virtualization and image handling libvirt instance vmwrapper which is responsible for setting up the environment for emulator the emulator, currently qemu with KVM support (with a possibility to disable KVM) In addition to the above, our example setup uses CRI proxy which provides the possibility to mix dockershim and VM based workloads on the same k8s node.","title":"Overview"},{"location":"dev/architecture/#virtlet-manager","text":"The main binary is responsible for providing API fullfiling CRI specification . It serves the requests from kubelet by doing the following: setting up libvirt VM environment (virtual drives, network interfaces, trimming resources like RAM, CPU) performing CNI setup telling libvirt to call vmwrapper instead of using emulator directly querying libvirt for VM statuses instructing libvirt to stop VMs performing CNI teardown and finally calling libvirt to tear down VM environment.","title":"Virtlet manager"},{"location":"dev/architecture/#vmwrapper","text":"vmrapper is run by libvirt and wraps the emulator (QEMU/KVM). It requests tap file descriptor from Virtlet, adds command line arguments needed by the emulator to use the tap device and then exec s the emulator.","title":"vmwrapper"},{"location":"dev/architecture/#cri-proxy","text":"CRI Proxy is an external project provides a way to run multiple CRI implementations on the same node, e.g. Virtlet and dockershim. This is handy for running infrastructure pods such as kube-proxy. It's possible to reuse dockershim component from kubelet to make it possible to have Docker as one of CRI implementations on the multi-runtime nodes.","title":"CRI Proxy"},{"location":"dev/build-tool/","text":"build/cmd.sh build/cmd.sh script helps automating usage of docker build container using Dockerfile.build . Script usage Currently the script supports the following commands: ./build/cmd.sh build ./build/cmd.sh test ./build/cmd.sh copy ./build/cmd.sh copy-dind ./build/cmd.sh start-dind ./build/cmd.sh vsh ./build/cmd.sh stop ./build/cmd.sh clean ./build/cmd.sh update-bindata ./build/cmd.sh update-generated-docs ./build/cmd.sh gotest [TEST_ARGS...] ./build/cmd.sh gobuild [BUILD_ARGS...] ./build/cmd.sh run CMD... ./build/cmd.sh release TAG ./build/cmd.sh serve-docs ./build/cmd.sh build-docs ./build/cmd.sh sync build , test , integration , run , gobuild , gotest , copy , copy-back and prepare-vendor commands check whether the build container image and data volumes are available before proceeding. They build the image if necessary and create the data volumes if they don't exist, then, on each run they copy the local sources into virtlet_src data volume. The build container used by build/cmd.sh needs to be able to access Docker socket inside it. By default it mounts /var/run/docker.sock inside the container but you can override the path by setting DOCKER_SOCKET_PATH environment variable. build Performs a full build of Virtlet. Also builds mirantis/virtlet:latest image. test Runs the unit tests inside a build container. integration Runs the integration tests. KVM is disabled for these so as to make them run inside limited CI environments that don't support kernel virtualization. You need to invoke build/cmd.sh build before running this command. copy Extracts output binaries from build container into _output/ in the current directory. copy-back Syncs local _output_ contents to the build container. This is used by CI for passing build artifacts between the jobs in a workflow. copy-dind Copies the binaries into kube-node-1 of kubeadm-dind-cluster (or kube-master if VIRTLET_ON_MASTER environment variable is set to a non-empty value). You need to do dind-cluster...sh up to be able to use this command. start-dind Starts Virtlet on kube-node-1 of kubeadm-dind-cluster (or kube-master if VIRTLET_ON_MASTER environment variable is set to a non-empty value). You need to do dind-cluster...sh up and build/cmd.sh copy-dind to be able to use this command. This command copies locally-built mirantis/virtlet image to the DIND node that will run Virtlet if it doesn't exist there or if FORCE_UPDATE_IMAGE is set to a non-empty value. This command requires kubectl . prepare-all-nodes Makes all the worker nodes in the kubeadm-dind-cluster ready for Virtlet pod, but doesn't start Virtlet pod on them (it doesn't apply extraRuntime=virtlet label). This is done automatically for the Virtlet node during start-dind , but it's necessary to do so for all the worker nodes for Virtlet e2e config test to pass. vsh Starts an interactive shell using build container. Useful for debugging. stop Removes the build container. clean Removes the build container and image along with data volumes as well as the binaries in local _output directory. update-bindata Updates generated binary assets. Currently needed if you edit files under deploy/data directory. update-generated-docs Updates generated documentation (source markdown files). This currently includes virtletctl and config documentation. gotest Runs unit tests suite in build container - suitable for use with IDEs/editors. gobuild Runs go build in the build container. This command can be used for syntax checking in IDEs/editors. It assumes that build command was invoked at least once since the last clean . run Runs the specified command inside the build container. This command can be used to debug the build scripts. prepare-vendor Populates vendor/ directory inside the build container. This is used by CI. start-build-container Starts the build container. This is done automatically by other build/cmd.sh commands that need the build container. update-bindata Updates the generated bindata. Run this command if you modify anything under deploy/data . update-generated Updates the generated Go files except for bindata. Run this command if you modify Virtlet CRD structs. e2e Runs Virtlet e2e tests against the currently running DIND cluster. You can also pass test options to this command: build/cmd.sh e2e -test.v You can also pass Ginkgo options to this command, e.g. the following command will run only the tests that have Should have default route in their description: build/cmd.sh e2e -test.v -ginkgo.focus= Should have default route This command requires kubectl . Virtlet e2e tests can produce JUnit-style XML output if asked to do so: build/cmd.sh e2e -test.v -junitOutput /tmp/junit.xml build/cmd.sh run 'cat /tmp/junit.xml' junit.xml serve-docs Starts serving the MkDocs -built documentation on the port specified by MKDOCS_SERVE_ADDRESS environment variable (default 8042). In order to update the docs from the current Virtlet working copy, you need to run build/cmd.sh sync . build-docs Builds the documentation using MkDocs and puts the result into docs branch. The copy of MkDocs output is stored under _docs/ subdirectory of the working copy. The build is done only if the docs changed since they were last build and stored under gh-docs branch or if the current working copy is dirty (i.e. has uncommitted changes). sync Synchronizes the working copy with the build container, starting the build container if it's not present. This command is handy with conjunction with build/cmd.sh . Control flags Some of the script commands also be customized by using environment variables: VIRTLET_SKIP_RSYNC=1 disables syncing of sources in the build docker container. If set, changes made to the sources on the host machine will not be included into the build. VIRTLET_RSYNC_PORT=18730 - port to use for rsync. VIRTLET_ON_MASTER=1 - start virtlet on the master (API) k8s node rather than on the worker. DOCKER_SOCKET_PATH=/var/run/docker.sock - docker Unix socket path to mount into the build container. DOCKER_HOST - docker server to be used in the build server (when it differs from the default Unix socket). FORCE_UPDATE_IMAGE=1 - always propagate virtlet docker image from the host into DinD cluster (by default, it is done when no such image exist). IMAGE_REGEXP_TRANSLATION=1 enables regexp syntax in image name translation rules. See Image Name Translation for more details.","title":"Build Tool"},{"location":"dev/build-tool/#buildcmdsh","text":"build/cmd.sh script helps automating usage of docker build container using Dockerfile.build .","title":"build/cmd.sh"},{"location":"dev/build-tool/#script-usage","text":"Currently the script supports the following commands: ./build/cmd.sh build ./build/cmd.sh test ./build/cmd.sh copy ./build/cmd.sh copy-dind ./build/cmd.sh start-dind ./build/cmd.sh vsh ./build/cmd.sh stop ./build/cmd.sh clean ./build/cmd.sh update-bindata ./build/cmd.sh update-generated-docs ./build/cmd.sh gotest [TEST_ARGS...] ./build/cmd.sh gobuild [BUILD_ARGS...] ./build/cmd.sh run CMD... ./build/cmd.sh release TAG ./build/cmd.sh serve-docs ./build/cmd.sh build-docs ./build/cmd.sh sync build , test , integration , run , gobuild , gotest , copy , copy-back and prepare-vendor commands check whether the build container image and data volumes are available before proceeding. They build the image if necessary and create the data volumes if they don't exist, then, on each run they copy the local sources into virtlet_src data volume. The build container used by build/cmd.sh needs to be able to access Docker socket inside it. By default it mounts /var/run/docker.sock inside the container but you can override the path by setting DOCKER_SOCKET_PATH environment variable.","title":"Script usage"},{"location":"dev/build-tool/#build","text":"Performs a full build of Virtlet. Also builds mirantis/virtlet:latest image.","title":"build"},{"location":"dev/build-tool/#test","text":"Runs the unit tests inside a build container.","title":"test"},{"location":"dev/build-tool/#integration","text":"Runs the integration tests. KVM is disabled for these so as to make them run inside limited CI environments that don't support kernel virtualization. You need to invoke build/cmd.sh build before running this command.","title":"integration"},{"location":"dev/build-tool/#copy","text":"Extracts output binaries from build container into _output/ in the current directory.","title":"copy"},{"location":"dev/build-tool/#copy-back","text":"Syncs local _output_ contents to the build container. This is used by CI for passing build artifacts between the jobs in a workflow.","title":"copy-back"},{"location":"dev/build-tool/#copy-dind","text":"Copies the binaries into kube-node-1 of kubeadm-dind-cluster (or kube-master if VIRTLET_ON_MASTER environment variable is set to a non-empty value). You need to do dind-cluster...sh up to be able to use this command.","title":"copy-dind"},{"location":"dev/build-tool/#start-dind","text":"Starts Virtlet on kube-node-1 of kubeadm-dind-cluster (or kube-master if VIRTLET_ON_MASTER environment variable is set to a non-empty value). You need to do dind-cluster...sh up and build/cmd.sh copy-dind to be able to use this command. This command copies locally-built mirantis/virtlet image to the DIND node that will run Virtlet if it doesn't exist there or if FORCE_UPDATE_IMAGE is set to a non-empty value. This command requires kubectl .","title":"start-dind"},{"location":"dev/build-tool/#prepare-all-nodes","text":"Makes all the worker nodes in the kubeadm-dind-cluster ready for Virtlet pod, but doesn't start Virtlet pod on them (it doesn't apply extraRuntime=virtlet label). This is done automatically for the Virtlet node during start-dind , but it's necessary to do so for all the worker nodes for Virtlet e2e config test to pass.","title":"prepare-all-nodes"},{"location":"dev/build-tool/#vsh","text":"Starts an interactive shell using build container. Useful for debugging.","title":"vsh"},{"location":"dev/build-tool/#stop","text":"Removes the build container.","title":"stop"},{"location":"dev/build-tool/#clean","text":"Removes the build container and image along with data volumes as well as the binaries in local _output directory.","title":"clean"},{"location":"dev/build-tool/#update-bindata","text":"Updates generated binary assets. Currently needed if you edit files under deploy/data directory.","title":"update-bindata"},{"location":"dev/build-tool/#update-generated-docs","text":"Updates generated documentation (source markdown files). This currently includes virtletctl and config documentation.","title":"update-generated-docs"},{"location":"dev/build-tool/#gotest","text":"Runs unit tests suite in build container - suitable for use with IDEs/editors.","title":"gotest"},{"location":"dev/build-tool/#gobuild","text":"Runs go build in the build container. This command can be used for syntax checking in IDEs/editors. It assumes that build command was invoked at least once since the last clean .","title":"gobuild"},{"location":"dev/build-tool/#run","text":"Runs the specified command inside the build container. This command can be used to debug the build scripts.","title":"run"},{"location":"dev/build-tool/#prepare-vendor","text":"Populates vendor/ directory inside the build container. This is used by CI.","title":"prepare-vendor"},{"location":"dev/build-tool/#start-build-container","text":"Starts the build container. This is done automatically by other build/cmd.sh commands that need the build container.","title":"start-build-container"},{"location":"dev/build-tool/#update-bindata_1","text":"Updates the generated bindata. Run this command if you modify anything under deploy/data .","title":"update-bindata"},{"location":"dev/build-tool/#update-generated","text":"Updates the generated Go files except for bindata. Run this command if you modify Virtlet CRD structs.","title":"update-generated"},{"location":"dev/build-tool/#e2e","text":"Runs Virtlet e2e tests against the currently running DIND cluster. You can also pass test options to this command: build/cmd.sh e2e -test.v You can also pass Ginkgo options to this command, e.g. the following command will run only the tests that have Should have default route in their description: build/cmd.sh e2e -test.v -ginkgo.focus= Should have default route This command requires kubectl . Virtlet e2e tests can produce JUnit-style XML output if asked to do so: build/cmd.sh e2e -test.v -junitOutput /tmp/junit.xml build/cmd.sh run 'cat /tmp/junit.xml' junit.xml","title":"e2e"},{"location":"dev/build-tool/#serve-docs","text":"Starts serving the MkDocs -built documentation on the port specified by MKDOCS_SERVE_ADDRESS environment variable (default 8042). In order to update the docs from the current Virtlet working copy, you need to run build/cmd.sh sync .","title":"serve-docs"},{"location":"dev/build-tool/#build-docs","text":"Builds the documentation using MkDocs and puts the result into docs branch. The copy of MkDocs output is stored under _docs/ subdirectory of the working copy. The build is done only if the docs changed since they were last build and stored under gh-docs branch or if the current working copy is dirty (i.e. has uncommitted changes).","title":"build-docs"},{"location":"dev/build-tool/#sync","text":"Synchronizes the working copy with the build container, starting the build container if it's not present. This command is handy with conjunction with build/cmd.sh .","title":"sync"},{"location":"dev/build-tool/#control-flags","text":"Some of the script commands also be customized by using environment variables: VIRTLET_SKIP_RSYNC=1 disables syncing of sources in the build docker container. If set, changes made to the sources on the host machine will not be included into the build. VIRTLET_RSYNC_PORT=18730 - port to use for rsync. VIRTLET_ON_MASTER=1 - start virtlet on the master (API) k8s node rather than on the worker. DOCKER_SOCKET_PATH=/var/run/docker.sock - docker Unix socket path to mount into the build container. DOCKER_HOST - docker server to be used in the build server (when it differs from the default Unix socket). FORCE_UPDATE_IMAGE=1 - always propagate virtlet docker image from the host into DinD cluster (by default, it is done when no such image exist). IMAGE_REGEXP_TRANSLATION=1 enables regexp syntax in image name translation rules. See Image Name Translation for more details.","title":"Control flags"},{"location":"dev/cirros/","text":"Building own CirrOS image The purpose of this document is to provide instructions for building custom CirrOS image that has proper Python-based Cloud Init implementation installed and added to the init sequence. Preparation This example is based on the official sources hosted on launchpad . Assuming that you have already installed git, clone the above repository with the following command: git clone https://git.launchpad.net/cirros Let's go to the cirros/ . The assumption is that the rest of the commands are run in that directory. cd cirros patch -p1 /path/to/virtlet-repository/contrib/cirros-patches/cirros-repo.diff Create the downloads directory and download the buildroot tar into it: mkdir ../downloads ; ln -s ../downloads br_ver= 2017.02 ( cd downloads ; wget http://buildroot.uclibc.org/downloads/buildroot-${br_ver}.tar.gz ) tar xf buildroot-${br_ver}.tar.gz Ensure that you have installed the packages listed in cirros/bin/system-setup (package names can be different for different Linux distros, for Debian-based system you can simply use cirros/bin/system-setup to install them). Applying the patches ( cd buildroot QUILT_PATCHES=$PWD/../patches-buildroot quilt push -a ) ( cd buildroot patch -p1 /path/to/virtlet-repository/contrib/cirros-patches/buildroot.diff ) Retrieving the sources of buildroot packages make ARCH=i386 br-source Building buildroot The initial make command is expected to fail. make ARCH=i386 OUT_D=$PWD/output/i386 Use the following command after make fails: ( cd output/i386/buildroot ; cp -a build/python-cloud-init*/build/lib/cloudinit target/usr/lib/python2.7/site-packages ) sed -i -e 's/BR2_PACKAGE_PYTHON_CLOUD_INIT=y/# BR2_PACKAGE_PYTHON_CLOUD_INIT is not set/' conf/buildroot-i386.config then run the previous build command again: make ARCH=i386 OUT_D=$PWD/output/i386 Downloading kernel and GRUB You need to set a correct version of kernel (the versions are listed here ) e.g.: kver= 4.13.0-32.35 ./bin/grab-kernels $kver Same for GRUB (version list is here ): gver= 2.02~beta3-4ubuntu2.2 ./bin/grab-grub-efi $gver Building the final image sudo ./bin/bundle -v --arch=i386 output/i386/rootfs.tar \\ download/kernel-i386.deb download/grub-efi-i386.tar.gz output/i386/images The final image will be named output/i386/images/disk.img .","title":"Building a custom CirrOS image"},{"location":"dev/cirros/#building-own-cirros-image","text":"The purpose of this document is to provide instructions for building custom CirrOS image that has proper Python-based Cloud Init implementation installed and added to the init sequence.","title":"Building own CirrOS image"},{"location":"dev/cirros/#preparation","text":"This example is based on the official sources hosted on launchpad . Assuming that you have already installed git, clone the above repository with the following command: git clone https://git.launchpad.net/cirros Let's go to the cirros/ . The assumption is that the rest of the commands are run in that directory. cd cirros patch -p1 /path/to/virtlet-repository/contrib/cirros-patches/cirros-repo.diff Create the downloads directory and download the buildroot tar into it: mkdir ../downloads ; ln -s ../downloads br_ver= 2017.02 ( cd downloads ; wget http://buildroot.uclibc.org/downloads/buildroot-${br_ver}.tar.gz ) tar xf buildroot-${br_ver}.tar.gz Ensure that you have installed the packages listed in cirros/bin/system-setup (package names can be different for different Linux distros, for Debian-based system you can simply use cirros/bin/system-setup to install them).","title":"Preparation"},{"location":"dev/cirros/#applying-the-patches","text":"( cd buildroot QUILT_PATCHES=$PWD/../patches-buildroot quilt push -a ) ( cd buildroot patch -p1 /path/to/virtlet-repository/contrib/cirros-patches/buildroot.diff )","title":"Applying the patches"},{"location":"dev/cirros/#retrieving-the-sources-of-buildroot-packages","text":"make ARCH=i386 br-source","title":"Retrieving the sources of buildroot packages"},{"location":"dev/cirros/#building-buildroot","text":"The initial make command is expected to fail. make ARCH=i386 OUT_D=$PWD/output/i386 Use the following command after make fails: ( cd output/i386/buildroot ; cp -a build/python-cloud-init*/build/lib/cloudinit target/usr/lib/python2.7/site-packages ) sed -i -e 's/BR2_PACKAGE_PYTHON_CLOUD_INIT=y/# BR2_PACKAGE_PYTHON_CLOUD_INIT is not set/' conf/buildroot-i386.config then run the previous build command again: make ARCH=i386 OUT_D=$PWD/output/i386","title":"Building buildroot"},{"location":"dev/cirros/#downloading-kernel-and-grub","text":"You need to set a correct version of kernel (the versions are listed here ) e.g.: kver= 4.13.0-32.35 ./bin/grab-kernels $kver Same for GRUB (version list is here ): gver= 2.02~beta3-4ubuntu2.2 ./bin/grab-grub-efi $gver","title":"Downloading kernel and GRUB"},{"location":"dev/cirros/#building-the-final-image","text":"sudo ./bin/bundle -v --arch=i386 output/i386/rootfs.tar \\ download/kernel-i386.deb download/grub-efi-i386.tar.gz output/i386/images The final image will be named output/i386/images/disk.img .","title":"Building the final image"},{"location":"dev/guidelines/","text":"Reviewer guidelines Be proffesional. When it's possible - follow general golang code review guidelines . Contributing guidelines Please check out Effective Go as a base source of rules for this project. If it's possible please: follow rules listed in code review guidelines remember that any newly added exported from package (by using capitalized name) function, structure, interface, has to have doc comment * consider adding issue to our issue tracker if you will find any deviation from this rules in our existing code","title":"Guidelines"},{"location":"dev/guidelines/#reviewer-guidelines","text":"Be proffesional. When it's possible - follow general golang code review guidelines .","title":"Reviewer guidelines"},{"location":"dev/guidelines/#contributing-guidelines","text":"Please check out Effective Go as a base source of rules for this project. If it's possible please: follow rules listed in code review guidelines remember that any newly added exported from package (by using capitalized name) function, structure, interface, has to have doc comment * consider adding issue to our issue tracker if you will find any deviation from this rules in our existing code","title":"Contributing guidelines"},{"location":"dev/logging/","text":"VM Logging Virtlet runs many VMs on Kubernetes and each of them writes something to stdout and stderr . User is given two options: redirect stdout / stderr to unix socket (DEFAULT) redirect stdout / stderr into pty console Redirecting stdout and stderr into unix socket By default VM logs are sent to unix socket: /var/lib/libvirt/streamer.sock This option is enabled by default. If you want to disable it set disable_logging to true in virtlet-config configmap. Architecture There is one directory on host needed: /var/log/pods (predefined by Kubernetes) is where Kubernetes expects JSON formatted logs to appear. NOTE: It is important to configure volume mount for mirantis/virtlet . Kubernetes is not able to find and understand raw log files that contain direct dumps from VM. Therefore we run a worker which is reformatting VM logs into a special JSON format that is understood by Kubernetes. Provided that redirecting stdout and stderr to unix socket is turned on (DEFAULT) user should be able to see logs on Kubernetes Dashboard: Obtaining logs from CLI should also work: $ kubectl logs cirros-vm ... === datasource: nocloud local === instance-id: cirros-vm-001 name: N/A availability-zone: N/A local-hostname: cirros-vm launch-index: N/A === cirros: current=0.3.5 uptime=1.75 === ____ ____ ____ / __/ __ ____ ____ / __ / __/ http://cirros-cloud.net login as 'cirros' user. default password: 'cubswin:)'. use 'sudo' for root. Limitations There is one limitation when redirecting logs to unix socket is enabled: Command virsh console vm no longer works since libvirt serial port type is 'unix' and not 'pty'. To make virsh console working set disable_logging to true in virtlet-config configmap which will disable logging. Redirecting stdout and stderr into pty In some cases it may be desired to enable virsh console VM command that is disabled in case when redirecting everything into files is enabled (see Limitations above). Just set disable_logging to true in virtlet-config configmap. Limitations There are some limitations when redirecting logs into unix socket is disabled: command kubectl logs MY-POD will not work command kubectl attach MY-POD will not work Kubernetes Dashboard will not display any logs for Virtlet pods","title":"Logging architecture"},{"location":"dev/logging/#vm-logging","text":"Virtlet runs many VMs on Kubernetes and each of them writes something to stdout and stderr . User is given two options: redirect stdout / stderr to unix socket (DEFAULT) redirect stdout / stderr into pty console","title":"VM Logging"},{"location":"dev/logging/#redirecting-stdout-and-stderr-into-unix-socket","text":"By default VM logs are sent to unix socket: /var/lib/libvirt/streamer.sock This option is enabled by default. If you want to disable it set disable_logging to true in virtlet-config configmap.","title":"Redirecting stdout and stderr into unix socket"},{"location":"dev/logging/#architecture","text":"There is one directory on host needed: /var/log/pods (predefined by Kubernetes) is where Kubernetes expects JSON formatted logs to appear. NOTE: It is important to configure volume mount for mirantis/virtlet . Kubernetes is not able to find and understand raw log files that contain direct dumps from VM. Therefore we run a worker which is reformatting VM logs into a special JSON format that is understood by Kubernetes. Provided that redirecting stdout and stderr to unix socket is turned on (DEFAULT) user should be able to see logs on Kubernetes Dashboard: Obtaining logs from CLI should also work: $ kubectl logs cirros-vm ... === datasource: nocloud local === instance-id: cirros-vm-001 name: N/A availability-zone: N/A local-hostname: cirros-vm launch-index: N/A === cirros: current=0.3.5 uptime=1.75 === ____ ____ ____ / __/ __ ____ ____ / __ / __/ http://cirros-cloud.net login as 'cirros' user. default password: 'cubswin:)'. use 'sudo' for root.","title":"Architecture"},{"location":"dev/logging/#limitations","text":"There is one limitation when redirecting logs to unix socket is enabled: Command virsh console vm no longer works since libvirt serial port type is 'unix' and not 'pty'. To make virsh console working set disable_logging to true in virtlet-config configmap which will disable logging.","title":"Limitations"},{"location":"dev/logging/#redirecting-stdout-and-stderr-into-pty","text":"In some cases it may be desired to enable virsh console VM command that is disabled in case when redirecting everything into files is enabled (see Limitations above). Just set disable_logging to true in virtlet-config configmap.","title":"Redirecting stdout and stderr into pty"},{"location":"dev/logging/#limitations_1","text":"There are some limitations when redirecting logs into unix socket is disabled: command kubectl logs MY-POD will not work command kubectl attach MY-POD will not work Kubernetes Dashboard will not display any logs for Virtlet pods","title":"Limitations"},{"location":"dev/setup/","text":"Prerequisites You'll need the following to run the local environment: SELinux/AppArmor disabled on the host (to disable them, follow the documentation ofthe Linux distribution that you run on the host) if host have libvirt installed, it should be stopped when working with Virtlet Docker should be installed on the host and the user account on which Virtlet will be built and run should be properly configured to use that Docker daemon (likely adding the user account into the group in which Docker deamon is running should be enough, but please follow the Docker documentation for your Linux distribution), kubeadm-dind-cluster script for Kubernetes version 1.12 ( dind-cluster-v1.12.sh ). You can get the cluster startup script like this: $ wget -O ~/dind-cluster-v1.12.sh https://github.com/kubernetes-sigs/kubeadm-dind-cluster/releases/download/v0.1.0/dind-cluster-v1.12.sh $ chmod +x ~/dind-cluster-v1.12.sh Running the local environment In order to start locally-built Virtlet and CRI proxy on kubeadm-dind-cluster : $ # Remove any previous build container related docker volumes $ # This is optional step that should be done if the build container $ # doesn't match current Virtlet sources (i.e. is too old) $ build/cmd.sh clean $ # build Virtlet binaries the image $ build/cmd.sh build $ # start DIND cluster $ ~/dind-cluster-v1.12.sh up $ # copy binaries to kube-node-1 $ build/cmd.sh copy-dind $ # inject Virtlet image into the DIND node and start the Virtlet DaemonSet $ build/cmd.sh start-dind $ # run some e2e tests $ build/cmd.sh e2e -test.v $ # run e2e tests that have 'Should have default route' in their description $ build/cmd.sh e2e -test.v -ginkgo.focus= Should have default route $ # Restart the DIND cluster. Binaries from copy-dind are preserved $ # (you may copy newer ones with another copy-dind command) $ ~/dind-cluster-v1.12.sh up $ # start Virtlet daemonset again $ build/cmd.sh start-dind You may use flannel instead of default CNI bridge networking for the test cluster. To do so, set CNI_PLUGIN environment variable: $ export CNI_PLUGIN=flannel Virtlet uses KVM if it's available by default. To disable it for your local development environment, set VIRTLET_DISABLE_KVM environment variable to a non-empty value before running build/cmd.sh start-dind .","title":"Setting up the environment"},{"location":"dev/setup/#prerequisites","text":"You'll need the following to run the local environment: SELinux/AppArmor disabled on the host (to disable them, follow the documentation ofthe Linux distribution that you run on the host) if host have libvirt installed, it should be stopped when working with Virtlet Docker should be installed on the host and the user account on which Virtlet will be built and run should be properly configured to use that Docker daemon (likely adding the user account into the group in which Docker deamon is running should be enough, but please follow the Docker documentation for your Linux distribution), kubeadm-dind-cluster script for Kubernetes version 1.12 ( dind-cluster-v1.12.sh ). You can get the cluster startup script like this: $ wget -O ~/dind-cluster-v1.12.sh https://github.com/kubernetes-sigs/kubeadm-dind-cluster/releases/download/v0.1.0/dind-cluster-v1.12.sh $ chmod +x ~/dind-cluster-v1.12.sh","title":"Prerequisites"},{"location":"dev/setup/#running-the-local-environment","text":"In order to start locally-built Virtlet and CRI proxy on kubeadm-dind-cluster : $ # Remove any previous build container related docker volumes $ # This is optional step that should be done if the build container $ # doesn't match current Virtlet sources (i.e. is too old) $ build/cmd.sh clean $ # build Virtlet binaries the image $ build/cmd.sh build $ # start DIND cluster $ ~/dind-cluster-v1.12.sh up $ # copy binaries to kube-node-1 $ build/cmd.sh copy-dind $ # inject Virtlet image into the DIND node and start the Virtlet DaemonSet $ build/cmd.sh start-dind $ # run some e2e tests $ build/cmd.sh e2e -test.v $ # run e2e tests that have 'Should have default route' in their description $ build/cmd.sh e2e -test.v -ginkgo.focus= Should have default route $ # Restart the DIND cluster. Binaries from copy-dind are preserved $ # (you may copy newer ones with another copy-dind command) $ ~/dind-cluster-v1.12.sh up $ # start Virtlet daemonset again $ build/cmd.sh start-dind You may use flannel instead of default CNI bridge networking for the test cluster. To do so, set CNI_PLUGIN environment variable: $ export CNI_PLUGIN=flannel Virtlet uses KVM if it's available by default. To disable it for your local development environment, set VIRTLET_DISABLE_KVM environment variable to a non-empty value before running build/cmd.sh start-dind .","title":"Running the local environment"},{"location":"dev/tests/","text":"Running unit and integration tests In order to run unit tests, use: $ build/cmd.sh test In order to run integration, use: $ build/cmd.sh integration There's a number of 'golden master' tests in Virtlet. These tests work by generating some JSON data and comparing it to the previous version of that data stored in git index (i.e. staged or committed). The test fails if there's any difference, and the file is updated in this case. You need to stage or commit the changes to make the test pass (or fix the code so there's no changes). Sample usage: $ cd pkg/libvirttools/ $ ../../build/cmd.sh gotest --- FAIL: TestDomainDefinitions (0.35s) --- FAIL: TestDomainDefinitions/cloud-init_with_user_data (0.05s) gm.go:58: got difference for TestDomainDefinitions/cloud-init_with_user_data : diff --git a/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json b/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json index e852f8b..6db03cc 100755 --- a/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json +++ b/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json @@ -66,7 +66,7 @@ name : domain conn: iso image , data : { meta-data : {\\ instance-id\\ :\\ testName_0.default\\ ,\\ local-hostname\\ :\\ testName_0\\ ,\\ public-keys\\ :[\\ key1\\ ,\\ key2\\ ]} , - user-data : #cloud-config\\nusers:\\n- name: cloudy\\n + user-data : #cloud-config\\nusers:\\n- name: cloudy1\\n } }, { FAIL exit status 1 FAIL github.com/Mirantis/virtlet/pkg/libvirttools 0.466s $ # accept the changes by staging them $ git add TestDomainDefinitions__cloud-init_with_user_data.json $ ../../build/cmd.sh gotest PASS ok github.com/Mirantis/virtlet/pkg/libvirttools 0.456s Running tests on Mac OS X To run tests on Mac OS X you need a working Go 1.8 installation and Glide . You also need to install cdrtools package and then make a symbolic link for mkisofs named genisoimage : $ brew install cdrtools $ sudo ln -s `which mkisofs` /usr/local/bin/genisoimage Some of the tests such as integration/e2e and network related tests only run on Linux. That being said, some of the tests do run on Mac OS X. First you need to make sure Virtlet is checked out as $GOPATH/src/github.com/Mirantis/virtlet and install the glide deps: $ cd $GOPATH/src/github.com/Mirantis/virtlet $ glide install --strip-vendor $ go test -v ./pkg/{flexvolume,imagetranslation,libvirttools,metadata,stream,utils,tapmanager} Running e2e tests You need a running Kubernetes cluster to run e2e tests. Virtlet e2e tests can be run using Virtlet build container: $ # run some e2e tests $ build/cmd.sh e2e -test.v $ # run e2e tests that have 'Should have default route' in their description $ build/cmd.sh e2e -test.v -ginkgo.focus= Should have default route You can also build the e2e runner locally and run it on either Mac or Linux: $ glide i --strip-vendor $ go test -i -c -o _output/virtlet-e2e-tests ./tests/e2e $ _output/virtlet-e2e-tests -ginkgo.v Virtlet uses Ginkgo for its e2e tests, so you can refer to the list of Ginkgo CLI flags . The Ginkgo flags should be passed using -ginkgo.XXX convention. The following additional flags are recognized by Virtlet e2e runner: -cluster-url=URL (default http://127.0.0.1:8080 ) specifies an insecure apiserver endpoint to use. You can use kubectl proxy when running the tests against a real cluster. -image=LOCATION (defaults to Virtlet's modified CirrOS image) specifies the location of the image to use for the tests. The LOCATION is an URL without the protocol prefix ( http(s):// ). -sshuser=USER specifies ssh user name that should be used with the image. -include-cloud-init-tests= (defaults to false) specifies that the cloud-init tests should included. -memoryLimit=N (defaults to 160) specifies the memory limit for the VM that should be used. You may need to adjust this setting according to the requirements of the image. -include-unsafe-tests (defaults to false) includes the tests that can be unsafe if they're run outside the build container. Use with care! -junitOutput=FILENAME specifies that a JUnit XML output file should be produced by the runner. Tests may have several labels which are included in their names and can be used in -ginkgo.focus / ginkgo.skip options: [Conformance] specifies the tests that any properly configured Kubernetes cluster with Virtlet should pass. [Heavy] specifies the tests that shouldn't be run when KVM cannot be used, for example, on Virtlet public CI. [Disruptive] tests may break your cluster or cause some or all of the VMs to be restarted. [MultiCNI] tests should be used to examine a multi-CNI setup. [Flaky] tests contain flakes that should be fixed. This label basically designates a known bug in either the test or Virtlet itself. These tests are skipped on Virtlet public CI.","title":"Running the tests"},{"location":"dev/tests/#running-unit-and-integration-tests","text":"In order to run unit tests, use: $ build/cmd.sh test In order to run integration, use: $ build/cmd.sh integration There's a number of 'golden master' tests in Virtlet. These tests work by generating some JSON data and comparing it to the previous version of that data stored in git index (i.e. staged or committed). The test fails if there's any difference, and the file is updated in this case. You need to stage or commit the changes to make the test pass (or fix the code so there's no changes). Sample usage: $ cd pkg/libvirttools/ $ ../../build/cmd.sh gotest --- FAIL: TestDomainDefinitions (0.35s) --- FAIL: TestDomainDefinitions/cloud-init_with_user_data (0.05s) gm.go:58: got difference for TestDomainDefinitions/cloud-init_with_user_data : diff --git a/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json b/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json index e852f8b..6db03cc 100755 --- a/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json +++ b/pkg/libvirttools/TestDomainDefinitions__cloud-init_with_user_data.json @@ -66,7 +66,7 @@ name : domain conn: iso image , data : { meta-data : {\\ instance-id\\ :\\ testName_0.default\\ ,\\ local-hostname\\ :\\ testName_0\\ ,\\ public-keys\\ :[\\ key1\\ ,\\ key2\\ ]} , - user-data : #cloud-config\\nusers:\\n- name: cloudy\\n + user-data : #cloud-config\\nusers:\\n- name: cloudy1\\n } }, { FAIL exit status 1 FAIL github.com/Mirantis/virtlet/pkg/libvirttools 0.466s $ # accept the changes by staging them $ git add TestDomainDefinitions__cloud-init_with_user_data.json $ ../../build/cmd.sh gotest PASS ok github.com/Mirantis/virtlet/pkg/libvirttools 0.456s","title":"Running unit and integration tests"},{"location":"dev/tests/#running-tests-on-mac-os-x","text":"To run tests on Mac OS X you need a working Go 1.8 installation and Glide . You also need to install cdrtools package and then make a symbolic link for mkisofs named genisoimage : $ brew install cdrtools $ sudo ln -s `which mkisofs` /usr/local/bin/genisoimage Some of the tests such as integration/e2e and network related tests only run on Linux. That being said, some of the tests do run on Mac OS X. First you need to make sure Virtlet is checked out as $GOPATH/src/github.com/Mirantis/virtlet and install the glide deps: $ cd $GOPATH/src/github.com/Mirantis/virtlet $ glide install --strip-vendor $ go test -v ./pkg/{flexvolume,imagetranslation,libvirttools,metadata,stream,utils,tapmanager}","title":"Running tests on Mac OS X"},{"location":"dev/tests/#running-e2e-tests","text":"You need a running Kubernetes cluster to run e2e tests. Virtlet e2e tests can be run using Virtlet build container: $ # run some e2e tests $ build/cmd.sh e2e -test.v $ # run e2e tests that have 'Should have default route' in their description $ build/cmd.sh e2e -test.v -ginkgo.focus= Should have default route You can also build the e2e runner locally and run it on either Mac or Linux: $ glide i --strip-vendor $ go test -i -c -o _output/virtlet-e2e-tests ./tests/e2e $ _output/virtlet-e2e-tests -ginkgo.v Virtlet uses Ginkgo for its e2e tests, so you can refer to the list of Ginkgo CLI flags . The Ginkgo flags should be passed using -ginkgo.XXX convention. The following additional flags are recognized by Virtlet e2e runner: -cluster-url=URL (default http://127.0.0.1:8080 ) specifies an insecure apiserver endpoint to use. You can use kubectl proxy when running the tests against a real cluster. -image=LOCATION (defaults to Virtlet's modified CirrOS image) specifies the location of the image to use for the tests. The LOCATION is an URL without the protocol prefix ( http(s):// ). -sshuser=USER specifies ssh user name that should be used with the image. -include-cloud-init-tests= (defaults to false) specifies that the cloud-init tests should included. -memoryLimit=N (defaults to 160) specifies the memory limit for the VM that should be used. You may need to adjust this setting according to the requirements of the image. -include-unsafe-tests (defaults to false) includes the tests that can be unsafe if they're run outside the build container. Use with care! -junitOutput=FILENAME specifies that a JUnit XML output file should be produced by the runner. Tests may have several labels which are included in their names and can be used in -ginkgo.focus / ginkgo.skip options: [Conformance] specifies the tests that any properly configured Kubernetes cluster with Virtlet should pass. [Heavy] specifies the tests that shouldn't be run when KVM cannot be used, for example, on Virtlet public CI. [Disruptive] tests may break your cluster or cause some or all of the VMs to be restarted. [MultiCNI] tests should be used to examine a multi-CNI setup. [Flaky] tests contain flakes that should be fixed. This label basically designates a known bug in either the test or Virtlet itself. These tests are skipped on Virtlet public CI.","title":"Running e2e tests"},{"location":"dev/virtlet-ci/","text":"Virtlet CI Virtlet uses CircleCI for building binaries, docker images and running the tests. The CI configuration resides in .circleci/config.yml file. You can sign into CircleCI, and make it run tests for your own fork of Virtlet by connecting it to your GitHub account. There are some conventions about branch naming that are respected by CircleCI. Namely, branches with names ending with -net will be checked with extra e2e jobs that examine Weave, Flannel and multi-CNI based network setups. For branches with names ending with -docs only documentation will be built. Also, you can append [ci skip] to your commit message to have CI skip the particular commit/PR altogether.","title":"Virtlet CI"},{"location":"dev/virtlet-ci/#virtlet-ci","text":"Virtlet uses CircleCI for building binaries, docker images and running the tests. The CI configuration resides in .circleci/config.yml file. You can sign into CircleCI, and make it run tests for your own fork of Virtlet by connecting it to your GitHub account. There are some conventions about branch naming that are respected by CircleCI. Namely, branches with names ending with -net will be checked with extra e2e jobs that examine Weave, Flannel and multi-CNI based network setups. For branches with names ending with -docs only documentation will be built. Also, you can append [ci skip] to your commit message to have CI skip the particular commit/PR altogether.","title":"Virtlet CI"},{"location":"dev/vm-pod-lifecycle/","text":"Lifecycle of a VM pod This document describes the lifecycle of VM pod managed by Virtlet. This description omits the details of volume setup (using flexvolumes ), handling of logs, the VM console and port forwarding (done by streaming server ), or port forwarding. Assumptions Communication between kubelet and Virtlet goes through criproxy which directs requests to Virtlet only if the requests concern a pod that has Virtlet-specific annotation or an image that has Virtlet-specific prefix. Lifecycle VM Pod Startup A pod is created in Kubernetes cluster, either directly by the user or via some other mechanism such as a higher-level Kubernetes object managed by kube-controller-manager (ReplicaSet, DaemonSet etc.). Scheduler places the pod on a node based on the requested resources (CPU, memory, etc.) as well as pod's nodeSelector and pod/node affinity constraints, taints/tolerations and so on. kubelet running on the target node accepts the pod. kubelet invokes a CRI call RunPodSandbox to create the pod sandbox which will enclose all the containers in the pod definition. Note that at this point no information about the containers within the pod is passed to the call. kubelet can later request the information about the pod by means of PodSandboxStatus calls. If there's a Virtlet-specific annotation kubernetes.io/target-runtime: virtlet.cloud , CRI proxy passes the call to Virtlet. Virtlet saves sandbox metadata in its internal database, sets up the network namespace and then uses internal tapmanager mechanism to invoke ADD operation via the CNI plugin as specified by the CNI configuration on the node. The CNI plugin configures the network namespace by setting up network interfaces, IP addresses, routes, iptables rules and so on, and returns the network configuration information to the caller as described in the CNI spec . Virtlet's tapmanager mechanism adjusts the configuration of the network namespace to make it work with the VM. After creating the sandbox, kubelet starts the containers defined in the pod sandbox. Currently, Virtlet supports just one container per VM pod. So, the VM pod startup steps after this one describe the startup of this single container. Depending on the image pull police of the container, kubelet checks if the image needs to be pulled by means of ImageStatus call and then uses PullImage CRI call to pull the image if it doesn't exist or if imagePullPolicy: Always is used. If PullImage is invoked, Virtlet resolves the image location based on the image name translation configuration , then downloads the file and stores it in the image store. After the image is ready (no pull was needed or the PullImage call completed successfully), kubelet uses CreateContainer CRI call to create the container in the pod sandbox using the specified image. Virtlet uses the sandbox and container metadata to generate libvirt domain definition, using vmwrapper binary as the emulator and without specifying any network configuration in the domain. After CreateContainer call completes, kubelet invokes StartContainer call on the newly created container. Virtlet starts the libvirt domain. libvirt invokes vmwrapper as the emulator, passing it the necessary command line arguments as well as environment variables set by Virtlet. vmwrapper uses the environment variable values passed to Virtlet to communicate with tapmanager over an Unix domain socket, retrieving a file descriptor for a tap device and/or pci address of SR-IOV device set up by tapmanager . tapmanager uses its own simple protocol to communicate with vmwrapper because it needs to send file descriptors over the socket. This is not usually supported by RPC libraries, see e.g. grpc/grpc#11417 . vmwrapper then updates the command line arguments to include the network interface information and execs the actual emulator ( qemu ). At this point the VM is running and accessible via the network, and the pod is in Running state as well as it's only container. Deleting a pod This sequence is initiated when the pod is deleted, either by means of kubectl delete or a controller manager action due to deletion or downscaling of a higher-level object. kubelet notices the pod being deleted. kubelet invokes StopContainer CRI calls which is getting forwared to Virtlet based on the containing pod sandbox annotations. Virtlet stops the libvirt domain. libvirt sends a signal to qemu , which initiates the shutdown. If it doesn't quit in a reasonable time determined by pod's termination grace period, Virtlet will forcibly terminate the domain, thus killing the qemu process. After all the containers in the pod (the single container in case of Virtlet VM pod) are stopped, kubelet invokes StopPodSandbox CRI call. Virtlet asks its tapmanager to remove pod from the network by means of CNI DEL command. after StopPodSandbox returns, the pod sandbox will be eventually GC'd by kubelet by means of RemovePodSandbox CRI call. Upon RemovePodSandbox , Virtlet removes the pod metadata from its internal database.","title":"Lifecycle of a VM pod"},{"location":"dev/vm-pod-lifecycle/#lifecycle-of-a-vm-pod","text":"This document describes the lifecycle of VM pod managed by Virtlet. This description omits the details of volume setup (using flexvolumes ), handling of logs, the VM console and port forwarding (done by streaming server ), or port forwarding.","title":"Lifecycle of a VM pod"},{"location":"dev/vm-pod-lifecycle/#assumptions","text":"Communication between kubelet and Virtlet goes through criproxy which directs requests to Virtlet only if the requests concern a pod that has Virtlet-specific annotation or an image that has Virtlet-specific prefix.","title":"Assumptions"},{"location":"dev/vm-pod-lifecycle/#lifecycle","text":"","title":"Lifecycle"},{"location":"dev/vm-pod-lifecycle/#vm-pod-startup","text":"A pod is created in Kubernetes cluster, either directly by the user or via some other mechanism such as a higher-level Kubernetes object managed by kube-controller-manager (ReplicaSet, DaemonSet etc.). Scheduler places the pod on a node based on the requested resources (CPU, memory, etc.) as well as pod's nodeSelector and pod/node affinity constraints, taints/tolerations and so on. kubelet running on the target node accepts the pod. kubelet invokes a CRI call RunPodSandbox to create the pod sandbox which will enclose all the containers in the pod definition. Note that at this point no information about the containers within the pod is passed to the call. kubelet can later request the information about the pod by means of PodSandboxStatus calls. If there's a Virtlet-specific annotation kubernetes.io/target-runtime: virtlet.cloud , CRI proxy passes the call to Virtlet. Virtlet saves sandbox metadata in its internal database, sets up the network namespace and then uses internal tapmanager mechanism to invoke ADD operation via the CNI plugin as specified by the CNI configuration on the node. The CNI plugin configures the network namespace by setting up network interfaces, IP addresses, routes, iptables rules and so on, and returns the network configuration information to the caller as described in the CNI spec . Virtlet's tapmanager mechanism adjusts the configuration of the network namespace to make it work with the VM. After creating the sandbox, kubelet starts the containers defined in the pod sandbox. Currently, Virtlet supports just one container per VM pod. So, the VM pod startup steps after this one describe the startup of this single container. Depending on the image pull police of the container, kubelet checks if the image needs to be pulled by means of ImageStatus call and then uses PullImage CRI call to pull the image if it doesn't exist or if imagePullPolicy: Always is used. If PullImage is invoked, Virtlet resolves the image location based on the image name translation configuration , then downloads the file and stores it in the image store. After the image is ready (no pull was needed or the PullImage call completed successfully), kubelet uses CreateContainer CRI call to create the container in the pod sandbox using the specified image. Virtlet uses the sandbox and container metadata to generate libvirt domain definition, using vmwrapper binary as the emulator and without specifying any network configuration in the domain. After CreateContainer call completes, kubelet invokes StartContainer call on the newly created container. Virtlet starts the libvirt domain. libvirt invokes vmwrapper as the emulator, passing it the necessary command line arguments as well as environment variables set by Virtlet. vmwrapper uses the environment variable values passed to Virtlet to communicate with tapmanager over an Unix domain socket, retrieving a file descriptor for a tap device and/or pci address of SR-IOV device set up by tapmanager . tapmanager uses its own simple protocol to communicate with vmwrapper because it needs to send file descriptors over the socket. This is not usually supported by RPC libraries, see e.g. grpc/grpc#11417 . vmwrapper then updates the command line arguments to include the network interface information and execs the actual emulator ( qemu ). At this point the VM is running and accessible via the network, and the pod is in Running state as well as it's only container.","title":"VM Pod Startup"},{"location":"dev/vm-pod-lifecycle/#deleting-a-pod","text":"This sequence is initiated when the pod is deleted, either by means of kubectl delete or a controller manager action due to deletion or downscaling of a higher-level object. kubelet notices the pod being deleted. kubelet invokes StopContainer CRI calls which is getting forwared to Virtlet based on the containing pod sandbox annotations. Virtlet stops the libvirt domain. libvirt sends a signal to qemu , which initiates the shutdown. If it doesn't quit in a reasonable time determined by pod's termination grace period, Virtlet will forcibly terminate the domain, thus killing the qemu process. After all the containers in the pod (the single container in case of Virtlet VM pod) are stopped, kubelet invokes StopPodSandbox CRI call. Virtlet asks its tapmanager to remove pod from the network by means of CNI DEL command. after StopPodSandbox returns, the pod sandbox will be eventually GC'd by kubelet by means of RemovePodSandbox CRI call. Upon RemovePodSandbox , Virtlet removes the pod metadata from its internal database.","title":"Deleting a pod"},{"location":"reference/cloud-init/","text":"Cloud-init options Virtlet uses Cloud-Init data generation mechanism for the following purposes: setting the host name based on the name of the pod injecting ssh keys setting up the VM network writing the contents of Secrets and ConfigMaps into the VM volume mounting setting up block devices based on PVs adding user-defined Cloud-Init settings such as startup scripts See also the list of annotations for the list of annotations that are used for Cloud-Init. Example Below is a pod definition that we'll be using. The annotations: part lists the cloud-init related annotations supported by Virtlet. apiVersion: v1 kind: Pod metadata: name: ubuntu-vm annotations: kubernetes.io/target-runtime: virtlet.cloud # override some fields in cloud-init meta-data VirtletCloudInitMetaData: | instance-id: foobar # override some fields in cloud-init user-data VirtletCloudInitUserData: | users: - name: cloudy gecos: Magic Cloud App Daemon User inactive: true system: true # this option disables merging of user-data keys. # By default the lists and dicts in user-data keys # are merged using the standard method (more on this below) # VirtletCloudInitUserDataOverwrite: true # this options makes it possible to write a script # in place of the user-data file. In case if this option # is present user-data part is not generated. # VirtletCloudInitUserDataScript: | # #!/bin/sh # echo hello world # it's also possible to use a configmap to override # parts of user-data VirtletCloudInitUserDataSource: configmap/vm-user-data # it's also possible to specify that the whole user-data contents # are stored in a ConfigMap under the specified key: # VirtletCloudInitUserDataSourceKey: user-data # by default, the contents of user-data in a ConfigMap key is specified # as plain text, but it can also be encoded using base64: # VirtletCloudInitUserDataSourceEncoding: base64 # Virtlet-specific annotations follow # Specify some ssh keys directly VirtletSSHKeys: | ssh-rsa AAAA... me@localhost ssh-rsa AAAA... me1@localhost # ssh keys may also be pulled from 'authorized_keys' key # in a ConfigMap or a Secret VirtletSSHKeySource: secret/mysecret # can also use the following: # VirtletSSHKeySource: configmap/configmap-with-public-keys spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet containers: - name: ubuntu-vm image: virtlet.cloud/cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img volumeMounts: # this will write configmap contents using the `write_file` cloud-init module - name: config mountPath: /etc/foobar # this will write secret contents using the `write_file` cloud-init module - name: secret mountPath: /etc/baz # mount the qcow2 volume under /var/lib/docker - name: extra mountPath: /var/lib/docker # mount a raw device under /var/lib/postgresql - name: raw mountPath: /var/lib/postgresql # mount a ceph volume under /cephdata - name: ceph mountPath: /cephdata volumes: - name: config configMap: name: someconfig - name: secret secret: secretName: somesecret - name: extra flexVolume: driver: virtlet/flexvolume_driver options: type: qcow2 capacity: 2048MiB - name: raw flexVolume: driver: virtlet/flexvolume_driver options: type: raw path: /dev/mapper/somevg - name: ceph flexVolume: driver: virtlet/flexvolume_driver options: type: ceph monitor: 10.0.0.1:6789 user: libvirt secret: .... volume: rbd-test-image pool: libvirt-pool Cloud-init data generation The cloud-init data generated by Virtlet consists of meta-data , user-data and network-config parts. It's currently provided by means of NoCloud and Config Drive datasource, which in this case involves making an iso9660 image to be mounted by the VM. An implementation of either Amazon EC2 or OpenStack datasource can be added later. The cloud-init data is generated based on the following sources: pod name direct user-data/meta-data values from pod definition direct user-data/meta-data values from configmap/secret pod volumes and container mounts pod network configuration ssh keys specified either in pod definition or using a configmap/secret Output ISO image format Virtlet supports two types of Cloud-init ISO9660-based datasources, NoCloud and ConfigDrive. The user may choose an appropriate one using VirtletCloudInitImageType annotation, which can have either nocloud or configdrive as its value. When there's no VirtletCloudInitImageType annotation, Virtlet defaults to nocloud . Detailed structure of the generated files The meta-data part is a piece of JSON that looks like this: { instance-id : foobar , local-hostname : ubuntu-vm , public-keys : [ ssh-rsa AAAA... me@localhost , ssh-rsa AAAA... me1@localhost , ... ] } In case of ConfigDrive, this JSON has instance-id repeated as uuid and local-hostname repeated as hostname . We're using JSON format here so as to be compatible with older cloud-init implementations such as one used in CirrOS images which are used to test Virtlet. The following fields are generated by Virtlet: instance-id by default contains a name generated from pod name and pod namespace name separated with dot ( ubuntu-vm.default ). In this case it's overridden using VirtletCloudInitMetaData in the pod definition. Most of the time this field doesn't change much in the behavior of Virtlet VMs. local-hostname contains the name of the pod so VM's hostname defaults just to the pod name public-keys is a list of ssh public keys to put into the default user's ~/.ssh/authorized_keys file. It's taken either from VirtletSSHKeys annotation or from authorized_keys key in a kubernetes Secret on ConfigMap specified via VirtletSSHKeySource . The user-data part is only generated if it's not empty. It's a YAML file (because CirrOS doesn't support #cloud-config anyway) with the following content: #cloud-config mounts: - [ /dev/vdc1, /var/lib/docker ] - [ /dev/vdd, /var/lib/postgresql ] - [ /dev/vde, /cephdata ] write_files: - path: /etc/foobar/some-file-from-configmap.conf permissions: '0644' encoding: b64 content: IyB0aGlzIGlzIGEgc2FtcGxlIGNvbmZpZyBmaWxlCiMgdGFrZW4gZnJvbSBhIGNvbmZpZ21hcAo= - path: /etc/baz/a-file-taken-from-secrets.conf permissions: '0600' encoding: b64 content: IyBzb21lIHNlY3JldCBjb250ZW50Cg== - path: /foobar.txt content: this part is from a configmap specified using VirtletCloudInitUserDataSource users: - name: cloudy gecos: Magic Cloud App Daemon User inactive: true system: true Network configuration uses YAML to provide data in Network Config Version 1 . The user-data content is generated as follows: mounts are generated based on volumeMount options of the container and pod's volumes that are flexVolume s and use virtlet/flexvolume_driver . These are seen as block devices by the VM and Virtlet mounts them into appropriate directories write_files are generated based on configmaps and secrets mounted into the container. It also includes /etc/cloud/environment file (see Environment variable support for more info) and optionally /etc/cloud/mount-volumes.sh that can be used to mount volumes on systems without udev (see Workarounds for volume mounting below) the yaml content specified using optional VirtletCloudInitUserData annotation as well as the content from Secret or ConfigMap specified using another optional VirtletCloudInitUserDataSource annotation are merged together using a simple algorithm described in cloud-init documentation. During the merge, the autogenerated user-data comes first, then the contents of VirtletCloudInitUserData and finally the data taken from a Secret or ConfigMap specified via VirtletCloudInitUserDataSource it's possible to disable user-data merge algorithm and use the simple key replacement scheme by adding VirtletCloudInitUserDataOverwrite: true annotation it's also possible to replace the user-data file content entirely by adding VirtletCloudInitUserDataScript option. This may be useful if you want to pass a script there which may be necessary for older/simpler cloud-init implementations such as one used by CirrOS. Within the content of this script, @virtlet-mount-script@ can be replaced with volume mounting shell commands and commands for making symlinks for raw block volumes (see Workarounds for volume mounting below) Propagating user-data from Kubernetes objects In addition to putting user-data document right in the pod definition using VirtletCloudInitUserData annotation, it is possible to have this document stored in either ConfigMap or Secret kubernetes object. This is achieved with yet another annotation, called VirtletCloudInitUserDataSource . It has the following format: kind/name where kind is either configmap or secret and name is the name of appropriate resource. When virtlet sees this annotation, it reads the object it refers to and puts all its keys into the user-data dictionary. This is done at the very beginning of the user-data generation process. If the pod definition has both VirtletCloudInitUserData and VirtletCloudInitUserDataSource annotations, the virtlet will load user-data from kubernetes object and then will merge it with that from VirtletCloudInitUserData (unless VirtletCloudInitUserDataOverwrite is set to \"true\" , in which case VirtletCloudInitUserData will overwrite it). There's also an option to store the whole contents of user-data in a single key of ConfigMap which can be specified using VirtletCloudInitUserDataSourceKey . By default, the data are stored there as plain text but you can also specify base64 encoding via VirtletCloudInitUserDataSourceEncoding annotation. Similar approach is taken with SSH keys. It is possible to provide VM with a list of SSH keys obtained from ConfigMap or Secret kubernetes objects. In order to do so, one uses VirtletSSHKeySource annotation with the following format: kind/name/key . As for the user-data, kind is one of configmap , secret , name is the name of resource and key is the key name in that resource containing SSH keys in the same format in VirtletSSHKeys . The key part is optional. When using kind/name annotation (without key ), virtlet will look for the authorized_keys key. As with the user-data VirtletSSHKeys keys are going to be appended to those from VirtletSSHKeySource unless it is set to overwrite them by VirtletCloudInitUserDataOverwrite: \"true\" . Workarounds for volume mounting and raw block volumes Currenly Virtlet uses /dev/disk/by-path to mount volumes specified using Virtlet flexvolume driver. There's a problem with this approach however, namely that this depends on udev being used by the guest system. Moreover, some images (e.g. CirrOS example image) may have limited cloud-init support and may not provide proper user-data handling necessary to mount the block devices. To handle the first of this problem, Virtlet uses write_files section of user-data to write a shell script named /etc/cloud/mount-volumes.sh (the script uses /bin/sh ) that performs the necessary volume mounts using sysfs to look up the proper device names under /dev . The script also checks whether the volumes are already mounted so as not to mount them several times (so it's idempotent). Another slightly related problem is that when raw block volumes are used, symlinks must be created inside the VM that lead to the proper device files under /dev . This is done by means of /etc/cloud/symlink-devs.sh script that's injected via cloud-init. It's also automatically added to the runcmd section, so it gets executed on the first VM boot and linked from /var/lib/cloud/scripts/per-boot/ so that it gets executed on subsequent VM boots, too. In any case, this script gets executed after mounts are processed, so Virtlet updates mounts entries in the user-specified user-data that point to the raw block volume devices to make them work as if the symlinks were in place during the mounting. To deal with the systems without proper user-data support such as CirrOS, Virtlet makes it possible to specify @virtlet-mount-script@ inside VirtletCloudInitUserDataScript annotation. This string will be replaced with the content that's normally written to /etc/cloud/symlink-devs.sh and /etc/cloud/mount-volumes.sh (in this order). Given that the script starts with #!/bin/sh , you can use the following construct in the pod annotations: VirtletCloudInitUserDataScript: @virtlet-mount-script@ Additional links These links may help to understand some basics about cloud-init: Cloud-init documentation Booting cloud images with libvirt Install cloud-init on Ubuntu and use locally... NoCloud EC2 instance metadata Automating Openstack with cloud init run a script on VM's first boot Create a Linux Lab on KVM Using Cloud Images","title":"Cloud-Init"},{"location":"reference/cloud-init/#cloud-init-options","text":"Virtlet uses Cloud-Init data generation mechanism for the following purposes: setting the host name based on the name of the pod injecting ssh keys setting up the VM network writing the contents of Secrets and ConfigMaps into the VM volume mounting setting up block devices based on PVs adding user-defined Cloud-Init settings such as startup scripts See also the list of annotations for the list of annotations that are used for Cloud-Init.","title":"Cloud-init options"},{"location":"reference/cloud-init/#example","text":"Below is a pod definition that we'll be using. The annotations: part lists the cloud-init related annotations supported by Virtlet. apiVersion: v1 kind: Pod metadata: name: ubuntu-vm annotations: kubernetes.io/target-runtime: virtlet.cloud # override some fields in cloud-init meta-data VirtletCloudInitMetaData: | instance-id: foobar # override some fields in cloud-init user-data VirtletCloudInitUserData: | users: - name: cloudy gecos: Magic Cloud App Daemon User inactive: true system: true # this option disables merging of user-data keys. # By default the lists and dicts in user-data keys # are merged using the standard method (more on this below) # VirtletCloudInitUserDataOverwrite: true # this options makes it possible to write a script # in place of the user-data file. In case if this option # is present user-data part is not generated. # VirtletCloudInitUserDataScript: | # #!/bin/sh # echo hello world # it's also possible to use a configmap to override # parts of user-data VirtletCloudInitUserDataSource: configmap/vm-user-data # it's also possible to specify that the whole user-data contents # are stored in a ConfigMap under the specified key: # VirtletCloudInitUserDataSourceKey: user-data # by default, the contents of user-data in a ConfigMap key is specified # as plain text, but it can also be encoded using base64: # VirtletCloudInitUserDataSourceEncoding: base64 # Virtlet-specific annotations follow # Specify some ssh keys directly VirtletSSHKeys: | ssh-rsa AAAA... me@localhost ssh-rsa AAAA... me1@localhost # ssh keys may also be pulled from 'authorized_keys' key # in a ConfigMap or a Secret VirtletSSHKeySource: secret/mysecret # can also use the following: # VirtletSSHKeySource: configmap/configmap-with-public-keys spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet containers: - name: ubuntu-vm image: virtlet.cloud/cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img volumeMounts: # this will write configmap contents using the `write_file` cloud-init module - name: config mountPath: /etc/foobar # this will write secret contents using the `write_file` cloud-init module - name: secret mountPath: /etc/baz # mount the qcow2 volume under /var/lib/docker - name: extra mountPath: /var/lib/docker # mount a raw device under /var/lib/postgresql - name: raw mountPath: /var/lib/postgresql # mount a ceph volume under /cephdata - name: ceph mountPath: /cephdata volumes: - name: config configMap: name: someconfig - name: secret secret: secretName: somesecret - name: extra flexVolume: driver: virtlet/flexvolume_driver options: type: qcow2 capacity: 2048MiB - name: raw flexVolume: driver: virtlet/flexvolume_driver options: type: raw path: /dev/mapper/somevg - name: ceph flexVolume: driver: virtlet/flexvolume_driver options: type: ceph monitor: 10.0.0.1:6789 user: libvirt secret: .... volume: rbd-test-image pool: libvirt-pool","title":"Example"},{"location":"reference/cloud-init/#cloud-init-data-generation","text":"The cloud-init data generated by Virtlet consists of meta-data , user-data and network-config parts. It's currently provided by means of NoCloud and Config Drive datasource, which in this case involves making an iso9660 image to be mounted by the VM. An implementation of either Amazon EC2 or OpenStack datasource can be added later. The cloud-init data is generated based on the following sources: pod name direct user-data/meta-data values from pod definition direct user-data/meta-data values from configmap/secret pod volumes and container mounts pod network configuration ssh keys specified either in pod definition or using a configmap/secret","title":"Cloud-init data generation"},{"location":"reference/cloud-init/#output-iso-image-format","text":"Virtlet supports two types of Cloud-init ISO9660-based datasources, NoCloud and ConfigDrive. The user may choose an appropriate one using VirtletCloudInitImageType annotation, which can have either nocloud or configdrive as its value. When there's no VirtletCloudInitImageType annotation, Virtlet defaults to nocloud .","title":"Output ISO image format"},{"location":"reference/cloud-init/#detailed-structure-of-the-generated-files","text":"The meta-data part is a piece of JSON that looks like this: { instance-id : foobar , local-hostname : ubuntu-vm , public-keys : [ ssh-rsa AAAA... me@localhost , ssh-rsa AAAA... me1@localhost , ... ] } In case of ConfigDrive, this JSON has instance-id repeated as uuid and local-hostname repeated as hostname . We're using JSON format here so as to be compatible with older cloud-init implementations such as one used in CirrOS images which are used to test Virtlet. The following fields are generated by Virtlet: instance-id by default contains a name generated from pod name and pod namespace name separated with dot ( ubuntu-vm.default ). In this case it's overridden using VirtletCloudInitMetaData in the pod definition. Most of the time this field doesn't change much in the behavior of Virtlet VMs. local-hostname contains the name of the pod so VM's hostname defaults just to the pod name public-keys is a list of ssh public keys to put into the default user's ~/.ssh/authorized_keys file. It's taken either from VirtletSSHKeys annotation or from authorized_keys key in a kubernetes Secret on ConfigMap specified via VirtletSSHKeySource . The user-data part is only generated if it's not empty. It's a YAML file (because CirrOS doesn't support #cloud-config anyway) with the following content: #cloud-config mounts: - [ /dev/vdc1, /var/lib/docker ] - [ /dev/vdd, /var/lib/postgresql ] - [ /dev/vde, /cephdata ] write_files: - path: /etc/foobar/some-file-from-configmap.conf permissions: '0644' encoding: b64 content: IyB0aGlzIGlzIGEgc2FtcGxlIGNvbmZpZyBmaWxlCiMgdGFrZW4gZnJvbSBhIGNvbmZpZ21hcAo= - path: /etc/baz/a-file-taken-from-secrets.conf permissions: '0600' encoding: b64 content: IyBzb21lIHNlY3JldCBjb250ZW50Cg== - path: /foobar.txt content: this part is from a configmap specified using VirtletCloudInitUserDataSource users: - name: cloudy gecos: Magic Cloud App Daemon User inactive: true system: true Network configuration uses YAML to provide data in Network Config Version 1 . The user-data content is generated as follows: mounts are generated based on volumeMount options of the container and pod's volumes that are flexVolume s and use virtlet/flexvolume_driver . These are seen as block devices by the VM and Virtlet mounts them into appropriate directories write_files are generated based on configmaps and secrets mounted into the container. It also includes /etc/cloud/environment file (see Environment variable support for more info) and optionally /etc/cloud/mount-volumes.sh that can be used to mount volumes on systems without udev (see Workarounds for volume mounting below) the yaml content specified using optional VirtletCloudInitUserData annotation as well as the content from Secret or ConfigMap specified using another optional VirtletCloudInitUserDataSource annotation are merged together using a simple algorithm described in cloud-init documentation. During the merge, the autogenerated user-data comes first, then the contents of VirtletCloudInitUserData and finally the data taken from a Secret or ConfigMap specified via VirtletCloudInitUserDataSource it's possible to disable user-data merge algorithm and use the simple key replacement scheme by adding VirtletCloudInitUserDataOverwrite: true annotation it's also possible to replace the user-data file content entirely by adding VirtletCloudInitUserDataScript option. This may be useful if you want to pass a script there which may be necessary for older/simpler cloud-init implementations such as one used by CirrOS. Within the content of this script, @virtlet-mount-script@ can be replaced with volume mounting shell commands and commands for making symlinks for raw block volumes (see Workarounds for volume mounting below)","title":"Detailed structure of the generated files"},{"location":"reference/cloud-init/#propagating-user-data-from-kubernetes-objects","text":"In addition to putting user-data document right in the pod definition using VirtletCloudInitUserData annotation, it is possible to have this document stored in either ConfigMap or Secret kubernetes object. This is achieved with yet another annotation, called VirtletCloudInitUserDataSource . It has the following format: kind/name where kind is either configmap or secret and name is the name of appropriate resource. When virtlet sees this annotation, it reads the object it refers to and puts all its keys into the user-data dictionary. This is done at the very beginning of the user-data generation process. If the pod definition has both VirtletCloudInitUserData and VirtletCloudInitUserDataSource annotations, the virtlet will load user-data from kubernetes object and then will merge it with that from VirtletCloudInitUserData (unless VirtletCloudInitUserDataOverwrite is set to \"true\" , in which case VirtletCloudInitUserData will overwrite it). There's also an option to store the whole contents of user-data in a single key of ConfigMap which can be specified using VirtletCloudInitUserDataSourceKey . By default, the data are stored there as plain text but you can also specify base64 encoding via VirtletCloudInitUserDataSourceEncoding annotation. Similar approach is taken with SSH keys. It is possible to provide VM with a list of SSH keys obtained from ConfigMap or Secret kubernetes objects. In order to do so, one uses VirtletSSHKeySource annotation with the following format: kind/name/key . As for the user-data, kind is one of configmap , secret , name is the name of resource and key is the key name in that resource containing SSH keys in the same format in VirtletSSHKeys . The key part is optional. When using kind/name annotation (without key ), virtlet will look for the authorized_keys key. As with the user-data VirtletSSHKeys keys are going to be appended to those from VirtletSSHKeySource unless it is set to overwrite them by VirtletCloudInitUserDataOverwrite: \"true\" .","title":"Propagating user-data from Kubernetes objects"},{"location":"reference/cloud-init/#workarounds-for-volume-mounting-and-raw-block-volumes","text":"Currenly Virtlet uses /dev/disk/by-path to mount volumes specified using Virtlet flexvolume driver. There's a problem with this approach however, namely that this depends on udev being used by the guest system. Moreover, some images (e.g. CirrOS example image) may have limited cloud-init support and may not provide proper user-data handling necessary to mount the block devices. To handle the first of this problem, Virtlet uses write_files section of user-data to write a shell script named /etc/cloud/mount-volumes.sh (the script uses /bin/sh ) that performs the necessary volume mounts using sysfs to look up the proper device names under /dev . The script also checks whether the volumes are already mounted so as not to mount them several times (so it's idempotent). Another slightly related problem is that when raw block volumes are used, symlinks must be created inside the VM that lead to the proper device files under /dev . This is done by means of /etc/cloud/symlink-devs.sh script that's injected via cloud-init. It's also automatically added to the runcmd section, so it gets executed on the first VM boot and linked from /var/lib/cloud/scripts/per-boot/ so that it gets executed on subsequent VM boots, too. In any case, this script gets executed after mounts are processed, so Virtlet updates mounts entries in the user-specified user-data that point to the raw block volume devices to make them work as if the symlinks were in place during the mounting. To deal with the systems without proper user-data support such as CirrOS, Virtlet makes it possible to specify @virtlet-mount-script@ inside VirtletCloudInitUserDataScript annotation. This string will be replaced with the content that's normally written to /etc/cloud/symlink-devs.sh and /etc/cloud/mount-volumes.sh (in this order). Given that the script starts with #!/bin/sh , you can use the following construct in the pod annotations: VirtletCloudInitUserDataScript: @virtlet-mount-script@","title":"Workarounds for volume mounting and raw block volumes"},{"location":"reference/cloud-init/#additional-links","text":"These links may help to understand some basics about cloud-init: Cloud-init documentation Booting cloud images with libvirt Install cloud-init on Ubuntu and use locally... NoCloud EC2 instance metadata Automating Openstack with cloud init run a script on VM's first boot Create a Linux Lab on KVM Using Cloud Images","title":"Additional links"},{"location":"reference/config/","text":"Using per-node configuration In order to use per-node configuration, you need to create Virtlet CRD definitions before you deploy Virtlet: virtletctl gen --crd | kubectl apply -f - After that, you can add one or more Virtlet configuration mappings: kubectl apply -f - EOF --- apiVersion: virtlet.k8s/v1 kind: VirtletConfigMapping metadata: name: test namespace: kube-system spec: nodeName: kube-node-2 priority: 1 config: disableKVM: true logLevel: 5 rawDevices: sda*,loop* --- apiVersion: virtlet.k8s/v1 kind: VirtletConfigMapping metadata: name: test-labels namespace: kube-system spec: nodeSelector: foo: bar priority: 10 config: logLevel: 3 EOF You can delete Virtlet pods to have them restarted and pick up the changes after you add, remove or modify the configuration mappings. All the config mappings must reside in kube-system namespace. Each mapping can specify nodeSelector or nodeName to target a subset of the nodes. If neither nodeSelector nor nodeName is specified, the mapping will target all the nodes. If the several mappings apply to the node, their order is determined by priority value, with mappings with higher value of priority taking precendence. config specifies the list of configuration fields (see the full table of config fields in the next section of this document). In the example above, for the node named kube-node-2 , KVM is disabled, log level is set to 5 and the raw device list is set to sda*,loop* . For nodes with foo=bar label, the log level is overridden and set to 3. If kube-node-2 node has foo=bar label, the log level will be set to 3 there as the label-based mapping has higher priority. Configuration options summary The table below lists all the Virtlet configuration fields, with their corresponding command line flags and environment variables that are handled by Virtlet binary. Note that you may only need these environment variables and configuration flags if you're not using the standard Virtlet deployment YAML as generated by virtletctl gen . Description Config field Default value Type Command line flag / Env Path to fd server socket fdServerSocketPath /var/lib/virtlet/tapfdserver.sock string --fd-server-socket-path / VIRTLET_FD_SERVER_SOCKET_PATH Path to the virtlet database databasePath /var/lib/virtlet/virtlet.db string --database-path / VIRTLET_DATABASE_PATH Image download protocol. Can be https or http downloadProtocol https string --image-download-protocol / VIRTLET_DOWNLOAD_PROTOCOL Image directory imageDir /var/lib/virtlet/images string --image-dir / VIRTLET_IMAGE_DIR Image name translation configs directory imageTranslationConfigsDir /etc/virtlet/images string --image-translation-configs-dir / VIRTLET_IMAGE_TRANSLATIONS_DIR Libvirt connection URI libvirtURI qemu:///system string --libvirt-uri / VIRTLET_LIBVIRT_URI Comma separated list of raw device glob patterns which VMs can access (without '/dev/' prefix) rawDevices loop* string --raw-devices / VIRTLET_RAW_DEVICES The path to UNIX domain socket for CRI service to listen on criSocketPath /run/virtlet.sock string --listen / VIRTLET_CRI_SOCKET_PATH Display logging and the streamer disableLogging false boolean --disable-logging / VIRTLET_DISABLE_LOGGING Forcibly disable KVM support disableKVM false boolean --disable-kvm / VIRTLET_DISABLE_KVM Enable SR-IOV support enableSriov false boolean --enable-sriov / VIRTLET_SRIOV_SUPPORT Path to CNI plugin binaries cniPluginDir /opt/cni/bin string --cni-bin-dir / VIRTLET_CNI_PLUGIN_DIR Path to the CNI configuration directory cniConfigDir /etc/cni/net.d string --cni-conf-dir / VIRTLET_CNI_CONFIG_DIR Calico subnet size to use calicoSubnetSize 24 integer --calico-subnet-size / VIRTLET_CALICO_SUBNET Enable regexp image name translation enableRegexpImageTranslation true boolean --enable-regexp-image-translation / IMAGE_REGEXP_TRANSLATION CPU model to use in libvirt domain definition (libvirt's default value will be used if not set) cpuModel string --cpu-model / VIRTLET_CPU_MODEL configurable port to the virtlet server streamPort 10010 integer --stream-port / VIRTLET_STREAM_PORT Pod's root dir in kubelet kubeletRootDir /var/lib/kubelet/pods string --kubelet-root-dir / KUBELET_ROOT_DIR Log level to use logLevel 1 integer --v / VIRTLET_LOGLEVEL Only the following config fields mentioned in this table can be used with standard Virtlet deployment YAML: downloadProtocol , rawDevices , disableKVM , enableSriov , calicoSubnetSize , enableRegexpImageTranslation , cpuModel and logLevel . Other options may need adjusting the YAML to change the paths of volume mounts. disableLogging option is intended for debugging purposes only.","title":"Configuration"},{"location":"reference/config/#using-per-node-configuration","text":"In order to use per-node configuration, you need to create Virtlet CRD definitions before you deploy Virtlet: virtletctl gen --crd | kubectl apply -f - After that, you can add one or more Virtlet configuration mappings: kubectl apply -f - EOF --- apiVersion: virtlet.k8s/v1 kind: VirtletConfigMapping metadata: name: test namespace: kube-system spec: nodeName: kube-node-2 priority: 1 config: disableKVM: true logLevel: 5 rawDevices: sda*,loop* --- apiVersion: virtlet.k8s/v1 kind: VirtletConfigMapping metadata: name: test-labels namespace: kube-system spec: nodeSelector: foo: bar priority: 10 config: logLevel: 3 EOF You can delete Virtlet pods to have them restarted and pick up the changes after you add, remove or modify the configuration mappings. All the config mappings must reside in kube-system namespace. Each mapping can specify nodeSelector or nodeName to target a subset of the nodes. If neither nodeSelector nor nodeName is specified, the mapping will target all the nodes. If the several mappings apply to the node, their order is determined by priority value, with mappings with higher value of priority taking precendence. config specifies the list of configuration fields (see the full table of config fields in the next section of this document). In the example above, for the node named kube-node-2 , KVM is disabled, log level is set to 5 and the raw device list is set to sda*,loop* . For nodes with foo=bar label, the log level is overridden and set to 3. If kube-node-2 node has foo=bar label, the log level will be set to 3 there as the label-based mapping has higher priority.","title":"Using per-node configuration"},{"location":"reference/config/#configuration-options-summary","text":"The table below lists all the Virtlet configuration fields, with their corresponding command line flags and environment variables that are handled by Virtlet binary. Note that you may only need these environment variables and configuration flags if you're not using the standard Virtlet deployment YAML as generated by virtletctl gen . Description Config field Default value Type Command line flag / Env Path to fd server socket fdServerSocketPath /var/lib/virtlet/tapfdserver.sock string --fd-server-socket-path / VIRTLET_FD_SERVER_SOCKET_PATH Path to the virtlet database databasePath /var/lib/virtlet/virtlet.db string --database-path / VIRTLET_DATABASE_PATH Image download protocol. Can be https or http downloadProtocol https string --image-download-protocol / VIRTLET_DOWNLOAD_PROTOCOL Image directory imageDir /var/lib/virtlet/images string --image-dir / VIRTLET_IMAGE_DIR Image name translation configs directory imageTranslationConfigsDir /etc/virtlet/images string --image-translation-configs-dir / VIRTLET_IMAGE_TRANSLATIONS_DIR Libvirt connection URI libvirtURI qemu:///system string --libvirt-uri / VIRTLET_LIBVIRT_URI Comma separated list of raw device glob patterns which VMs can access (without '/dev/' prefix) rawDevices loop* string --raw-devices / VIRTLET_RAW_DEVICES The path to UNIX domain socket for CRI service to listen on criSocketPath /run/virtlet.sock string --listen / VIRTLET_CRI_SOCKET_PATH Display logging and the streamer disableLogging false boolean --disable-logging / VIRTLET_DISABLE_LOGGING Forcibly disable KVM support disableKVM false boolean --disable-kvm / VIRTLET_DISABLE_KVM Enable SR-IOV support enableSriov false boolean --enable-sriov / VIRTLET_SRIOV_SUPPORT Path to CNI plugin binaries cniPluginDir /opt/cni/bin string --cni-bin-dir / VIRTLET_CNI_PLUGIN_DIR Path to the CNI configuration directory cniConfigDir /etc/cni/net.d string --cni-conf-dir / VIRTLET_CNI_CONFIG_DIR Calico subnet size to use calicoSubnetSize 24 integer --calico-subnet-size / VIRTLET_CALICO_SUBNET Enable regexp image name translation enableRegexpImageTranslation true boolean --enable-regexp-image-translation / IMAGE_REGEXP_TRANSLATION CPU model to use in libvirt domain definition (libvirt's default value will be used if not set) cpuModel string --cpu-model / VIRTLET_CPU_MODEL configurable port to the virtlet server streamPort 10010 integer --stream-port / VIRTLET_STREAM_PORT Pod's root dir in kubelet kubeletRootDir /var/lib/kubelet/pods string --kubelet-root-dir / KUBELET_ROOT_DIR Log level to use logLevel 1 integer --v / VIRTLET_LOGLEVEL Only the following config fields mentioned in this table can be used with standard Virtlet deployment YAML: downloadProtocol , rawDevices , disableKVM , enableSriov , calicoSubnetSize , enableRegexpImageTranslation , cpuModel and logLevel . Other options may need adjusting the YAML to change the paths of volume mounts. disableLogging option is intended for debugging purposes only.","title":"Configuration options summary"},{"location":"reference/diagnostics/","text":"Direct invocation The most basic diagnostics command is virtletctl diag dump : $ virtletctl diag out/ $ ls -lR out total 0 drwxr-xr-x 3 user wheel 96 Jul 11 01:56 nodes out/nodes: total 0 drwxr-xr-x 12 user wheel 384 Jul 11 01:56 kube-node-1 out/nodes/kube-node-1: total 5352 -rwxr-xr-x 1 user wheel 1276000 Jul 11 01:56 criproxy.log -rwxr-xr-x 1 user wheel 1787 Jul 11 01:56 ip-a.txt -rwxr-xr-x 1 user wheel 322 Jul 11 01:56 ip-r.txt drwxr-xr-x 3 user wheel 96 Jul 11 01:56 libvirt-logs drwxr-xr-x 5 user wheel 160 Jul 11 01:56 libvirt-xml -rwxr-xr-x 1 user wheel 9964 Jul 11 01:56 metadata.txt -rwxr-xr-x 1 user wheel 1443 Jul 11 01:56 netns.txt -rwxr-xr-x 1 user wheel 9217 Jul 11 02:56 psaux.txt -rwxr-xr-x 1 user wheel 18214 Jul 11 01:56 stack.log -rwxr-xr-x 1 user wheel 64314 Jul 11 01:56 virtlet-pod-libvirt.log -rwxr-xr-x 1 user wheel 1349763 Jul 11 01:56 virtlet-pod-virtlet.log out/nodes/kube-node-1/libvirt-logs: total 8 -rwxr-xr-x 1 user wheel 2172 Jul 11 01:56 virtlet-1b2261ca-7ed6-cirros-vm.log out/nodes/kube-node-1/libvirt-xml: total 24 -rwxr-xr-x 1 user wheel 3511 Jul 11 01:56 domain-virtlet-1b2261ca-7ed6-cirros-vm.xml -rwxr-xr-x 1 user wheel 445 Jul 11 01:56 pool-volumes.xml -rwxr-xr-x 1 user wheel 1041 Jul 11 01:56 volume-virtlet_root_1b2261ca-7ed6-58e7-58de-0eef2c9d5320.xml The following files and directories are produced for each Kubernetes node that runs Virtlet: criproxy.log - the logs of CRI Proxy's systemd unit ip-a.txt - the output of ip a on the node ip-r.txt - the output of ip r on the node metadata.txt - the contents of Virtlet's internal metadata db in a text form netns.txt - the output of ip a and ip r for each network namespace that's managed by Virtlet psaux.txt - the output of ps aux command on the node stack.log - the dump of Go stack of Virtlet process virtlet-pod-libvirt.log - the log of Virtlet pod's libvirt container virtlet-pod-virtlet.log - the log of Virtlet pod's virtlet container livirt-logs - a directory with libvirt/QEMU logs for each domain libvirt-xml - the dumps of all the domains, storage pools and storage volumes in libvirt It's also possible to dump Virtlet diagnostics as JSON to stdout using virtletctl diag dump --json . The JSON file can be subsequently unpacked into the aforementioned directory structure using virtletctl diag unpack . Sonobuoy Virtlet diagnostics can be run as a Sonobuoy plugin. Unfortunately, right now Sonobuoy's plugin support is somewhat limited . Because of that problem, Sonobuoy run must be done in two phases, first generating YAML and then using virtletctl to patch it (inject Virtlet sonobuoy plugin): $ cat sonobuoy.json { plugins : [ { name : virtlet } ] } $ sonobuoy gen --config sonobuoy.json --e2e-focus nosuchtest | virtletctl diag sonobuoy | kubectl apply -f - $ # wait till sonobuoy run is complete $ sonobuoy status PLUGIN STATUS COUNT virtlet complete 1 Sonobuoy has completed. Use `sonobuoy retrieve` to get results. $ sonobuoy retrieve The diagnostics results are placed under plugins/virtlet/results and can be unpacked using virtletctl diag unpack : $ virtletctl diag unpack out/ sonobuoy_output_dir/plugins/virtlet/results","title":"Diagnostics"},{"location":"reference/diagnostics/#direct-invocation","text":"The most basic diagnostics command is virtletctl diag dump : $ virtletctl diag out/ $ ls -lR out total 0 drwxr-xr-x 3 user wheel 96 Jul 11 01:56 nodes out/nodes: total 0 drwxr-xr-x 12 user wheel 384 Jul 11 01:56 kube-node-1 out/nodes/kube-node-1: total 5352 -rwxr-xr-x 1 user wheel 1276000 Jul 11 01:56 criproxy.log -rwxr-xr-x 1 user wheel 1787 Jul 11 01:56 ip-a.txt -rwxr-xr-x 1 user wheel 322 Jul 11 01:56 ip-r.txt drwxr-xr-x 3 user wheel 96 Jul 11 01:56 libvirt-logs drwxr-xr-x 5 user wheel 160 Jul 11 01:56 libvirt-xml -rwxr-xr-x 1 user wheel 9964 Jul 11 01:56 metadata.txt -rwxr-xr-x 1 user wheel 1443 Jul 11 01:56 netns.txt -rwxr-xr-x 1 user wheel 9217 Jul 11 02:56 psaux.txt -rwxr-xr-x 1 user wheel 18214 Jul 11 01:56 stack.log -rwxr-xr-x 1 user wheel 64314 Jul 11 01:56 virtlet-pod-libvirt.log -rwxr-xr-x 1 user wheel 1349763 Jul 11 01:56 virtlet-pod-virtlet.log out/nodes/kube-node-1/libvirt-logs: total 8 -rwxr-xr-x 1 user wheel 2172 Jul 11 01:56 virtlet-1b2261ca-7ed6-cirros-vm.log out/nodes/kube-node-1/libvirt-xml: total 24 -rwxr-xr-x 1 user wheel 3511 Jul 11 01:56 domain-virtlet-1b2261ca-7ed6-cirros-vm.xml -rwxr-xr-x 1 user wheel 445 Jul 11 01:56 pool-volumes.xml -rwxr-xr-x 1 user wheel 1041 Jul 11 01:56 volume-virtlet_root_1b2261ca-7ed6-58e7-58de-0eef2c9d5320.xml The following files and directories are produced for each Kubernetes node that runs Virtlet: criproxy.log - the logs of CRI Proxy's systemd unit ip-a.txt - the output of ip a on the node ip-r.txt - the output of ip r on the node metadata.txt - the contents of Virtlet's internal metadata db in a text form netns.txt - the output of ip a and ip r for each network namespace that's managed by Virtlet psaux.txt - the output of ps aux command on the node stack.log - the dump of Go stack of Virtlet process virtlet-pod-libvirt.log - the log of Virtlet pod's libvirt container virtlet-pod-virtlet.log - the log of Virtlet pod's virtlet container livirt-logs - a directory with libvirt/QEMU logs for each domain libvirt-xml - the dumps of all the domains, storage pools and storage volumes in libvirt It's also possible to dump Virtlet diagnostics as JSON to stdout using virtletctl diag dump --json . The JSON file can be subsequently unpacked into the aforementioned directory structure using virtletctl diag unpack .","title":"Direct invocation"},{"location":"reference/diagnostics/#sonobuoy","text":"Virtlet diagnostics can be run as a Sonobuoy plugin. Unfortunately, right now Sonobuoy's plugin support is somewhat limited . Because of that problem, Sonobuoy run must be done in two phases, first generating YAML and then using virtletctl to patch it (inject Virtlet sonobuoy plugin): $ cat sonobuoy.json { plugins : [ { name : virtlet } ] } $ sonobuoy gen --config sonobuoy.json --e2e-focus nosuchtest | virtletctl diag sonobuoy | kubectl apply -f - $ # wait till sonobuoy run is complete $ sonobuoy status PLUGIN STATUS COUNT virtlet complete 1 Sonobuoy has completed. Use `sonobuoy retrieve` to get results. $ sonobuoy retrieve The diagnostics results are placed under plugins/virtlet/results and can be unpacked using virtletctl diag unpack : $ virtletctl diag unpack out/ sonobuoy_output_dir/plugins/virtlet/results","title":"Sonobuoy"},{"location":"reference/images/","text":"VM Image Handling Virtlet supports QCOW2 format for VM images. The image is specified in the image field of the container definition and must have virtlet.cloud/ prefix. If no image name translation is specified, the URL for the QCOW2 file is constructed by prepending https:// (the default) or http:// to the rest of the image name of virtlet.cloud/ prefix, with any image tags stripped. The protocol to use is controlled via downloadProtocol config option . ImagePullSecrets are not supported at the moment, but you can use image name translation to use client TLS certificates or specify user:password@ as a part of the URL. An example of container definition: containers: - name: test-vm image: download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img Restrictions and pitfalls Image names are subject to the strict validation rules that normally applied to the docker image names. Thus one cannot just put arbitrary URL into the image name. In particular, image names cannot have capital letters, colons and some other characters that are commonly found in the URLs. Using image name with invalid characters is a common reason for VM creation failure with non-obvious error status. In order to overcome these limitations, Virtlet provides image name translation that allows to use alias name for the image and define how this alias translates into the URL along with additional transport options elsewhere. Image Name Translation By default, the image URL is encoded into image name as described above . However, due to the strict rules for the image name format such approach has number of significant restrictions: Colon cannot appear in the name. Thus the URL cannot include scheme part ( http:// or https:// ). As a consequence it becomes impossible use images that have scheme that differs from configured default. For the same reasons it's impossible to use URLs that include queries, authentication credentials or a port number. The URL must be all lower-case which works well for the domain part, but may not be acceptable for the path part. To overcome these limitations, Virtlet provides a mechanism for image name translation. The idea is that image can be identified by some abstract ID rather than URL. Virtlet then will map this ID to arbitrary URL using special translation table that specifies rules for image name translation. Thus instead of virtlet.cloud/example.net/path/to/my.qcow2 one would use virtlet.cloud/my-image and put a mapping that says that my-image must be translated to http://example.net/path/to/my.qcow2 into translation table. Here and below we assume that CRI Proxy is used. Otherwise, the virtlet.cloud/ prefix is not needed. Translation configs The translation table is built from arbitrary number of translation configs. The config has the following format: prefix: my-prefix translations: - name: cirros url: https://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img - name: ubuntu/16.04 url: https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img - regexp: 'cirros/(\\d\\.\\d\\.\\d)' url: 'https://download.cirros-cloud.net/$1/cirros-$1-x86_64-disk.img' - regexp: 'centos/(\\d+)-(\\d+)' url: 'https://cloud.centos.org/centos/$1/images/CentOS-$1-x86_64-GenericCloud-$2.qcow2' The prefix is optional and may be omitted. In example above the image name virtlet.cloud/my-prefix/cirros is going to be translated into https://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img , but virtlet.cloud/cirros won't be unless there is also a translation config without a prefix (or with empty string prefix). In the later case, the cirros part will be treated as an URL (the default behavior without translations). There are two types of translations: those that map a fixed image name and those that map set of names identified by regexp expression. In the later case, URL can be generalized by using regexp sub-matches through the $n syntax. In example above, virtlet.cloud/my-prefix/centos/7-01 is going to be translated to https://cloud.centos.org/centos/$1/images/CentOS-7-x86_64-GenericCloud-01.qcow2 . The regexp translations are only available when Virtlet is run with IMAGE_REGEXP_TRANSLATION environment variable set to a non-empty value, which is not the case by default. Fixed name translations has a higher precedence than regexp ones. Thus for ambiguous names, fixed name translations are always preferred. Creating translation configs There are two ways how translation configs can be delivered to Virtlet: Through static YAML files Through custom Kubernetes resource VirtletImageMapping In the first case the translation configs are read from the yaml files from a directory in the virtlet container. There are many ways, how files can be put into virtlet container. Default Virtlet setup uses ConfigMap-based volume to mount deploy/images.yaml into /etc/virtlet/images path inside the Virtlet container. The path is provide to Virtlet through the imageTranslationConfigsDir config option . The flag is optional, and, when omitted, completely disables file-based translation configs. With the second method, the configs are provided through custom Kubernetes resource VirtletImageMapping which looks as following: apiVersion: virtlet.k8s/v1 kind: VirtletImageMapping metadata: name: primary namespace: kube-system spec: prefix: translations: - ... - ... where a translation config is placed into spec field and wrapped with usual Kubernetes metadata. One can use kubectl apply -f mappings.yaml to create such resources. But for this to be possible VirtletImageMapping resource kind must be registered in Kubernetes. Virtlet does it on the first run. This such mappings cannot be created in the Kubernetes cluster that never had Virtlet running. There can be any number of VirtletImageMapping resource. However, currently all such mappings must be in the kube-system namespace. VirtletImageMapping resource have a precedence over file-based configs for ambiguous image names. Thus it is convenient to put defaults into static config files and then override them with VirtletImageMapping resources when needed. Configure HTTP transport for image download By default, the image downloader uses default transport settings: system-wide CA certificates for HTTPS URLs, up to 9 redirects and proxy from the HTTP_PROXY / HTTPS_PROXY environment variables. However, with image translation configs it is possible to override these default and provide custom transport configuration. Transport settings are grouped into profiles, each with the name and bunch of configuration settings. Each translation rule may optionally have transport attribute set to profile name to be used for the image URL of that rule. Below is an example of translation config that has all possible transport settings though all of them are optional: translations: - name: mySmallImage url: https://my.host.loc/small.qcow2 transport: my-server - name: myImage url: https://my.host.loc/big.qcow2 transport: my-server transports: my-server: timeout: 30000 # in ms. 0 = no timeout (default) maxRedirects: 1 # at most 1 redirect allowed (i.e. 2 HTTP requests). null or missing value = any number of redirects proxy: http://my-proxy.loc:8080 tls: # optional TLS settings. Use default system settings when not specified certificates: # there can be any mumber of certificates. Both CA and client certificates are put here - cert: | -----BEGIN CERTIFICATE----- # CA PEM block goes here # CA certificates are recognized by IsCA:TRUE flag in the certificate. Private key is not needed in this case # CA certificates are appended to the Linux system-wide list -----END CERTIFICATE----- - cert: | -----BEGIN CERTIFICATE----- # Client-based authentication certificate PEM block goes here # There can be several certificates put together if they share a single key -----END CERTIFICATE----- key: | -----BEGIN RSA PRIVATE KEY----- # PEM-encoded private key # for certificate-based client authentication private key must be present # Also the key is not required if it already contained in the cert PEM -----END RSA PRIVATE KEY----- serverName: my.host.com # because the certificate is for .com but we're connecting to .loc insecure: false # when true, no server certificate validation is going to be performed When no transport profile is specified for translation rule, the default system settings are used. However, since the default value for transport attribute is an empty string, defining profile with empty name can be used to override this default for all images in that particular config: translations: - name: mySmallImage url: https://my.host.loc/small.qcow2 - name: myImage url: https://my.host.loc/big.qcow2 transports: : proxy: http://my-proxy.loc:8080 # proxy for all images without explicit transport name Of course, the same settings can be put into VirtletImageMapping objects: apiVersion: virtlet.k8s/v1 kind: VirtletImageMapping metadata: name: primary namespace: kube-system spec: translations: - name: mySmallImage url: https://my.host.loc/small.qcow2 - name: myImage url: https://my.host.loc/big.qcow2 transports: : proxy: http://my-proxy.loc:8080 # proxy for all images without explicit transport name The details of Virtlet image storage Virtlet uses filesystem-based image store for the VM images. The images are stored like this: /var/lib/virtlet/images links/ example.com%whatever%etc - ../data/2d711642b726b04401627ca9fbac32f5c8530fb1903cc4db02258717921a4881 example.com%same%image - ../data/2d711642b726b04401627ca9fbac32f5c8530fb1903cc4db02258717921a4881 anotherimg - ../data/a1fce4363854ff888cff4b8e7875d600c2682390412a8cf79b37d0b11148b0fa data/ 2d711642b726b04401627ca9fbac32f5c8530fb1903cc4db02258717921a4881 a1fce4363854ff888cff4b8e7875d600c2682390412a8cf79b37d0b11148b0fa The files are downloaded to data/ . File names correspond to SHA256 hashes of their content. The images are pulled upon PullImage gRPC request made by kubelet. Files are named part_SOME_RANDOM_STRING while being downloaded. After the download finishes, SHA256 hash is calculated to be used as the data file name, and if the file with that name already exists, the newly downloaded file is removed, otherwise it's renamed to that SHA256 digest string. In both cases a symbolic link is created with the name equal to docker image name but with / replaced by % , with the link target being the matching data file. The image store performs GC upon Virtlet startup, which consists of removing any part_* files and those files in data/ which have no symlinks leading to them aren't being used by any containers. The VMs are started from QCOW2 volumes which use the boot images as backing store files. The images are stored under /var/lib/libvirt/images/data . VM volumes are stored in \" volumes \" libvirt pool under /var/lib/virtlet/volumes during the VM execution time and are automatically garbage collected by Virtlet after stopping VM pod environment (sandbox). Note: Virtlet currently ignores image tags, but their meaning may change in future, so it\u2019s better not to set them for VM pods. If there\u2019s no tag provided in the image specification kubelet defaults to imagePullPolicy: Always , which means that the image is always redownloaded when the pod is created. In order to make pod creation faster and more reliable, we set in examples imagePullPolicy to IfNotPresent so a previously downloaded image is reused if there is one in Virtlet\u2019s image store.","title":"VM Image Handling"},{"location":"reference/images/#vm-image-handling","text":"Virtlet supports QCOW2 format for VM images. The image is specified in the image field of the container definition and must have virtlet.cloud/ prefix. If no image name translation is specified, the URL for the QCOW2 file is constructed by prepending https:// (the default) or http:// to the rest of the image name of virtlet.cloud/ prefix, with any image tags stripped. The protocol to use is controlled via downloadProtocol config option . ImagePullSecrets are not supported at the moment, but you can use image name translation to use client TLS certificates or specify user:password@ as a part of the URL. An example of container definition: containers: - name: test-vm image: download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img","title":"VM Image Handling"},{"location":"reference/images/#restrictions-and-pitfalls","text":"Image names are subject to the strict validation rules that normally applied to the docker image names. Thus one cannot just put arbitrary URL into the image name. In particular, image names cannot have capital letters, colons and some other characters that are commonly found in the URLs. Using image name with invalid characters is a common reason for VM creation failure with non-obvious error status. In order to overcome these limitations, Virtlet provides image name translation that allows to use alias name for the image and define how this alias translates into the URL along with additional transport options elsewhere.","title":"Restrictions and pitfalls"},{"location":"reference/images/#image-name-translation","text":"By default, the image URL is encoded into image name as described above . However, due to the strict rules for the image name format such approach has number of significant restrictions: Colon cannot appear in the name. Thus the URL cannot include scheme part ( http:// or https:// ). As a consequence it becomes impossible use images that have scheme that differs from configured default. For the same reasons it's impossible to use URLs that include queries, authentication credentials or a port number. The URL must be all lower-case which works well for the domain part, but may not be acceptable for the path part. To overcome these limitations, Virtlet provides a mechanism for image name translation. The idea is that image can be identified by some abstract ID rather than URL. Virtlet then will map this ID to arbitrary URL using special translation table that specifies rules for image name translation. Thus instead of virtlet.cloud/example.net/path/to/my.qcow2 one would use virtlet.cloud/my-image and put a mapping that says that my-image must be translated to http://example.net/path/to/my.qcow2 into translation table. Here and below we assume that CRI Proxy is used. Otherwise, the virtlet.cloud/ prefix is not needed.","title":"Image Name Translation"},{"location":"reference/images/#translation-configs","text":"The translation table is built from arbitrary number of translation configs. The config has the following format: prefix: my-prefix translations: - name: cirros url: https://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img - name: ubuntu/16.04 url: https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img - regexp: 'cirros/(\\d\\.\\d\\.\\d)' url: 'https://download.cirros-cloud.net/$1/cirros-$1-x86_64-disk.img' - regexp: 'centos/(\\d+)-(\\d+)' url: 'https://cloud.centos.org/centos/$1/images/CentOS-$1-x86_64-GenericCloud-$2.qcow2' The prefix is optional and may be omitted. In example above the image name virtlet.cloud/my-prefix/cirros is going to be translated into https://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img , but virtlet.cloud/cirros won't be unless there is also a translation config without a prefix (or with empty string prefix). In the later case, the cirros part will be treated as an URL (the default behavior without translations). There are two types of translations: those that map a fixed image name and those that map set of names identified by regexp expression. In the later case, URL can be generalized by using regexp sub-matches through the $n syntax. In example above, virtlet.cloud/my-prefix/centos/7-01 is going to be translated to https://cloud.centos.org/centos/$1/images/CentOS-7-x86_64-GenericCloud-01.qcow2 . The regexp translations are only available when Virtlet is run with IMAGE_REGEXP_TRANSLATION environment variable set to a non-empty value, which is not the case by default. Fixed name translations has a higher precedence than regexp ones. Thus for ambiguous names, fixed name translations are always preferred.","title":"Translation configs"},{"location":"reference/images/#creating-translation-configs","text":"There are two ways how translation configs can be delivered to Virtlet: Through static YAML files Through custom Kubernetes resource VirtletImageMapping In the first case the translation configs are read from the yaml files from a directory in the virtlet container. There are many ways, how files can be put into virtlet container. Default Virtlet setup uses ConfigMap-based volume to mount deploy/images.yaml into /etc/virtlet/images path inside the Virtlet container. The path is provide to Virtlet through the imageTranslationConfigsDir config option . The flag is optional, and, when omitted, completely disables file-based translation configs. With the second method, the configs are provided through custom Kubernetes resource VirtletImageMapping which looks as following: apiVersion: virtlet.k8s/v1 kind: VirtletImageMapping metadata: name: primary namespace: kube-system spec: prefix: translations: - ... - ... where a translation config is placed into spec field and wrapped with usual Kubernetes metadata. One can use kubectl apply -f mappings.yaml to create such resources. But for this to be possible VirtletImageMapping resource kind must be registered in Kubernetes. Virtlet does it on the first run. This such mappings cannot be created in the Kubernetes cluster that never had Virtlet running. There can be any number of VirtletImageMapping resource. However, currently all such mappings must be in the kube-system namespace. VirtletImageMapping resource have a precedence over file-based configs for ambiguous image names. Thus it is convenient to put defaults into static config files and then override them with VirtletImageMapping resources when needed.","title":"Creating translation configs"},{"location":"reference/images/#configure-http-transport-for-image-download","text":"By default, the image downloader uses default transport settings: system-wide CA certificates for HTTPS URLs, up to 9 redirects and proxy from the HTTP_PROXY / HTTPS_PROXY environment variables. However, with image translation configs it is possible to override these default and provide custom transport configuration. Transport settings are grouped into profiles, each with the name and bunch of configuration settings. Each translation rule may optionally have transport attribute set to profile name to be used for the image URL of that rule. Below is an example of translation config that has all possible transport settings though all of them are optional: translations: - name: mySmallImage url: https://my.host.loc/small.qcow2 transport: my-server - name: myImage url: https://my.host.loc/big.qcow2 transport: my-server transports: my-server: timeout: 30000 # in ms. 0 = no timeout (default) maxRedirects: 1 # at most 1 redirect allowed (i.e. 2 HTTP requests). null or missing value = any number of redirects proxy: http://my-proxy.loc:8080 tls: # optional TLS settings. Use default system settings when not specified certificates: # there can be any mumber of certificates. Both CA and client certificates are put here - cert: | -----BEGIN CERTIFICATE----- # CA PEM block goes here # CA certificates are recognized by IsCA:TRUE flag in the certificate. Private key is not needed in this case # CA certificates are appended to the Linux system-wide list -----END CERTIFICATE----- - cert: | -----BEGIN CERTIFICATE----- # Client-based authentication certificate PEM block goes here # There can be several certificates put together if they share a single key -----END CERTIFICATE----- key: | -----BEGIN RSA PRIVATE KEY----- # PEM-encoded private key # for certificate-based client authentication private key must be present # Also the key is not required if it already contained in the cert PEM -----END RSA PRIVATE KEY----- serverName: my.host.com # because the certificate is for .com but we're connecting to .loc insecure: false # when true, no server certificate validation is going to be performed When no transport profile is specified for translation rule, the default system settings are used. However, since the default value for transport attribute is an empty string, defining profile with empty name can be used to override this default for all images in that particular config: translations: - name: mySmallImage url: https://my.host.loc/small.qcow2 - name: myImage url: https://my.host.loc/big.qcow2 transports: : proxy: http://my-proxy.loc:8080 # proxy for all images without explicit transport name Of course, the same settings can be put into VirtletImageMapping objects: apiVersion: virtlet.k8s/v1 kind: VirtletImageMapping metadata: name: primary namespace: kube-system spec: translations: - name: mySmallImage url: https://my.host.loc/small.qcow2 - name: myImage url: https://my.host.loc/big.qcow2 transports: : proxy: http://my-proxy.loc:8080 # proxy for all images without explicit transport name","title":"Configure HTTP transport for image download"},{"location":"reference/images/#the-details-of-virtlet-image-storage","text":"Virtlet uses filesystem-based image store for the VM images. The images are stored like this: /var/lib/virtlet/images links/ example.com%whatever%etc - ../data/2d711642b726b04401627ca9fbac32f5c8530fb1903cc4db02258717921a4881 example.com%same%image - ../data/2d711642b726b04401627ca9fbac32f5c8530fb1903cc4db02258717921a4881 anotherimg - ../data/a1fce4363854ff888cff4b8e7875d600c2682390412a8cf79b37d0b11148b0fa data/ 2d711642b726b04401627ca9fbac32f5c8530fb1903cc4db02258717921a4881 a1fce4363854ff888cff4b8e7875d600c2682390412a8cf79b37d0b11148b0fa The files are downloaded to data/ . File names correspond to SHA256 hashes of their content. The images are pulled upon PullImage gRPC request made by kubelet. Files are named part_SOME_RANDOM_STRING while being downloaded. After the download finishes, SHA256 hash is calculated to be used as the data file name, and if the file with that name already exists, the newly downloaded file is removed, otherwise it's renamed to that SHA256 digest string. In both cases a symbolic link is created with the name equal to docker image name but with / replaced by % , with the link target being the matching data file. The image store performs GC upon Virtlet startup, which consists of removing any part_* files and those files in data/ which have no symlinks leading to them aren't being used by any containers. The VMs are started from QCOW2 volumes which use the boot images as backing store files. The images are stored under /var/lib/libvirt/images/data . VM volumes are stored in \" volumes \" libvirt pool under /var/lib/virtlet/volumes during the VM execution time and are automatically garbage collected by Virtlet after stopping VM pod environment (sandbox). Note: Virtlet currently ignores image tags, but their meaning may change in future, so it\u2019s better not to set them for VM pods. If there\u2019s no tag provided in the image specification kubelet defaults to imagePullPolicy: Always , which means that the image is always redownloaded when the pod is created. In order to make pod creation faster and more reliable, we set in examples imagePullPolicy to IfNotPresent so a previously downloaded image is reused if there is one in Virtlet\u2019s image store.","title":"The details of Virtlet image storage"},{"location":"reference/injecting-files/","text":"Injecting files into the VM Virtlet makes it possible to write set of files to the root filesystem of a VM using Config Map or Secret as a source of data. Key-value conventions ConfigMap or Secret should contain keys and values according to the following convention: entry: content entry_path: encoded/path/in/filesystem entry_encoding: encoding_of_content second_entry: content second_entry_path: encoded/path/in/filesystem second_entry_encoding: encoding_of_content where entry is an arbitrary name, entry_name contains the destination path on the VM root filesystem, and optional entry_encoding denotes the encoding of the file content which can be plain for plain text (for use in ConfigMaps) or base64 (the default). ConfigMap example Create a ConfigMap like this: kubectl apply -f - EOF apiVersion: v1 kind: ConfigMap metadata: name: my-files-set data: locale: LANG= pl_PL.UTF-8 locale_path: /etc/locale.conf locale_encoding: plain host_conf: bXVsdGkgb2ZmCg== host_conf_path: /etc/host.conf EOF and then add it to a pod using the following annotation: ... metadata: ... annotations: VirtletFilesFromDataSource: configmap/my-files-set Secret example Same data as show above can be specified via a Secret : mkdir data echo 'LANG= pl_PL.UTF-8 ' data/locale echo /etc/locale.conf data/locale_path echo multi off data/host_conf echo /etc/host.conf data/host_conf_path kubectl create secret generic my-files-set --from-file=data/ rm -r data/ or recreating above using yaml notation (note: secrets use base64 encoding for each value stored under each key): kubectl apply -f - EOF apiVersion: v1 kind: Secret metadata: name: my-files-set data: locale: TEFORz0icGxfUEwuVVRGLTgiCg== locale_path: L2V0Yy9sb2NhbGUuY29uZgo= host_conf: bXVsdGkgb2ZmCg== host_conf_path: L2V0Yy9ob3N0LmNvbmYK EOF The secret can be injected into the root filesystem like that: ... metadata: ... annotations: VirtletFilesFromDataSource: secret/my-files-set","title":"Injecting Files into the VM"},{"location":"reference/injecting-files/#injecting-files-into-the-vm","text":"Virtlet makes it possible to write set of files to the root filesystem of a VM using Config Map or Secret as a source of data.","title":"Injecting files into the VM"},{"location":"reference/injecting-files/#key-value-conventions","text":"ConfigMap or Secret should contain keys and values according to the following convention: entry: content entry_path: encoded/path/in/filesystem entry_encoding: encoding_of_content second_entry: content second_entry_path: encoded/path/in/filesystem second_entry_encoding: encoding_of_content where entry is an arbitrary name, entry_name contains the destination path on the VM root filesystem, and optional entry_encoding denotes the encoding of the file content which can be plain for plain text (for use in ConfigMaps) or base64 (the default).","title":"Key-value conventions"},{"location":"reference/injecting-files/#configmap-example","text":"Create a ConfigMap like this: kubectl apply -f - EOF apiVersion: v1 kind: ConfigMap metadata: name: my-files-set data: locale: LANG= pl_PL.UTF-8 locale_path: /etc/locale.conf locale_encoding: plain host_conf: bXVsdGkgb2ZmCg== host_conf_path: /etc/host.conf EOF and then add it to a pod using the following annotation: ... metadata: ... annotations: VirtletFilesFromDataSource: configmap/my-files-set","title":"ConfigMap example"},{"location":"reference/injecting-files/#secret-example","text":"Same data as show above can be specified via a Secret : mkdir data echo 'LANG= pl_PL.UTF-8 ' data/locale echo /etc/locale.conf data/locale_path echo multi off data/host_conf echo /etc/host.conf data/host_conf_path kubectl create secret generic my-files-set --from-file=data/ rm -r data/ or recreating above using yaml notation (note: secrets use base64 encoding for each value stored under each key): kubectl apply -f - EOF apiVersion: v1 kind: Secret metadata: name: my-files-set data: locale: TEFORz0icGxfUEwuVVRGLTgiCg== locale_path: L2V0Yy9sb2NhbGUuY29uZgo= host_conf: bXVsdGkgb2ZmCg== host_conf_path: L2V0Yy9ob3N0LmNvbmYK EOF The secret can be injected into the root filesystem like that: ... metadata: ... annotations: VirtletFilesFromDataSource: secret/my-files-set","title":"Secret example"},{"location":"reference/networking/","text":"Virtlet networking principles Virtlet currently uses CNI-based networking, which means you must use --network-plugin=cni for kubelet on Virtlet nodes. Virtlet should work with any CNI plugin which returns correct CNI Result according to the CNI protocol specification. The CNI spec version in use must be 0.3.0 at least. Virtlet expects the CNI plugins to provide veth or SR-IOV interfaces inside the network namespace. For each veth inside the network namespace Virtlet sets up a TAP interface that's passed to the hypervisor process ( QEMU ) and a bridge that's used to connect veth with the TAP interface. The bridge is also used by Virtlet's internal DHCP server which passes the IP configuration to the VM. Each SR-IOV device available in the network namespace will be removed from host visibility and passed to the hypervisor via PCI host passthrough mechanism. Any VLAN IDs set up by the plugin still apply. Supported CNI implementations Virtlet is verified to work correctly with the following CNI implementations: Flannel Calico Weave original and Intel fork of SR-IOV combination of them with CNI-Genie Virtlet may or may not work with CNI implementations that aren't listed here. DHCP-based VM network configuration The network namespace configuration provided by the CNI plugin(s) in use is passed to the VM using Virtlet's internal DHCP server. The DHCP server is started for each CNI-configured network interface, except for SR-IOV interfaces. The DHCP server can only talk to the VM and is not accessible from the cluster network. DHCP server isn't used for SR-IOV devices as they're passed directly to the VM while their links are removed from host. Configuring using Cloud-Init Besides using DHCP server, Virtlet can also pass the network configuration as a part of Cloud-init data. This is the preferred way of setting up SR-IOV devices. The network configuration is added to the cloud-init user-data using Network Config Version 1 format. Note: Cloud-init network configuration is not supported for persistent rootfs for now. Setting up Multiple CNIs Virtlet allows to configure multiple interfaces for VM when all of them are properly described in CNI Result . The supported way to achieve that using CNI plugins is by combining output of their chain using CNI-Genie . Before you proceed, please read the CNI Genie documentation . There are two ways to tell CNI Genie which CNI networks to use: by setting cni: plugins list in the pod annotation by setting the default_plugin option in the CNI Genie configuration file Note: when using Calico plugin, you must specify it as the first one in the plugin chain, or, alternatively, disable the default route setup for other plugins that precede Calico. This is a Calico-specific limitation. Configuring networks CNI plugins are expected to use 0.3.0 version of CNI Result spec or later. Each CNI config must include cniVersion field, with minimum version being 0.3.0, too. Sample configuration Please refer to the detailed documentation that contains an example of configuration files for CNI Genie with Calico being used for the primary interface and Flannel being used for the secondary one. SR-IOV Any SR-IOV devices contained in the CNI result are passed to the VM using PCI host-passthrough. The hardware configuration which is set up by the CNI plugin (MAC address, VLAN tag) is preserved by Virtlet. If a VLAN ID is set it's configured on the host side, so it can't be changed from within the VM to gain unauthorised network access.","title":"Networking"},{"location":"reference/networking/#virtlet-networking-principles","text":"Virtlet currently uses CNI-based networking, which means you must use --network-plugin=cni for kubelet on Virtlet nodes. Virtlet should work with any CNI plugin which returns correct CNI Result according to the CNI protocol specification. The CNI spec version in use must be 0.3.0 at least. Virtlet expects the CNI plugins to provide veth or SR-IOV interfaces inside the network namespace. For each veth inside the network namespace Virtlet sets up a TAP interface that's passed to the hypervisor process ( QEMU ) and a bridge that's used to connect veth with the TAP interface. The bridge is also used by Virtlet's internal DHCP server which passes the IP configuration to the VM. Each SR-IOV device available in the network namespace will be removed from host visibility and passed to the hypervisor via PCI host passthrough mechanism. Any VLAN IDs set up by the plugin still apply.","title":"Virtlet networking principles"},{"location":"reference/networking/#supported-cni-implementations","text":"Virtlet is verified to work correctly with the following CNI implementations: Flannel Calico Weave original and Intel fork of SR-IOV combination of them with CNI-Genie Virtlet may or may not work with CNI implementations that aren't listed here.","title":"Supported CNI implementations"},{"location":"reference/networking/#dhcp-based-vm-network-configuration","text":"The network namespace configuration provided by the CNI plugin(s) in use is passed to the VM using Virtlet's internal DHCP server. The DHCP server is started for each CNI-configured network interface, except for SR-IOV interfaces. The DHCP server can only talk to the VM and is not accessible from the cluster network. DHCP server isn't used for SR-IOV devices as they're passed directly to the VM while their links are removed from host.","title":"DHCP-based VM network configuration"},{"location":"reference/networking/#configuring-using-cloud-init","text":"Besides using DHCP server, Virtlet can also pass the network configuration as a part of Cloud-init data. This is the preferred way of setting up SR-IOV devices. The network configuration is added to the cloud-init user-data using Network Config Version 1 format. Note: Cloud-init network configuration is not supported for persistent rootfs for now.","title":"Configuring using Cloud-Init"},{"location":"reference/networking/#configuring-networks","text":"CNI plugins are expected to use 0.3.0 version of CNI Result spec or later. Each CNI config must include cniVersion field, with minimum version being 0.3.0, too.","title":"Configuring networks"},{"location":"reference/networking/#sample-configuration","text":"Please refer to the detailed documentation that contains an example of configuration files for CNI Genie with Calico being used for the primary interface and Flannel being used for the secondary one.","title":"Sample configuration"},{"location":"reference/networking/#sr-iov","text":"Any SR-IOV devices contained in the CNI result are passed to the VM using PCI host-passthrough. The hardware configuration which is set up by the CNI plugin (MAC address, VLAN tag) is preserved by Virtlet. If a VLAN ID is set it's configured on the host side, so it can't be changed from within the VM to gain unauthorised network access.","title":"SR-IOV"},{"location":"reference/resources/","text":"CPU Model By default, libvirt runs QEMU with a CPU model that doesn't support nested virtualization. It's possible to change this behavior by using VirtletCPUModel: host-model annotation in the pod definition. You can also use cpuModel value in Virtlet config to override the value globally for the cluster or for a particular subset of nodes. If you are familiar with the cpu part in libvirt domain definition, you can use VirtletLibvirtCPUSetting annotation, the value is directly passed to libvirt after reading it from yaml string. It is more flexible than usage of VirtletCPUModel as it allows to provide more detailed configuration. For example: annotations: VirtletLibvirtCPUSetting: | mode: custom model: value: Westmere features: - name: avx policy: disable See cpuSetting for a full example. Resource monitoring on the node As Kubelet uses cAdvisor to collect metrics about running containers and Virtlet doesn't create container per each VM, and instead spawns VMs inside Virtlet container. This leads to all the resource usage being lumped together and ascribed to Virtlet pod. CPU management CPU cgroups facilities: shares - relative value of cpu time assigned, not recommended for using in production as it's hard to predict the actual performance which highly depends on the neighboring cgroups. CFS CPU bandwidth control - period and quota - hard limits. Parent_Period/Quota = Child_1_Period/Quota + .. + Child_N_Period/Quota , where Child_N_Period/Quota = Parent_Period/Quota . K8s CPU allocation: shares are set per container. CFS CPU bandwidth control - period and quota - are set per container. Defaults: In absence of explicitly set values each container has 2 shares set by default. Libvirt CPU allocation: shares is set per each vCPU. period and quota are set per each vCPU. As libvirt imposes limits per each vCPU thread, so actual CPU quota is quota value from the domain definition times the number of vCPUs. More details re reasons of libvirt per vCPU cgroup approach can be found there . emulator_period and emulator_quota denote the limits for emulator threads (those excluding vcpus). At the same time for unlimited domains benchmarks show that these activities may measure up to 40-80% of overall physical CPU usage by QEMU/KVM process running the guest VM. vCPUs per VM - it's commonly recommended to have vCPU count set to 1 (see details in section \"CPU overcommit\" below). Defaults: In absence of explicitly set values each domain has 1024 shares set by default. CPU overcommit It's outlined that linux scheduler doesn't perform well in case of CPU overcommitment and if it's not caused real need (like having multi-core VM to perform build/compile, running application inside that can effectively utilize multiple cores and was designed for parallel processing) and widely recommended to use one vCPU per VM otherwise you can expect performance degradation. It is not recommended to have more than 10 virtual CPUs per physical processor core. Any number of overcommitted virtual CPUs above the number of physical processor cores may cause problems with certain virtualized guests, so it's always up to cluster administrators how to set up number vCPUs per VMs. See more considerations on KVM limitations . Virtlet CPU resources management By default, all VMs are created with 1 vCPU. To change vCPU number for VM-Pod you have to add annotation VirtletVCPUCount with desired number, see examples/cirros-vm.yaml . Due to p.2 in \"Libvirt CPU Allocation\" Virtlet spreads the assigned CPU resource limit equally among VM's vCPU threads. According to p.3 in \"Libvirt CPU Allocation\" Virtlet must set limits for emulator threads(those excluding vcpus). At this time Virtlet doesn't support setting these values, but there are plans to fix this in future. Memory management K8s memory allocation Setting memory limit to 0 or omitting it means there's no memory limit for the container. K8s doesn't support swap on the nodes (for example, k8s creates docker containers with --memory-swappiness=0, see more at https://github.com/kubernetes/kubernetes/issues/7294). Libvirt memory allocation memory - allocated RAM memory at VM boot. memtune= hard_limit - cgroup memory limit on all domain including qemu itself usage. However, it's claimed that such limit should be set accurately . Swap unlimited by default. Memory overcommit Overcommit memory value can reach ~150% of physical RAM amount. This relies on assumption that most processes do not access 100% of their allocated memory all the time. So you can grant guest VMs more RAM than actually is available on the host. However, this strongly depends on memory swap size available on the node and workloads of VMs memory consumptions. For more details check Overcommitting with KVM . Virtlet Memory resources management By default, each VM is assigned 1GB of RAM. To set other value you need set resource memory limit for container, see examples/cirros-vm.yaml . Virtlet generates domain XML with memoryBacking=locked setting to prevent swapping out domain's pages. Future improvements According to 2 and 3 in \"Libvirt CPU Allocation\" we need to invent some rule of setting CFS CPU bandwidth limit spread among QEMU and vCPU threads, so as to make k8s scheduler have right assumptions about the resources allocated on the node. Research how to configure the hard limits on memory for VM pod.","title":"Resource management"},{"location":"reference/resources/#cpu-model","text":"By default, libvirt runs QEMU with a CPU model that doesn't support nested virtualization. It's possible to change this behavior by using VirtletCPUModel: host-model annotation in the pod definition. You can also use cpuModel value in Virtlet config to override the value globally for the cluster or for a particular subset of nodes. If you are familiar with the cpu part in libvirt domain definition, you can use VirtletLibvirtCPUSetting annotation, the value is directly passed to libvirt after reading it from yaml string. It is more flexible than usage of VirtletCPUModel as it allows to provide more detailed configuration. For example: annotations: VirtletLibvirtCPUSetting: | mode: custom model: value: Westmere features: - name: avx policy: disable See cpuSetting for a full example.","title":"CPU Model"},{"location":"reference/resources/#resource-monitoring-on-the-node","text":"As Kubelet uses cAdvisor to collect metrics about running containers and Virtlet doesn't create container per each VM, and instead spawns VMs inside Virtlet container. This leads to all the resource usage being lumped together and ascribed to Virtlet pod.","title":"Resource monitoring on the node"},{"location":"reference/resources/#cpu-management","text":"","title":"CPU management"},{"location":"reference/resources/#cpu-cgroups-facilities","text":"shares - relative value of cpu time assigned, not recommended for using in production as it's hard to predict the actual performance which highly depends on the neighboring cgroups. CFS CPU bandwidth control - period and quota - hard limits. Parent_Period/Quota = Child_1_Period/Quota + .. + Child_N_Period/Quota , where Child_N_Period/Quota = Parent_Period/Quota .","title":"CPU cgroups facilities:"},{"location":"reference/resources/#k8s-cpu-allocation","text":"shares are set per container. CFS CPU bandwidth control - period and quota - are set per container. Defaults: In absence of explicitly set values each container has 2 shares set by default.","title":"K8s CPU allocation:"},{"location":"reference/resources/#libvirt-cpu-allocation","text":"shares is set per each vCPU. period and quota are set per each vCPU. As libvirt imposes limits per each vCPU thread, so actual CPU quota is quota value from the domain definition times the number of vCPUs. More details re reasons of libvirt per vCPU cgroup approach can be found there . emulator_period and emulator_quota denote the limits for emulator threads (those excluding vcpus). At the same time for unlimited domains benchmarks show that these activities may measure up to 40-80% of overall physical CPU usage by QEMU/KVM process running the guest VM. vCPUs per VM - it's commonly recommended to have vCPU count set to 1 (see details in section \"CPU overcommit\" below). Defaults: In absence of explicitly set values each domain has 1024 shares set by default.","title":"Libvirt CPU allocation:"},{"location":"reference/resources/#cpu-overcommit","text":"It's outlined that linux scheduler doesn't perform well in case of CPU overcommitment and if it's not caused real need (like having multi-core VM to perform build/compile, running application inside that can effectively utilize multiple cores and was designed for parallel processing) and widely recommended to use one vCPU per VM otherwise you can expect performance degradation. It is not recommended to have more than 10 virtual CPUs per physical processor core. Any number of overcommitted virtual CPUs above the number of physical processor cores may cause problems with certain virtualized guests, so it's always up to cluster administrators how to set up number vCPUs per VMs. See more considerations on KVM limitations .","title":"CPU overcommit"},{"location":"reference/resources/#virtlet-cpu-resources-management","text":"By default, all VMs are created with 1 vCPU. To change vCPU number for VM-Pod you have to add annotation VirtletVCPUCount with desired number, see examples/cirros-vm.yaml . Due to p.2 in \"Libvirt CPU Allocation\" Virtlet spreads the assigned CPU resource limit equally among VM's vCPU threads. According to p.3 in \"Libvirt CPU Allocation\" Virtlet must set limits for emulator threads(those excluding vcpus). At this time Virtlet doesn't support setting these values, but there are plans to fix this in future.","title":"Virtlet CPU resources management"},{"location":"reference/resources/#memory-management","text":"","title":"Memory management"},{"location":"reference/resources/#k8s-memory-allocation","text":"Setting memory limit to 0 or omitting it means there's no memory limit for the container. K8s doesn't support swap on the nodes (for example, k8s creates docker containers with --memory-swappiness=0, see more at https://github.com/kubernetes/kubernetes/issues/7294).","title":"K8s memory allocation"},{"location":"reference/resources/#libvirt-memory-allocation","text":"memory - allocated RAM memory at VM boot. memtune= hard_limit - cgroup memory limit on all domain including qemu itself usage. However, it's claimed that such limit should be set accurately . Swap unlimited by default.","title":"Libvirt memory allocation"},{"location":"reference/resources/#memory-overcommit","text":"Overcommit memory value can reach ~150% of physical RAM amount. This relies on assumption that most processes do not access 100% of their allocated memory all the time. So you can grant guest VMs more RAM than actually is available on the host. However, this strongly depends on memory swap size available on the node and workloads of VMs memory consumptions. For more details check Overcommitting with KVM .","title":"Memory overcommit"},{"location":"reference/resources/#virtlet-memory-resources-management","text":"By default, each VM is assigned 1GB of RAM. To set other value you need set resource memory limit for container, see examples/cirros-vm.yaml . Virtlet generates domain XML with memoryBacking=locked setting to prevent swapping out domain's pages.","title":"Virtlet Memory resources management"},{"location":"reference/resources/#future-improvements","text":"According to 2 and 3 in \"Libvirt CPU Allocation\" we need to invent some rule of setting CFS CPU bandwidth limit spread among QEMU and vCPU threads, so as to make k8s scheduler have right assumptions about the resources allocated on the node. Research how to configure the hard limits on memory for VM pod.","title":"Future improvements"},{"location":"reference/virtletctl/","text":"virtletctl Virtlet control tool Synopsis virtletctl provides a number of utilities for Virtet-enabled Kubernetes cluster. Subcommands virtletctl diag - Virtlet diagnostics virtletctl gen - Generate Kubernetes YAML for Virtlet deployment virtletctl gendoc - Generate Markdown documentation for the commands virtletctl install - Install virtletctl as a kubectl plugin virtletctl ssh - Connect to a VM pod using ssh virtletctl validate - Make sure the cluster is ready for Virtlet deployment virtletctl version - Display Virtlet version information virtletctl virsh - Execute a virsh command virtletctl vnc - Provide access to the VNC console of a VM pod virtletctl diag Virtlet diagnostics Synopsis Retrieve and unpack Virtlet diagnostics information Subcommands virtletctl diag dump - Dump Virtlet diagnostics information virtletctl diag sonobuoy - Add Virtlet sonobuoy plugin to the sonobuoy output virtletctl diag unpack - Unpack Virtlet diagnostics information virtletctl diag dump Dump Virtlet diagnostics information Synopsis Pull Virtlet diagnostics information from the nodes and dump it as a directory tree or JSON virtletctl diag dump output_dir [flags] Options --json Use JSON output virtletctl diag sonobuoy Add Virtlet sonobuoy plugin to the sonobuoy output Synopsis Find and patch sonobuoy configmap in the yaml that's read from stdin to include Virtlet sonobuoy plugin virtletctl diag sonobuoy [flags] Options --tag string Set virtlet image tag for the plugin virtletctl diag unpack Unpack Virtlet diagnostics information Synopsis Read Virtlet diagnostics information as JSON from stdin and unpacks into a directory tree virtletctl diag unpack output_dir [flags] virtletctl gen Generate Kubernetes YAML for Virtlet deployment Synopsis This command produces YAML suitable for use with kubectl apply -f - virtletctl gen [flags] Options --compat Produce YAML that's compatible with older Kubernetes versions --crd Dump CRD definitions only --dev Development mode for use with kubeadm-dind-cluster --tag string Set virtlet image tag virtletctl gendoc Generate Markdown documentation for the commands Synopsis This command produces documentation for the whole command tree, or the Virtlet configuration data. virtletctl gendoc output_dir [flags] Options --config Produce documentation for Virtlet config virtletctl install Install virtletctl as a kubectl plugin Synopsis This command install virtletctl as a kubectl plugin. After running this command, it becomes possible to run virtletctl via 'kubectl plugin virt'. virtletctl install [flags] virtletctl ssh Connect to a VM pod using ssh Synopsis This command runs ssh and makes it connect to a VM pod. virtletctl ssh [flags] user@pod -- [ssh args...] virtletctl validate Make sure the cluster is ready for Virtlet deployment Synopsis Check configuration of the cluster nodes to make sure they're ready for Virtlet deployment virtletctl validate [flags] virtletctl version Display Virtlet version information Synopsis Display information about virtletctl version and Virtlet versions on the nodes virtletctl version [flags] Options --client Print virtletctl version only -o, --output string One of 'text', 'short', 'yaml' or 'json' (default value: \"text\" ) --short Print just the version number(s) (same as -o short) virtletctl virsh Execute a virsh command Synopsis This command executes libvirt virsh command. A VM pod name in the form @podname is translated to the corresponding libvirt domain name. If @podname is specified, the target k8s node name is inferred automatically based on the information of the VM pod. In case if no @podname is specified, the command is executed on every node and the output for every node is prepended with a line with the node name and corresponding Virtlet pod name. virtletctl virsh [flags] virsh_command -- [virsh_command_args...] Options --node string the name of the target node virtletctl vnc Provide access to the VNC console of a VM pod Synopsis This command forwards a local port to the VNC port used by the specified VM pod. If no local port number is provided, a random available port is picked instead. The port number is displayed after the forwarding is set up, after which the commands enters an endless loop until it's interrupted with Ctrl-C. virtletctl vnc pod [port] [flags] Global options --alsologtostderr log to standard error as well as files --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use -h, --help help for virtletctl --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string Path to the kubeconfig file to use for CLI requests. --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default value: :0 ) --log-dir string If non-empty, write log files in this directory --logtostderr log to standard error instead of files -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default value: \"0\" ) -s, --server string The address and port of the Kubernetes API server --stderrthreshold severity logs at or above this threshold go to stderr (default value: 2 ) --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level log level for V logs --virtlet-runtime string the name of virtlet runtime used in kubernetes.io/target-runtime annotation (default value: \"virtlet.cloud\" ) --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Command Line Tool"},{"location":"reference/virtletctl/#virtletctl","text":"Virtlet control tool Synopsis virtletctl provides a number of utilities for Virtet-enabled Kubernetes cluster. Subcommands virtletctl diag - Virtlet diagnostics virtletctl gen - Generate Kubernetes YAML for Virtlet deployment virtletctl gendoc - Generate Markdown documentation for the commands virtletctl install - Install virtletctl as a kubectl plugin virtletctl ssh - Connect to a VM pod using ssh virtletctl validate - Make sure the cluster is ready for Virtlet deployment virtletctl version - Display Virtlet version information virtletctl virsh - Execute a virsh command virtletctl vnc - Provide access to the VNC console of a VM pod","title":"virtletctl"},{"location":"reference/virtletctl/#virtletctl-diag","text":"Virtlet diagnostics Synopsis Retrieve and unpack Virtlet diagnostics information Subcommands virtletctl diag dump - Dump Virtlet diagnostics information virtletctl diag sonobuoy - Add Virtlet sonobuoy plugin to the sonobuoy output virtletctl diag unpack - Unpack Virtlet diagnostics information","title":"virtletctl diag"},{"location":"reference/virtletctl/#virtletctl-diag-dump","text":"Dump Virtlet diagnostics information Synopsis Pull Virtlet diagnostics information from the nodes and dump it as a directory tree or JSON virtletctl diag dump output_dir [flags] Options --json Use JSON output","title":"virtletctl diag dump"},{"location":"reference/virtletctl/#virtletctl-diag-sonobuoy","text":"Add Virtlet sonobuoy plugin to the sonobuoy output Synopsis Find and patch sonobuoy configmap in the yaml that's read from stdin to include Virtlet sonobuoy plugin virtletctl diag sonobuoy [flags] Options --tag string Set virtlet image tag for the plugin","title":"virtletctl diag sonobuoy"},{"location":"reference/virtletctl/#virtletctl-diag-unpack","text":"Unpack Virtlet diagnostics information Synopsis Read Virtlet diagnostics information as JSON from stdin and unpacks into a directory tree virtletctl diag unpack output_dir [flags]","title":"virtletctl diag unpack"},{"location":"reference/virtletctl/#virtletctl-gen","text":"Generate Kubernetes YAML for Virtlet deployment Synopsis This command produces YAML suitable for use with kubectl apply -f - virtletctl gen [flags] Options --compat Produce YAML that's compatible with older Kubernetes versions --crd Dump CRD definitions only --dev Development mode for use with kubeadm-dind-cluster --tag string Set virtlet image tag","title":"virtletctl gen"},{"location":"reference/virtletctl/#virtletctl-gendoc","text":"Generate Markdown documentation for the commands Synopsis This command produces documentation for the whole command tree, or the Virtlet configuration data. virtletctl gendoc output_dir [flags] Options --config Produce documentation for Virtlet config","title":"virtletctl gendoc"},{"location":"reference/virtletctl/#virtletctl-install","text":"Install virtletctl as a kubectl plugin Synopsis This command install virtletctl as a kubectl plugin. After running this command, it becomes possible to run virtletctl via 'kubectl plugin virt'. virtletctl install [flags]","title":"virtletctl install"},{"location":"reference/virtletctl/#virtletctl-ssh","text":"Connect to a VM pod using ssh Synopsis This command runs ssh and makes it connect to a VM pod. virtletctl ssh [flags] user@pod -- [ssh args...]","title":"virtletctl ssh"},{"location":"reference/virtletctl/#virtletctl-validate","text":"Make sure the cluster is ready for Virtlet deployment Synopsis Check configuration of the cluster nodes to make sure they're ready for Virtlet deployment virtletctl validate [flags]","title":"virtletctl validate"},{"location":"reference/virtletctl/#virtletctl-version","text":"Display Virtlet version information Synopsis Display information about virtletctl version and Virtlet versions on the nodes virtletctl version [flags] Options --client Print virtletctl version only -o, --output string One of 'text', 'short', 'yaml' or 'json' (default value: \"text\" ) --short Print just the version number(s) (same as -o short)","title":"virtletctl version"},{"location":"reference/virtletctl/#virtletctl-virsh","text":"Execute a virsh command Synopsis This command executes libvirt virsh command. A VM pod name in the form @podname is translated to the corresponding libvirt domain name. If @podname is specified, the target k8s node name is inferred automatically based on the information of the VM pod. In case if no @podname is specified, the command is executed on every node and the output for every node is prepended with a line with the node name and corresponding Virtlet pod name. virtletctl virsh [flags] virsh_command -- [virsh_command_args...] Options --node string the name of the target node","title":"virtletctl virsh"},{"location":"reference/virtletctl/#virtletctl-vnc","text":"Provide access to the VNC console of a VM pod Synopsis This command forwards a local port to the VNC port used by the specified VM pod. If no local port number is provided, a random available port is picked instead. The port number is displayed after the forwarding is set up, after which the commands enters an endless loop until it's interrupted with Ctrl-C. virtletctl vnc pod [port] [flags]","title":"virtletctl vnc"},{"location":"reference/virtletctl/#global-options","text":"--alsologtostderr log to standard error as well as files --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use -h, --help help for virtletctl --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string Path to the kubeconfig file to use for CLI requests. --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default value: :0 ) --log-dir string If non-empty, write log files in this directory --logtostderr log to standard error instead of files -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default value: \"0\" ) -s, --server string The address and port of the Kubernetes API server --stderrthreshold severity logs at or above this threshold go to stderr (default value: 2 ) --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level log level for V logs --virtlet-runtime string the name of virtlet runtime used in kubernetes.io/target-runtime annotation (default value: \"virtlet.cloud\" ) --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Global options"},{"location":"reference/vm-pod-spec/","text":"Defining a VM Pod The basic idea of a VM pod is that it's a plain Kubernetes pod definition with the following conditions satisfied: It has kubernetes.io/target-runtime: virtlet.cloud annotation so it can be recognized by CRI Proxy . The pod has exactly one container. The container's image has virtlet.cloud/ prefix followed by the image name which is recognized according to Virtlet's image handling rules and used to download the QCOW2 image for the VM. If you have Virtlet running only on some of the nodes in the cluster, you also need to specify either nodeSelector or nodeAffinity for the pod to have it land on a node with Virtlet. If a VM pod lands on a node which doesn't have Virtlet or where Virtlet and CRI Proxy aren't configured properly, you can see the following messages in kubectl describe output for the VM pod (the message can be printed as a single line): Warning Failed 5s (x2 over 17s) kubelet, kubemaster Failed to pull image virtlet.cloud/cirros : rpc error: code = Unknown desc = Error response from daemon: Get https://virtlet.cloud/v2/: dial tcp 50.63.202.10:443: connect: connection refused This means that kubelet is trying to pull the VM image from a Docker registry. It's also possible to construct higher-level Kubernetes objects such as Deployment, StatefulSet or DaemonSet out of VM pods, in which case the template section of the object must follow the above rules for VM pods. Below is an example of a Virtlet pod. The comments describe the particular parts of the pod spec. The following sections will give more details on each part of the spec. # Standard k8s pod header apiVersion: v1 kind: Pod metadata: # the name of the pod name: cirros-vm # See 'Annotations recognized by Virtlet' below annotations: # This tells CRI Proxy that this pod belongs to Virtlet runtime kubernetes.io/target-runtime: virtlet.cloud # An optional annotation specifying the count of virtual CPUs. # Defaults to 1 . VirtletVCPUCount: 1 # CirrOS doesn't load nocloud data from SCSI CD-ROM for some reason VirtletDiskDriver: virtio # inject ssh keys via cloud-init VirtletSSHKeys: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost # set root volume size VirtletRootVolumeSize: 1Gi spec: # This nodeAffinity specification tells Kubernetes to run this # pod only on the nodes that have extraRuntime=virtlet label. # This label is used by Virtlet DaemonSet to select nodes # that must have Virtlet runtime affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet containers: - name: cirros-vm # This specifies the image to use. # virtlet.cloud/ prefix is used by CRI proxy, the remaining part # of the image name is prepended with https:// and used to download the image image: virtlet.cloud/cirros imagePullPolicy: IfNotPresent # tty and stdin required for `kubectl attach -t` to work tty: true stdin: true resources: limits: # This memory limit is applied to the libvirt domain definition memory: 160Mi Annotations recognized by Virtlet The annotations can be specified under annotations key of the metadata part of the pod spec. Note that the values are always strings, but they can be parsed in different way by Virtlet (see Type column). For boolean keys, the string \"true\" (lowercase) is interpreted as a true value. All other values are interpreted as false. Several keys belong to the Cloud-Init settings which are described in detail in the corresponding section . Key Description Value Default kubernetes.io/target-runtime CRI runtime setting for CRI Proxy virtlet.cloud virtlet.cloud VirtletChown9pfsMounts Recursively chown 9pfs mounts boolean \"\" VirtletCloudInitImageType Cloud-Init image type to use \"nocloud\" \"configdrive\" \"\" VirtletCloudInitMetaData The contents of Cloud-Init metadata json / yaml \"\" VirtletCloudInitUserData The contents of Cloud-Init user-data (mergeable) json / yaml \"\" VirtletCloudInitUserDataOverwrite Disable merging of Cloud-Init user-data keys boolean \"\" VirtletCloudInitUserDataScript The contents of Cloud-Init user-data as a script text \"\" VirtletCloudInitUserDataSource Data source for Cloud-Init user-data \"configmap/...\" \"secret/...\" \"\" VirtletCloudInitUserDataSourceEncoding Encoding to use for loading Cloud-Init user-data from a ConfigMap key \"plain\" \"base|4\" VirtletCloudInitUserDataSourceKey ConfigMap key to load Cloud-Init user-data from \"\" VirtletCPUModel CPU model to use \"\" \"host-model\" \"\" VirtletDiskDriver Disk driver to use \"scsi\" \"virtio\" \"scsi\" VirtletFilesFromDataSource Inject files from a ConfigMap or a Secret into the image \"configmap/...\" \"secret/...\" \"\" VirtletLibvirtCPUSetting libvirt CPU model setting yaml \"\" VirtletRootVolumeSize Root volume size quantity \"\" VirtletSSHKeys SSH keys to add to the VM injected via Cloud-Init a list of strings \"\" VirtletSSHKeySource Data source for ssh keys injected via Cloud-Init \"configmap/...\" \"secret/...\" \"\" VirtletVCPUCount The number of vCPUs to assign to the VM pod integer \"1\" CRI Proxy annotation Besides Virtlet annotations, there's kubernetes.io/target-runtime: virtlet.cloud annotation which is handled by CRI Proxy . It's important to specify it as well as virtlet.cloud prefix for CRI Proxy to be able to direct requests to Virtlet. Chowning 9pfs mounts Setting VirtletChown9pfsMounts to true causes 9pfs mounts to chown their volume contents to make it readable and writable by the VM. CPU Model VirtletCPUModel: host-model annotation enables nested virtualization. VirtletLibvirtCPUSetting is an expert-only annotation that sets the CPU options for libvirt. The YAML keys correspond to the XML elements and attributes in the libvirt XML definition , but are capitalized. The value of Model field goes into the Value key. For example: Match: exact Model: Fallback: allow Value: core2duo Disk driver The driver is set using VirtletDiskDriver annotation which may have the value of scsi (the default) or virtio . Some OS images may have problem with the default scsi driver, for example, CirrOS can't handle Cloud-Init data unless virtio driver is used. Injecting files into the image By using VirtletFilesFromDataSource annotation, it's possible to place the contents of a ConfigMap or a Secret on the image before booting the VM. For more information, refer to Injecting files into the VM . vCPU count Virtlet defaults to using just one vCPU per VM. You can change this value by setting VirtletVCPUCount annotation to the desired value, for example, VirtletVCPUCount: \"2\" . Volume handling Virtlet can recognize and handle pod's volumes and container's volumeMounts sections. This can also be used to make the VM use a persistent root filesystem which will survive pod removal and re-creation. For more information on working with volumes, please refer to the Volumes section. Environment variables Virtlet supports passing environment variables to the VM using the standard env settings in the container definition: ... spec: ... containers: - name: cirros-vm ... env: - name: MY_FOO_VAR value: foo - name: MY_FOOBAR_VAR value: foobar Virtlet uses Cloud-Init mechanisms to write the values into /etc/cloud/environment file inside the VM which has the same key=value per line format as /etc/environment and can be either read by an application or sourced by a shell: MY_FOO_VAR=foo MY_FOOBAR_VAR=foobar For this environment mechanism to work, the cloud-init implementation inside the VM must be able to handle write_files inside the Cloud-Init user-data.","title":"Defining a VM Pod"},{"location":"reference/vm-pod-spec/#defining-a-vm-pod","text":"The basic idea of a VM pod is that it's a plain Kubernetes pod definition with the following conditions satisfied: It has kubernetes.io/target-runtime: virtlet.cloud annotation so it can be recognized by CRI Proxy . The pod has exactly one container. The container's image has virtlet.cloud/ prefix followed by the image name which is recognized according to Virtlet's image handling rules and used to download the QCOW2 image for the VM. If you have Virtlet running only on some of the nodes in the cluster, you also need to specify either nodeSelector or nodeAffinity for the pod to have it land on a node with Virtlet. If a VM pod lands on a node which doesn't have Virtlet or where Virtlet and CRI Proxy aren't configured properly, you can see the following messages in kubectl describe output for the VM pod (the message can be printed as a single line): Warning Failed 5s (x2 over 17s) kubelet, kubemaster Failed to pull image virtlet.cloud/cirros : rpc error: code = Unknown desc = Error response from daemon: Get https://virtlet.cloud/v2/: dial tcp 50.63.202.10:443: connect: connection refused This means that kubelet is trying to pull the VM image from a Docker registry. It's also possible to construct higher-level Kubernetes objects such as Deployment, StatefulSet or DaemonSet out of VM pods, in which case the template section of the object must follow the above rules for VM pods. Below is an example of a Virtlet pod. The comments describe the particular parts of the pod spec. The following sections will give more details on each part of the spec. # Standard k8s pod header apiVersion: v1 kind: Pod metadata: # the name of the pod name: cirros-vm # See 'Annotations recognized by Virtlet' below annotations: # This tells CRI Proxy that this pod belongs to Virtlet runtime kubernetes.io/target-runtime: virtlet.cloud # An optional annotation specifying the count of virtual CPUs. # Defaults to 1 . VirtletVCPUCount: 1 # CirrOS doesn't load nocloud data from SCSI CD-ROM for some reason VirtletDiskDriver: virtio # inject ssh keys via cloud-init VirtletSSHKeys: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost # set root volume size VirtletRootVolumeSize: 1Gi spec: # This nodeAffinity specification tells Kubernetes to run this # pod only on the nodes that have extraRuntime=virtlet label. # This label is used by Virtlet DaemonSet to select nodes # that must have Virtlet runtime affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet containers: - name: cirros-vm # This specifies the image to use. # virtlet.cloud/ prefix is used by CRI proxy, the remaining part # of the image name is prepended with https:// and used to download the image image: virtlet.cloud/cirros imagePullPolicy: IfNotPresent # tty and stdin required for `kubectl attach -t` to work tty: true stdin: true resources: limits: # This memory limit is applied to the libvirt domain definition memory: 160Mi","title":"Defining a VM Pod"},{"location":"reference/vm-pod-spec/#annotations-recognized-by-virtlet","text":"The annotations can be specified under annotations key of the metadata part of the pod spec. Note that the values are always strings, but they can be parsed in different way by Virtlet (see Type column). For boolean keys, the string \"true\" (lowercase) is interpreted as a true value. All other values are interpreted as false. Several keys belong to the Cloud-Init settings which are described in detail in the corresponding section . Key Description Value Default kubernetes.io/target-runtime CRI runtime setting for CRI Proxy virtlet.cloud virtlet.cloud VirtletChown9pfsMounts Recursively chown 9pfs mounts boolean \"\" VirtletCloudInitImageType Cloud-Init image type to use \"nocloud\" \"configdrive\" \"\" VirtletCloudInitMetaData The contents of Cloud-Init metadata json / yaml \"\" VirtletCloudInitUserData The contents of Cloud-Init user-data (mergeable) json / yaml \"\" VirtletCloudInitUserDataOverwrite Disable merging of Cloud-Init user-data keys boolean \"\" VirtletCloudInitUserDataScript The contents of Cloud-Init user-data as a script text \"\" VirtletCloudInitUserDataSource Data source for Cloud-Init user-data \"configmap/...\" \"secret/...\" \"\" VirtletCloudInitUserDataSourceEncoding Encoding to use for loading Cloud-Init user-data from a ConfigMap key \"plain\" \"base|4\" VirtletCloudInitUserDataSourceKey ConfigMap key to load Cloud-Init user-data from \"\" VirtletCPUModel CPU model to use \"\" \"host-model\" \"\" VirtletDiskDriver Disk driver to use \"scsi\" \"virtio\" \"scsi\" VirtletFilesFromDataSource Inject files from a ConfigMap or a Secret into the image \"configmap/...\" \"secret/...\" \"\" VirtletLibvirtCPUSetting libvirt CPU model setting yaml \"\" VirtletRootVolumeSize Root volume size quantity \"\" VirtletSSHKeys SSH keys to add to the VM injected via Cloud-Init a list of strings \"\" VirtletSSHKeySource Data source for ssh keys injected via Cloud-Init \"configmap/...\" \"secret/...\" \"\" VirtletVCPUCount The number of vCPUs to assign to the VM pod integer \"1\"","title":"Annotations recognized by Virtlet"},{"location":"reference/vm-pod-spec/#cri-proxy-annotation","text":"Besides Virtlet annotations, there's kubernetes.io/target-runtime: virtlet.cloud annotation which is handled by CRI Proxy . It's important to specify it as well as virtlet.cloud prefix for CRI Proxy to be able to direct requests to Virtlet.","title":"CRI Proxy annotation"},{"location":"reference/vm-pod-spec/#chowning-9pfs-mounts","text":"Setting VirtletChown9pfsMounts to true causes 9pfs mounts to chown their volume contents to make it readable and writable by the VM.","title":"Chowning 9pfs mounts"},{"location":"reference/vm-pod-spec/#cpu-model","text":"VirtletCPUModel: host-model annotation enables nested virtualization. VirtletLibvirtCPUSetting is an expert-only annotation that sets the CPU options for libvirt. The YAML keys correspond to the XML elements and attributes in the libvirt XML definition , but are capitalized. The value of Model field goes into the Value key. For example: Match: exact Model: Fallback: allow Value: core2duo","title":"CPU Model"},{"location":"reference/vm-pod-spec/#disk-driver","text":"The driver is set using VirtletDiskDriver annotation which may have the value of scsi (the default) or virtio . Some OS images may have problem with the default scsi driver, for example, CirrOS can't handle Cloud-Init data unless virtio driver is used.","title":"Disk driver"},{"location":"reference/vm-pod-spec/#injecting-files-into-the-image","text":"By using VirtletFilesFromDataSource annotation, it's possible to place the contents of a ConfigMap or a Secret on the image before booting the VM. For more information, refer to Injecting files into the VM .","title":"Injecting files into the image"},{"location":"reference/vm-pod-spec/#vcpu-count","text":"Virtlet defaults to using just one vCPU per VM. You can change this value by setting VirtletVCPUCount annotation to the desired value, for example, VirtletVCPUCount: \"2\" .","title":"vCPU count"},{"location":"reference/vm-pod-spec/#volume-handling","text":"Virtlet can recognize and handle pod's volumes and container's volumeMounts sections. This can also be used to make the VM use a persistent root filesystem which will survive pod removal and re-creation. For more information on working with volumes, please refer to the Volumes section.","title":"Volume handling"},{"location":"reference/vm-pod-spec/#environment-variables","text":"Virtlet supports passing environment variables to the VM using the standard env settings in the container definition: ... spec: ... containers: - name: cirros-vm ... env: - name: MY_FOO_VAR value: foo - name: MY_FOOBAR_VAR value: foobar Virtlet uses Cloud-Init mechanisms to write the values into /etc/cloud/environment file inside the VM which has the same key=value per line format as /etc/environment and can be either read by an application or sourced by a shell: MY_FOO_VAR=foo MY_FOOBAR_VAR=foobar For this environment mechanism to work, the cloud-init implementation inside the VM must be able to handle write_files inside the Cloud-Init user-data.","title":"Environment variables"},{"location":"reference/vm-pod/","text":"Differences between \"plain\" Kubernetes pods and VM pods Virtlet tries hard to make VM pods appear as plain Kubernetes pods. Still, there are some important differences, including: VM pods can have just one \"container\" You can't use container images for VM pods Some container-specific settings such as network/PID/IPC namespaces, SELinux/AppArmor settings, privileged flag etc. aren't applicable to VM pods Some volume types are handled differently. There are VM-pod-specific settings such as Cloud-Init, persistent rootfs, etc. For more information, see Volumes . kubectl exec and exec readiness/liveness probes aren't supported for VM pods yet There are VM-pod-specific settings such as Cloud-Init , persistent rootfs, etc. Another important point is that when using a persistent root filesystem, the lifetime of the VM is not limited to that of the pod. Despite these differences, there are quite a few Kubernetes features that work for VM pods just as well as for \"plain\" pods, for example, most kubectl commands, pointing services at VM pods, and so on. Besides kubectl , a Virtlet-specific tool called virtletctl can be used to perform Virtlet-specific actions on VM pods and Virtlet processes in the cluster, such as connecting to the VM using SSH and providing VNC connection and dumping cluster diagnostic info. Supported kubectl commands Most kubectl commands' behavior doesn't differ between \"plain\" and VM pods. Exceptions are kubectl exec which isn't supported at the moment, and kubectl attach / kubectl logs which work only if the VM has serial console configured. kubectl attach attaches to the VM serial console. Detaching from the console is done via Ctrl-] . kubectl logs displays the logs for the pod. In case of VM pod, the log is the serial console output. kubectl logs -f , which follows the log as it grows, is supported, too. Using higher-level Kubernetes objects One of the advantages of pod-based approach to running VMs on Kubernetes is an ability to use higher-level Kubernetes objects such as StatefulSets, Deployments, DaemonSets etc. trivially with VMs. Virtlet includes a nested Kubernetes example which makes a nested Kubernetes cluster using a StatefulSet of 3 VM pods, which are initialized using kubeadm . In the k8s-in-k8s example, first we create a headless service that will point domain names k8s-0 , k8s-1 and k8s-2 as resolved by cluster's DNS to the corresponding StatefulSet replicas: apiVersion: v1 kind: Service metadata: name: k8s labels: app: k8s spec: ports: - port: 22 name: ssh clusterIP: None selector: app: inner-k8s Then, we begin defining a StatefulSet with 3 replicas: apiVersion: apps/v1 kind: StatefulSet metadata: name: k8s spec: serviceName: k8s replicas: 3 selector: matchLabels: app: inner-k8s The pods that comprise the StatefulSet are VM pods and thus must have kubernetes.io/target-runtime: virtlet.cloud annotation. Also, we set the root volume size to 4Gi to have some place for Docker images: template: metadata: labels: app: inner-k8s annotations: kubernetes.io/target-runtime: virtlet.cloud # set root volume size VirtletRootVolumeSize: 4Gi Then we add another annotation that will contain Cloud-Init user-data, in which we write some files to adjust Docker settings and add the Kubernetes repository for apt, as well as a provisioning script that will install the necessary packages and then run kubeadm init on k8s-0 and kubeadm join on k8s-1 and k8s-2 . The script makes use of StatefulSet's stable network IDs , so the nodes have names k8s-0 , k8s-1 and k8s-2 that can be resolved by cluster DNS. VirtletCloudInitUserData: | write_files: - path: /etc/systemd/system/docker.service.d/env.conf permissions: 0644 owner: root content: | [Service] Environment= DOCKER_OPTS=--storage-driver=overlay2 - path: /etc/apt/sources.list.d/kubernetes.list permissions: 0644 owner: root content: | deb http://apt.kubernetes.io/ kubernetes-xenial main - path: /usr/local/bin/provision.sh permissions: 0755 owner: root content: | #!/bin/bash set -u -e set -o pipefail curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - apt-get update apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni sed -i 's/--cluster-dns=10\\.96\\.0\\.10/--cluster-dns=10.97.0.10/' \\ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl daemon-reload if [[ $(hostname) =~ -0$ ]]; then # master node kubeadm init --token adcb82.4eae29627dc4c5a6 \\ --pod-network-cidr=10.200.0.0/16 \\ --service-cidr=10.97.0.0/16 \\ --apiserver-cert-extra-sans=127.0.0.1,localhost export KUBECONFIG=/etc/kubernetes/admin.conf export kubever=$(kubectl version | base64 | tr -d '\\n') kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$kubever while ! kubectl get pods -n kube-system -l k8s-app=kube-dns | grep ' 1/1'; do sleep 1 done mkdir -p /root/.kube chmod 700 /root/.kube cp ${KUBECONFIG} /root/.kube/config echo Master setup complete. 2 else # worker node kubeadm join --token adcb82.4eae29627dc4c5a6 \\ --discovery-token-unsafe-skip-ca-verification k8s-0.k8s:6443 echo Node setup complete. 2 fi We then add an ssh public key (corresponding to examples/vmkey ) for the user root : users: - name: root # VirtletSSHKeys only affects 'ubuntu' user for this image, but we want root access ssh-authorized-keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost and make Cloud-Init run the provisioning script when the VM boots: runcmd: - /usr/local/bin/provision.sh After that, we get to the pod spec, in which we limit the possibilities of running the pod to the nodes with extraRuntime=virtlet label where Virtlet daemon pods are placed and use Ubuntu 16.04 image: spec: nodeSelector: extraRuntime: virtlet containers: - name: ubuntu-vm image: virtlet.cloud/cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img imagePullPolicy: IfNotPresent # tty and stdin required for `kubectl attach -t` to work tty: true stdin: true We then specify a readiness probe which will mark each pod as Ready once its ssh port becomes active: readinessProbe: tcpSocket: port: 22 initialDelaySeconds: 5 We could use a more sophisticated check, e.g. by making sure that apiserver is accessible, but for the purpose of the example we use this trivial scheme to keep things simple. Note that kubeadm join on k8s-1 and k8s-2 will keep retrying till kubeadm init on k8s-0 completes its task. In order to test the example, we start it and wait till k8s-0 , k8s-1 and k8s-2 pods appear: $ kubectl apply -f examples/k8s.yaml $ kubectl get pods -w Then we can view the logs on each of the node to see the progress of Kubernetes setup, e.g. $ kubectl logs -f k8s-0 ... [ 226.115652] cloud-init[1513]: Master setup complete. ... After the setup is complete, we can use virtletctl to ssh into the master VM and check the cluster: $ virtletctl ssh root@k8s-0 -- -i examples/vmkey Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.4.0-138-generic x86_64) ... root@k8s-0:~# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-576cbf47c7-8dnh6 1/1 Running 0 69m kube-system coredns-576cbf47c7-q622f 1/1 Running 0 69m kube-system etcd-k8s-0 1/1 Running 0 69m kube-system kube-apiserver-k8s-0 1/1 Running 0 68m kube-system kube-controller-manager-k8s-0 1/1 Running 0 68m kube-system kube-proxy-4dgfx 1/1 Running 0 69m kube-system kube-proxy-jmw6c 1/1 Running 0 69m kube-system kube-proxy-qwbw7 1/1 Running 0 69m kube-system kube-scheduler-k8s-0 1/1 Running 0 68m kube-system weave-net-88jv4 2/2 Running 1 69m kube-system weave-net-kz698 2/2 Running 0 69m kube-system weave-net-rbnmf 2/2 Running 1 69m root@k8s-0:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-0 Ready master 69m v1.12.2 k8s-1 Ready none 69m v1.12.2 k8s-2 Ready none 69m v1.12.2","title":"Working with VM Pods"},{"location":"reference/vm-pod/#differences-between-plain-kubernetes-pods-and-vm-pods","text":"Virtlet tries hard to make VM pods appear as plain Kubernetes pods. Still, there are some important differences, including: VM pods can have just one \"container\" You can't use container images for VM pods Some container-specific settings such as network/PID/IPC namespaces, SELinux/AppArmor settings, privileged flag etc. aren't applicable to VM pods Some volume types are handled differently. There are VM-pod-specific settings such as Cloud-Init, persistent rootfs, etc. For more information, see Volumes . kubectl exec and exec readiness/liveness probes aren't supported for VM pods yet There are VM-pod-specific settings such as Cloud-Init , persistent rootfs, etc. Another important point is that when using a persistent root filesystem, the lifetime of the VM is not limited to that of the pod. Despite these differences, there are quite a few Kubernetes features that work for VM pods just as well as for \"plain\" pods, for example, most kubectl commands, pointing services at VM pods, and so on. Besides kubectl , a Virtlet-specific tool called virtletctl can be used to perform Virtlet-specific actions on VM pods and Virtlet processes in the cluster, such as connecting to the VM using SSH and providing VNC connection and dumping cluster diagnostic info.","title":"Differences between \"plain\" Kubernetes pods and VM pods"},{"location":"reference/vm-pod/#supported-kubectl-commands","text":"Most kubectl commands' behavior doesn't differ between \"plain\" and VM pods. Exceptions are kubectl exec which isn't supported at the moment, and kubectl attach / kubectl logs which work only if the VM has serial console configured. kubectl attach attaches to the VM serial console. Detaching from the console is done via Ctrl-] . kubectl logs displays the logs for the pod. In case of VM pod, the log is the serial console output. kubectl logs -f , which follows the log as it grows, is supported, too.","title":"Supported kubectl commands"},{"location":"reference/vm-pod/#using-higher-level-kubernetes-objects","text":"One of the advantages of pod-based approach to running VMs on Kubernetes is an ability to use higher-level Kubernetes objects such as StatefulSets, Deployments, DaemonSets etc. trivially with VMs. Virtlet includes a nested Kubernetes example which makes a nested Kubernetes cluster using a StatefulSet of 3 VM pods, which are initialized using kubeadm . In the k8s-in-k8s example, first we create a headless service that will point domain names k8s-0 , k8s-1 and k8s-2 as resolved by cluster's DNS to the corresponding StatefulSet replicas: apiVersion: v1 kind: Service metadata: name: k8s labels: app: k8s spec: ports: - port: 22 name: ssh clusterIP: None selector: app: inner-k8s Then, we begin defining a StatefulSet with 3 replicas: apiVersion: apps/v1 kind: StatefulSet metadata: name: k8s spec: serviceName: k8s replicas: 3 selector: matchLabels: app: inner-k8s The pods that comprise the StatefulSet are VM pods and thus must have kubernetes.io/target-runtime: virtlet.cloud annotation. Also, we set the root volume size to 4Gi to have some place for Docker images: template: metadata: labels: app: inner-k8s annotations: kubernetes.io/target-runtime: virtlet.cloud # set root volume size VirtletRootVolumeSize: 4Gi Then we add another annotation that will contain Cloud-Init user-data, in which we write some files to adjust Docker settings and add the Kubernetes repository for apt, as well as a provisioning script that will install the necessary packages and then run kubeadm init on k8s-0 and kubeadm join on k8s-1 and k8s-2 . The script makes use of StatefulSet's stable network IDs , so the nodes have names k8s-0 , k8s-1 and k8s-2 that can be resolved by cluster DNS. VirtletCloudInitUserData: | write_files: - path: /etc/systemd/system/docker.service.d/env.conf permissions: 0644 owner: root content: | [Service] Environment= DOCKER_OPTS=--storage-driver=overlay2 - path: /etc/apt/sources.list.d/kubernetes.list permissions: 0644 owner: root content: | deb http://apt.kubernetes.io/ kubernetes-xenial main - path: /usr/local/bin/provision.sh permissions: 0755 owner: root content: | #!/bin/bash set -u -e set -o pipefail curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - apt-get update apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni sed -i 's/--cluster-dns=10\\.96\\.0\\.10/--cluster-dns=10.97.0.10/' \\ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl daemon-reload if [[ $(hostname) =~ -0$ ]]; then # master node kubeadm init --token adcb82.4eae29627dc4c5a6 \\ --pod-network-cidr=10.200.0.0/16 \\ --service-cidr=10.97.0.0/16 \\ --apiserver-cert-extra-sans=127.0.0.1,localhost export KUBECONFIG=/etc/kubernetes/admin.conf export kubever=$(kubectl version | base64 | tr -d '\\n') kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$kubever while ! kubectl get pods -n kube-system -l k8s-app=kube-dns | grep ' 1/1'; do sleep 1 done mkdir -p /root/.kube chmod 700 /root/.kube cp ${KUBECONFIG} /root/.kube/config echo Master setup complete. 2 else # worker node kubeadm join --token adcb82.4eae29627dc4c5a6 \\ --discovery-token-unsafe-skip-ca-verification k8s-0.k8s:6443 echo Node setup complete. 2 fi We then add an ssh public key (corresponding to examples/vmkey ) for the user root : users: - name: root # VirtletSSHKeys only affects 'ubuntu' user for this image, but we want root access ssh-authorized-keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCaJEcFDXEK2ZbX0ZLS1EIYFZRbDAcRfuVjpstSc0De8+sV1aiu+dePxdkuDRwqFtCyk6dEZkssjOkBXtri00MECLkir6FcH3kKOJtbJ6vy3uaJc9w1ERo+wyl6SkAh/+JTJkp7QRXj8oylW5E20LsbnA/dIwWzAF51PPwF7A7FtNg9DnwPqMkxFo1Th/buOMKbP5ZA1mmNNtmzbMpMfJATvVyiv3ccsSJKOiyQr6UG+j7sc/7jMVz5Xk34Vd0l8GwcB0334MchHckmqDB142h/NCWTr8oLakDNvkfC1YneAfAO41hDkUbxPtVBG5M/o7P4fxoqiHEX+ZLfRxDtHB53 me@localhost and make Cloud-Init run the provisioning script when the VM boots: runcmd: - /usr/local/bin/provision.sh After that, we get to the pod spec, in which we limit the possibilities of running the pod to the nodes with extraRuntime=virtlet label where Virtlet daemon pods are placed and use Ubuntu 16.04 image: spec: nodeSelector: extraRuntime: virtlet containers: - name: ubuntu-vm image: virtlet.cloud/cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img imagePullPolicy: IfNotPresent # tty and stdin required for `kubectl attach -t` to work tty: true stdin: true We then specify a readiness probe which will mark each pod as Ready once its ssh port becomes active: readinessProbe: tcpSocket: port: 22 initialDelaySeconds: 5 We could use a more sophisticated check, e.g. by making sure that apiserver is accessible, but for the purpose of the example we use this trivial scheme to keep things simple. Note that kubeadm join on k8s-1 and k8s-2 will keep retrying till kubeadm init on k8s-0 completes its task. In order to test the example, we start it and wait till k8s-0 , k8s-1 and k8s-2 pods appear: $ kubectl apply -f examples/k8s.yaml $ kubectl get pods -w Then we can view the logs on each of the node to see the progress of Kubernetes setup, e.g. $ kubectl logs -f k8s-0 ... [ 226.115652] cloud-init[1513]: Master setup complete. ... After the setup is complete, we can use virtletctl to ssh into the master VM and check the cluster: $ virtletctl ssh root@k8s-0 -- -i examples/vmkey Welcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.4.0-138-generic x86_64) ... root@k8s-0:~# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-576cbf47c7-8dnh6 1/1 Running 0 69m kube-system coredns-576cbf47c7-q622f 1/1 Running 0 69m kube-system etcd-k8s-0 1/1 Running 0 69m kube-system kube-apiserver-k8s-0 1/1 Running 0 68m kube-system kube-controller-manager-k8s-0 1/1 Running 0 68m kube-system kube-proxy-4dgfx 1/1 Running 0 69m kube-system kube-proxy-jmw6c 1/1 Running 0 69m kube-system kube-proxy-qwbw7 1/1 Running 0 69m kube-system kube-scheduler-k8s-0 1/1 Running 0 68m kube-system weave-net-88jv4 2/2 Running 1 69m kube-system weave-net-kz698 2/2 Running 0 69m kube-system weave-net-rbnmf 2/2 Running 1 69m root@k8s-0:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-0 Ready master 69m v1.12.2 k8s-1 Ready none 69m v1.12.2 k8s-2 Ready none 69m v1.12.2","title":"Using higher-level Kubernetes objects"},{"location":"reference/volumes/","text":"Using volumes Virtlet can recognize and handle pod's volumes and container's volumeMounts / volumeDevices sections. These can be used to mount Kubernetes volumes into the VM, as well as attaching block volumes to the VM and specifying a persistent root filesystem for a VM. Consuming raw block PVs Virtlet supports consuming Raw Block Volumes in the VMs. In order to do this, you need a PVC with volumeMode: Block (let's say its name is testpvc ) bound to a PV (which needs also to be volumeMode: Block ). You can use this mechanism with both local and non-local PVs. You can then add the following to pod's volumes: volumes: - name: testpvc persistentVolumeClaim: claimName: local-block-pvc and corresponding volumeDevices entry to the container: volumeDevices: - devicePath: /dev/testpvc name: testpvc Virtlet will ensure that /dev/testpvc inside the VM is a symlink pointing to the device that corresponds to the block volume (for more details on this, see Cloud-Init description. You can also mount the block device inside the VM using cloud-init: VirtletCloudInitUserData: | mounts: - [ /dev/testpvc , /mnt ] See also block PV examples . Persistent root filesystem Although initially Virtlet was only supporting \"cattle\" VMs that had their lifespan limited to the one of the pod, it's now possible to have VMs with persistent root filesystem that survives pod removal and re-creation, too. If a persistent block volume is specified for a pod and listed in container's volumeDevices with devicePath of / : volumeDevices: - devicePath: / name: testpvc the corresponding PV will be used as a persistent root filesystem for a pod. The persistent root filesystem is reused as long as the image SHA256 hash doesn't change. Upon the change of SHA256 hash of the VM image, the PV will be overwritten again. Internally, Virtlet uses sector 0 of the block device to store persistent root filesystem metadata, and the block device visible inside the VM will use the sectors starting from sector 1. Overall, the following algorithm is used: 1. The block device is checked for the presence of Virtlet header. 2. If there's no Virtlet header, a new header is written to the sector 0 and the device is overwritten with the contents of the image. 3. If the header contains a future persistent root filesystem metadata version number, an error is logged and container creation fails. 4. If the header contains mismatching image SHA256 hash, a new header is written to the sector 0 and the device is overwritten with the contents of the image. Unless this algorithm fails on step 3, the VM is booted using the block PV starting from sector 1 as it's boot device. IMPORTANT NOTE: in case if persistent root filesystem is used, cloud-init based network setup is disabled for the VM. This is done because some cloud-init implementations only apply cloud-init network configuration once, but the IP address given to the VM may change if the persistent root filesystem is reused by another pod. See also block PV examples . Consuming ConfigMaps and Secrets If a Secret or ConfigMap volume is specified for a Virtlet pod, its contents is written to the filesystem of the VM using write_files Cloud-Init feature which needs to be supported by the VM's Cloud-Init implementation. 9pfs mounts Specifying volumeMounts with volumes that don't refer to either Secrets, ConfigMaps, block PVs or Virtlet-specific flexvolumes causes Virtlet to mount them using QEMU's VirtFS (9pfs). Note that this means that the performance may be suboptimal in some cases. File permissions can also constitute a problem here; you can set VirtletChown9pfsMounts pod annotation to true to make Virtlet change the owner user/group on the directory recursively to one enabling read-write access for the VM. Using FlexVolumes Virtlet uses custom FlexVolume driver ( virtlet/flexvolume_driver ) to specify block devices for the VMs. Flexvolume options must include type field with one of the following values: qcow2 - ephemeral volume raw - raw device. This flexvolume type is deprecated in favor of Kubernetes' local PVs consumed in BlockVolume mode . ceph - Ceph RBD. This flexvolume type is deprecated in favor of Kubernetes' RBD PVs consumed in BlockVolume mode . Ephemeral Local Storage All ephemeral volumes created by request as well as VM root volumes are stored in the local libvirt storage pool \" volumes \" which is located at /var/lib/virtlet/volumes . The libvirt volume is named using the following scheme: domain-uuid - vol-name-specified-in-the-flexvolume . The flexvolume has capacity option which specifies the size of the ephemeral volume and default to 1024 MB. See the following example: apiVersion: v1 kind: Pod metadata: name: test-vm-pod annotations: kubernetes.io/target-runtime: virtlet.cloud spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet containers: - name: test-vm image: download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img volumes: - name: vol1 flexVolume: driver: virtlet/flexvolume_driver options: type: qcow2 capacity: 1024MB - name: vol2 flexVolume: driver: virtlet/flexvolume_driver options: type: qcow2 According to this definition will be created VM-POD with VM with 2 equal volumes, attached, which can be found in \"volumes\" pool under domain-uuid -vol1 and domain-uuid -vol2 . The root volume, which uses the VM's QCOW2 image as its backing file, is exposed as sda device to the guest OS. On a typical Linux system the additional volume disks are assigned to /dev/sdX ( /dev/vdX in case of virtio-blk ) devices in an alphabetical order, so vol1 will be /dev/sdb ( /dev/vdb ) and vol2 will be /dev/sdc ( /dev/vdc ), but please refer to the caveat #3 at the beginning of this document. When a pod is removed, all the volumes related to it are removed too. This includes the root volume and any additional volumes. Root volume size You can set the size of the root volume of a Virtlet VM by using VirtletRootVolumeSize annotation. The specified size must be greater than the QCOW2 volume size, otherwise it will be ignored. Here's an example: metadata: name: my-vm annotations: kubernetes.io/target-runtime: virtlet.cloud VirtletRootVolumeSize: 4Gi This sets the root volume size to 4 GiB unless QCOW2 image size is larger than 4 GiB, in which case the QCOW2 volume size is used. The annotation uses the standard Kubernetes quantity specification format, for more info, see here . Disk drivers Virtlet volumes can use either virtio-blk or virtio-scsi storage backends for the volumes. virtio-scsi is the default, but it can be overridden using VirtletDiskDriver annotation, which can have one of two values: virtio meaning virtio-blk and scsi meaning virtio-scsi (the default). Below is an example of switching a pod to virtio-blk driver: apiVersion: v1 kind: Pod metadata: name: cirros-vm annotations: kubernetes.io/target-runtime: virtlet.cloud VirtletDiskDriver: virtio The values of the VirtletDiskDriver annotation correspond to values of bus attribute of libvirt disk target specification. The selected mechanism is used for the rootfs, nocloud cloud-init CD-ROM and all the flexvolume types that Virtlet supports. Most of the time setting the driver is not necessary, but some OS images may have problem with the default scsi driver, for example, CirrOS can't handle Cloud-Init data unless virtio driver is used. Caveats and limitations The total allowed number of volumes that can be attached to a single VM (including implicit volumes for the boot disk and nocloud cloud-init CD-ROM) is 20 in case of virtio-blk and 26 in case of virtio-scsi driver. The limits can be extended in future. When generating libvirt domain definition, Virtlet constructs disk names as sd + disk-char in case of virtio-scsi and as vd + disk-char in case of virtio-blk , where disk-char is a lowercase latin letter starting with 'a'. The first block device, sda or vda , is used for the boot disk. domain type='qemu' id='2' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0' name de0ae972-4154-4f8f-70ff-48335987b5ce-cirros-vm-rbd /name .... devices emulator /vmwrapper /emulator disk type='file' device='disk' ... target dev='sda' bus='scsi/' ... /disk disk type='file' device='disk' ... target dev='sdb' bus='scsi'/ ... /disk disk type='network' device='disk' ... target dev='sdc' bus='scsi'/ ... /disk ... /devices ... /domain The attached disks are visible by the OS inside VM as hard disk devices /dev/sdb , /dev/sdc and so on ( /dev/vdb , /dev/vdc and so on in case of virtio-blk ). Note that the naming of the devices inside guest OS is usually unpredictable. The use of Virtlet-generated Cloud-Init data is recommended for mounting of the volumes. Virtlet uses udev-provided /dev/disk/by-path/... or, failing that, sysfs information for finding the device inside the virtual machine. Note that both mechanisms are Linux-specific.","title":"Volumes"},{"location":"reference/volumes/#using-volumes","text":"Virtlet can recognize and handle pod's volumes and container's volumeMounts / volumeDevices sections. These can be used to mount Kubernetes volumes into the VM, as well as attaching block volumes to the VM and specifying a persistent root filesystem for a VM.","title":"Using volumes"},{"location":"reference/volumes/#consuming-raw-block-pvs","text":"Virtlet supports consuming Raw Block Volumes in the VMs. In order to do this, you need a PVC with volumeMode: Block (let's say its name is testpvc ) bound to a PV (which needs also to be volumeMode: Block ). You can use this mechanism with both local and non-local PVs. You can then add the following to pod's volumes: volumes: - name: testpvc persistentVolumeClaim: claimName: local-block-pvc and corresponding volumeDevices entry to the container: volumeDevices: - devicePath: /dev/testpvc name: testpvc Virtlet will ensure that /dev/testpvc inside the VM is a symlink pointing to the device that corresponds to the block volume (for more details on this, see Cloud-Init description. You can also mount the block device inside the VM using cloud-init: VirtletCloudInitUserData: | mounts: - [ /dev/testpvc , /mnt ] See also block PV examples .","title":"Consuming raw block PVs"},{"location":"reference/volumes/#persistent-root-filesystem","text":"Although initially Virtlet was only supporting \"cattle\" VMs that had their lifespan limited to the one of the pod, it's now possible to have VMs with persistent root filesystem that survives pod removal and re-creation, too. If a persistent block volume is specified for a pod and listed in container's volumeDevices with devicePath of / : volumeDevices: - devicePath: / name: testpvc the corresponding PV will be used as a persistent root filesystem for a pod. The persistent root filesystem is reused as long as the image SHA256 hash doesn't change. Upon the change of SHA256 hash of the VM image, the PV will be overwritten again. Internally, Virtlet uses sector 0 of the block device to store persistent root filesystem metadata, and the block device visible inside the VM will use the sectors starting from sector 1. Overall, the following algorithm is used: 1. The block device is checked for the presence of Virtlet header. 2. If there's no Virtlet header, a new header is written to the sector 0 and the device is overwritten with the contents of the image. 3. If the header contains a future persistent root filesystem metadata version number, an error is logged and container creation fails. 4. If the header contains mismatching image SHA256 hash, a new header is written to the sector 0 and the device is overwritten with the contents of the image. Unless this algorithm fails on step 3, the VM is booted using the block PV starting from sector 1 as it's boot device. IMPORTANT NOTE: in case if persistent root filesystem is used, cloud-init based network setup is disabled for the VM. This is done because some cloud-init implementations only apply cloud-init network configuration once, but the IP address given to the VM may change if the persistent root filesystem is reused by another pod. See also block PV examples .","title":"Persistent root filesystem"},{"location":"reference/volumes/#consuming-configmaps-and-secrets","text":"If a Secret or ConfigMap volume is specified for a Virtlet pod, its contents is written to the filesystem of the VM using write_files Cloud-Init feature which needs to be supported by the VM's Cloud-Init implementation.","title":"Consuming ConfigMaps and Secrets"},{"location":"reference/volumes/#9pfs-mounts","text":"Specifying volumeMounts with volumes that don't refer to either Secrets, ConfigMaps, block PVs or Virtlet-specific flexvolumes causes Virtlet to mount them using QEMU's VirtFS (9pfs). Note that this means that the performance may be suboptimal in some cases. File permissions can also constitute a problem here; you can set VirtletChown9pfsMounts pod annotation to true to make Virtlet change the owner user/group on the directory recursively to one enabling read-write access for the VM.","title":"9pfs mounts"},{"location":"reference/volumes/#using-flexvolumes","text":"Virtlet uses custom FlexVolume driver ( virtlet/flexvolume_driver ) to specify block devices for the VMs. Flexvolume options must include type field with one of the following values: qcow2 - ephemeral volume raw - raw device. This flexvolume type is deprecated in favor of Kubernetes' local PVs consumed in BlockVolume mode . ceph - Ceph RBD. This flexvolume type is deprecated in favor of Kubernetes' RBD PVs consumed in BlockVolume mode .","title":"Using FlexVolumes"},{"location":"reference/volumes/#ephemeral-local-storage","text":"All ephemeral volumes created by request as well as VM root volumes are stored in the local libvirt storage pool \" volumes \" which is located at /var/lib/virtlet/volumes . The libvirt volume is named using the following scheme: domain-uuid - vol-name-specified-in-the-flexvolume . The flexvolume has capacity option which specifies the size of the ephemeral volume and default to 1024 MB. See the following example: apiVersion: v1 kind: Pod metadata: name: test-vm-pod annotations: kubernetes.io/target-runtime: virtlet.cloud spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: extraRuntime operator: In values: - virtlet containers: - name: test-vm image: download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img volumes: - name: vol1 flexVolume: driver: virtlet/flexvolume_driver options: type: qcow2 capacity: 1024MB - name: vol2 flexVolume: driver: virtlet/flexvolume_driver options: type: qcow2 According to this definition will be created VM-POD with VM with 2 equal volumes, attached, which can be found in \"volumes\" pool under domain-uuid -vol1 and domain-uuid -vol2 . The root volume, which uses the VM's QCOW2 image as its backing file, is exposed as sda device to the guest OS. On a typical Linux system the additional volume disks are assigned to /dev/sdX ( /dev/vdX in case of virtio-blk ) devices in an alphabetical order, so vol1 will be /dev/sdb ( /dev/vdb ) and vol2 will be /dev/sdc ( /dev/vdc ), but please refer to the caveat #3 at the beginning of this document. When a pod is removed, all the volumes related to it are removed too. This includes the root volume and any additional volumes.","title":"Ephemeral Local Storage"},{"location":"reference/volumes/#root-volume-size","text":"You can set the size of the root volume of a Virtlet VM by using VirtletRootVolumeSize annotation. The specified size must be greater than the QCOW2 volume size, otherwise it will be ignored. Here's an example: metadata: name: my-vm annotations: kubernetes.io/target-runtime: virtlet.cloud VirtletRootVolumeSize: 4Gi This sets the root volume size to 4 GiB unless QCOW2 image size is larger than 4 GiB, in which case the QCOW2 volume size is used. The annotation uses the standard Kubernetes quantity specification format, for more info, see here .","title":"Root volume size"},{"location":"reference/volumes/#disk-drivers","text":"Virtlet volumes can use either virtio-blk or virtio-scsi storage backends for the volumes. virtio-scsi is the default, but it can be overridden using VirtletDiskDriver annotation, which can have one of two values: virtio meaning virtio-blk and scsi meaning virtio-scsi (the default). Below is an example of switching a pod to virtio-blk driver: apiVersion: v1 kind: Pod metadata: name: cirros-vm annotations: kubernetes.io/target-runtime: virtlet.cloud VirtletDiskDriver: virtio The values of the VirtletDiskDriver annotation correspond to values of bus attribute of libvirt disk target specification. The selected mechanism is used for the rootfs, nocloud cloud-init CD-ROM and all the flexvolume types that Virtlet supports. Most of the time setting the driver is not necessary, but some OS images may have problem with the default scsi driver, for example, CirrOS can't handle Cloud-Init data unless virtio driver is used.","title":"Disk drivers"},{"location":"reference/volumes/#caveats-and-limitations","text":"The total allowed number of volumes that can be attached to a single VM (including implicit volumes for the boot disk and nocloud cloud-init CD-ROM) is 20 in case of virtio-blk and 26 in case of virtio-scsi driver. The limits can be extended in future. When generating libvirt domain definition, Virtlet constructs disk names as sd + disk-char in case of virtio-scsi and as vd + disk-char in case of virtio-blk , where disk-char is a lowercase latin letter starting with 'a'. The first block device, sda or vda , is used for the boot disk. domain type='qemu' id='2' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0' name de0ae972-4154-4f8f-70ff-48335987b5ce-cirros-vm-rbd /name .... devices emulator /vmwrapper /emulator disk type='file' device='disk' ... target dev='sda' bus='scsi/' ... /disk disk type='file' device='disk' ... target dev='sdb' bus='scsi'/ ... /disk disk type='network' device='disk' ... target dev='sdc' bus='scsi'/ ... /disk ... /devices ... /domain The attached disks are visible by the OS inside VM as hard disk devices /dev/sdb , /dev/sdc and so on ( /dev/vdb , /dev/vdc and so on in case of virtio-blk ). Note that the naming of the devices inside guest OS is usually unpredictable. The use of Virtlet-generated Cloud-Init data is recommended for mounting of the volumes. Virtlet uses udev-provided /dev/disk/by-path/... or, failing that, sysfs information for finding the device inside the virtual machine. Note that both mechanisms are Linux-specific.","title":"Caveats and limitations"},{"location":"user-guide/apparmor/","text":"AppArmor profiles In order to get the Virtlet DaemonSet work in an AppArmor enabled environment follow the next steps: install the profiles located in this directory into the corresponding directory ( /etc/apparmor.d/ if you use Debian or its derivatives) sudo install -m 0644 libvirtd virtlet vms -t /etc/apparmor.d/ apply them by restarting the apparmor service sudo systemctl restart apparmor or by hand, using the following commands sudo apparmor_parser -r /etc/apparmor.d/libvirtd sudo apparmor_parser -r /etc/apparmor.d/virtlet sudo apparmor_parser -r /etc/apparmor.d/vms set the corresponding profiles in the Virtlet DaemonSet: spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/libvirt: localhost/libvirtd container.apparmor.security.beta.kubernetes.io/vms: localhost/vms container.apparmor.security.beta.kubernetes.io/virtlet: localhost/virtlet [re]create the Virtlet DamonSet using standard Kubernetes approach","title":"AppArmor profiles"},{"location":"user-guide/apparmor/#apparmor-profiles","text":"In order to get the Virtlet DaemonSet work in an AppArmor enabled environment follow the next steps: install the profiles located in this directory into the corresponding directory ( /etc/apparmor.d/ if you use Debian or its derivatives) sudo install -m 0644 libvirtd virtlet vms -t /etc/apparmor.d/ apply them by restarting the apparmor service sudo systemctl restart apparmor or by hand, using the following commands sudo apparmor_parser -r /etc/apparmor.d/libvirtd sudo apparmor_parser -r /etc/apparmor.d/virtlet sudo apparmor_parser -r /etc/apparmor.d/vms set the corresponding profiles in the Virtlet DaemonSet: spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/libvirt: localhost/libvirtd container.apparmor.security.beta.kubernetes.io/vms: localhost/vms container.apparmor.security.beta.kubernetes.io/virtlet: localhost/virtlet [re]create the Virtlet DamonSet using standard Kubernetes approach","title":"AppArmor profiles"},{"location":"user-guide/real-cluster/","text":"Installing Virtlet on a real cluster For Virtlet to work, the following prerequisites have to be fulfilled on the nodes which will run them: Node names must be resolvable via DNS configured on the nodes SELinux must be disabled on the nodes (apparmor is currently supported) Virtlet deployment consists of preparing the nodes and then deploying the Virtlet DaemonSet. Installing CRI Proxy Virtlet requires CRI Proxy package to be able to run as DaemonSet on the nodes and support runnings system pods like kube-proxy there. To install CRI Proxy, please follow the steps from its documentation . Repeat it on each node that's going to run Virtlet. Deploying Virtlet DaemonSet Applying apparmor profiles Follow this guide if you deploy the Virtlet DaemonSet on an apparmor-enabled environment. First, you need to apply extraRuntime=virtlet label to each node that will run Virtlet DaemonSet (replace XXXXXX with the node name): kubectl label node XXXXXX extraRuntime=virtlet Then you need to install image translations configmap. You can use the default one: curl https://raw.githubusercontent.com/Mirantis/virtlet/master/deploy/images.yaml images.yaml kubectl create configmap -n kube-system virtlet-image-translations --from-file images.yaml After that, you need to get virtletctl command line tool (replace N.N.N in the command below accordingly): curl -SL -o virtletctl https://github.com/Mirantis/virtlet/releases/download/vN.N.N/virtletctl In case if you're using Mac OS X, you need to use this command instead: curl -SL -o virtletctl https://github.com/Mirantis/virtlet/releases/download/vN.N.N/virtletctl.darwin You can also use virtletctl from Virtlet image, see below. Then you can deploy Virtlet: ./virtletctl gen | kubectl apply -f - If you want to use the latest image, you can use virtletctl from that image: docker run --rm mirantis/virtlet:latest virtletctl gen --tag latest | kubectl apply -f - You can also use other image tag instead of latest , just replace it in both places in the above command. By default it has KVM enabled, but you can configure Virtlet to disable it. In order to do so, create a configmap named virtlet-config in kube-system prior to creating Virtlet DaemonSet that contains key-value pair disable_kvm=y : kubectl create configmap -n kube-system virtlet-config --from-literal=disable_kvm=y After completing this step, you can look at the list of pods to see when Virtlet DaemonSet is ready: kubectl get pods --all-namespaces -o wide -w Testing the installation Checking basic pod startup To test your Virtlet installation, start a sample VM: kubectl create -f https://raw.githubusercontent.com/Mirantis/virtlet/master/examples/cirros-vm.yaml kubectl get pods --all-namespaces -o wide -w And then connect to console: $ kubectl attach -it cirros-vm If you don't see a command prompt, try pressing enter. Press enter and you will see: login as 'cirros' user. default password: 'gosubsgo'. use 'sudo' for root. cirros-vm login: cirros Password: $ Escape character is ^] Verifying ssh access to a VM pod You can also ssh into the VM using virtletctl tool (available as part of each Virtlet release on GitHub starting from Virtlet 1.0). virtletctl ssh cirros@cirros-vm -- -i examples/vmkey Verifying accessing services from a VM pod After connecting to the VM using one of the above methods you can check access from the VM to cluster services. To check DNS resolution of cluster services, use the following command: nslookup kubernetes.default.svc.cluster.local The following command may be used to check service connectivity (note that it'll give you an authentication error): curl -k https://kubernetes.default.svc.cluster.local You can also verify Internet access from the VM: curl -k https://google.com ping -c 1 8.8.8.8 If you have Kubernetes Dashboard installed (it's present in kubeadm-dind-cluster installations for example), you can check dashboard access using this command: curl http://kubernetes-dashboard.kube-system.svc.cluster.local This should display some html from the dashboard's main page. Removing Virtlet In order to remove Virtlet, first you need to delete all the VM pods. You can remove Virtlet DaemonSet with the following command: kubectl delete daemonset -R -n kube-system virtlet After doing this, remove CRI proxy from each node by reverting the changes in Kubelet flags, e.g. by removing /etc/systemd/system/kubelet.service.d/20-virtlet.conf in case of kubeadm scenario described above. After this you need to restart kubelet and remove the CRI Proxy binary ( /usr/local/bin/criproxy ) and its node configuration file ( /etc/criproxy/node.conf ). Customizing Virtlet per-node configuration It's possible to specify per-node configuration options for Virtlet. See this document for more information.","title":"Installing Virtlet on a real cluster"},{"location":"user-guide/real-cluster/#installing-virtlet-on-a-real-cluster","text":"For Virtlet to work, the following prerequisites have to be fulfilled on the nodes which will run them: Node names must be resolvable via DNS configured on the nodes SELinux must be disabled on the nodes (apparmor is currently supported) Virtlet deployment consists of preparing the nodes and then deploying the Virtlet DaemonSet.","title":"Installing Virtlet on a real cluster"},{"location":"user-guide/real-cluster/#installing-cri-proxy","text":"Virtlet requires CRI Proxy package to be able to run as DaemonSet on the nodes and support runnings system pods like kube-proxy there. To install CRI Proxy, please follow the steps from its documentation . Repeat it on each node that's going to run Virtlet.","title":"Installing CRI Proxy"},{"location":"user-guide/real-cluster/#deploying-virtlet-daemonset","text":"","title":"Deploying Virtlet DaemonSet"},{"location":"user-guide/real-cluster/#applying-apparmor-profiles","text":"Follow this guide if you deploy the Virtlet DaemonSet on an apparmor-enabled environment.","title":"Applying apparmor profiles"},{"location":"user-guide/real-cluster/#testing-the-installation","text":"","title":"Testing the installation"},{"location":"user-guide/real-cluster/#checking-basic-pod-startup","text":"To test your Virtlet installation, start a sample VM: kubectl create -f https://raw.githubusercontent.com/Mirantis/virtlet/master/examples/cirros-vm.yaml kubectl get pods --all-namespaces -o wide -w And then connect to console: $ kubectl attach -it cirros-vm If you don't see a command prompt, try pressing enter. Press enter and you will see: login as 'cirros' user. default password: 'gosubsgo'. use 'sudo' for root. cirros-vm login: cirros Password: $ Escape character is ^]","title":"Checking basic pod startup"},{"location":"user-guide/real-cluster/#verifying-ssh-access-to-a-vm-pod","text":"You can also ssh into the VM using virtletctl tool (available as part of each Virtlet release on GitHub starting from Virtlet 1.0). virtletctl ssh cirros@cirros-vm -- -i examples/vmkey","title":"Verifying ssh access to a VM pod"},{"location":"user-guide/real-cluster/#verifying-accessing-services-from-a-vm-pod","text":"After connecting to the VM using one of the above methods you can check access from the VM to cluster services. To check DNS resolution of cluster services, use the following command: nslookup kubernetes.default.svc.cluster.local The following command may be used to check service connectivity (note that it'll give you an authentication error): curl -k https://kubernetes.default.svc.cluster.local You can also verify Internet access from the VM: curl -k https://google.com ping -c 1 8.8.8.8 If you have Kubernetes Dashboard installed (it's present in kubeadm-dind-cluster installations for example), you can check dashboard access using this command: curl http://kubernetes-dashboard.kube-system.svc.cluster.local This should display some html from the dashboard's main page.","title":"Verifying accessing services from a VM pod"},{"location":"user-guide/real-cluster/#removing-virtlet","text":"In order to remove Virtlet, first you need to delete all the VM pods. You can remove Virtlet DaemonSet with the following command: kubectl delete daemonset -R -n kube-system virtlet After doing this, remove CRI proxy from each node by reverting the changes in Kubelet flags, e.g. by removing /etc/systemd/system/kubelet.service.d/20-virtlet.conf in case of kubeadm scenario described above. After this you need to restart kubelet and remove the CRI Proxy binary ( /usr/local/bin/criproxy ) and its node configuration file ( /etc/criproxy/node.conf ).","title":"Removing Virtlet"},{"location":"user-guide/real-cluster/#customizing-virtlet-per-node-configuration","text":"It's possible to specify per-node configuration options for Virtlet. See this document for more information.","title":"Customizing Virtlet per-node configuration"},{"location":"user-guide/virtlet-on-kdc/","text":"Deploying Virtlet as a DaemonSet on kubeadm-dind-cluster The steps described here are performed automatically by demo.sh script. Start kubeadm-dind-cluster with Kubernetes version 1.12 (you're not required to download it to your home directory). The cluster script stores appropriate kubectl version in ~/.kubeadm-dind-cluster . wget -O ~/dind-cluster-v1.12.sh \\ https://github.com/kubernetes-sigs/kubeadm-dind-cluster/releases/download/v0.1.0/dind-cluster-v1.12.sh chmod +x ~/dind-cluster-v1.12.sh ~/dind-cluster-v1.12.sh up export PATH=\"$HOME/.kubeadm-dind-cluster:$PATH\" Label a node to accept Virtlet pod: kubectl label node kube-node-1 extraRuntime=virtlet Make several mounts shared inside the dind node that will run Virtlet: for p in /dind /dev /boot /sys/fs/cgroup; do docker exec kube-node-1 mount --make-shared $p done Add virtlet image translation configmap: kubectl create configmap -n kube-system virtlet-image-translations \\ --from-file images.yaml Install CRI proxy on the node: CRIPROXY_DEB_URL=\"https://github.com/Mirantis/criproxy/releases/download/v0.14.0/criproxy-nodeps_0.14.0_amd64.deb\" docker exec kube-node-1 /bin/bash -c \\ \"curl -sSL '${CRIPROXY_DEB_URL}' /criproxy.deb dpkg -i /criproxy.deb rm /criproxy.deb\" Download virtletctl binary for virtlet release you need (replace N.N.N in the command below accordingly). You can also use virtletctl from Virtlet image, see below. # for Linux curl -SL -o virtletctl \\ https://github.com/Mirantis/virtlet/releases/download/vN.N.N/virtletctl chmod +x virtletctl # for Mac OS X curl -SL -o virtletctl \\ https://github.com/Mirantis/virtlet/releases/download/vN.N.N/virtletctl.darwin chmod +x virtletctl Deploy Virtlet DaemonSet and related objects: ./virtletctl gen | kubectl apply -f - # or using latest Virtlet image: # (you can replace both occurences of 'latest' with an image tag you need) docker run --rm mirantis/virtlet:latest virtletctl gen --tag latest | kubectl apply -f - Wait for Virtlet pod to activate: kubectl get pods -w -n kube-system Go to examples/ directory and follow the instructions from there. Configuring Virtlet Virtlet can be customized through the virtlet-config ConfigMap Kuberenetes object. The following keys in the config map are honored by Virtlet when it's deployed using k8s yaml produced by virtletctl gen : disable_kvm - disables KVM support and forces QEMU instead. Use \"1\" as a value. download_protocol - default image download protocol - either http or https . The default is https. loglevel - integer log level value for the virtlet written as a string (e.g. \"3\", \"2\", \"1\"). calico-subnet - netmask width for the Calico CNI. Default is \"24\". image_regexp_translation - enables regexp syntax for the image name translation rules. disable_logging - disables log streaming from VMs. Use \"1\" to disable. It's also possible to set per-node configuration for Virtlet using CRDs. Removing Virtlet In order to remove Virtlet, first you need to delete all the VM pods. You can remove Virtlet DaemonSet with the following command: kubectl delete daemonset -R -n kube-system virtlet After that you can remove CRI Proxy if you're not going to use the node for Virtlet again by undoing the steps you made to install it (see CRI Proxy documentation ).","title":"Installing Virtlet on kubeadm-dind-cluster"},{"location":"user-guide/virtlet-on-kdc/#deploying-virtlet-as-a-daemonset-on-kubeadm-dind-cluster","text":"The steps described here are performed automatically by demo.sh script. Start kubeadm-dind-cluster with Kubernetes version 1.12 (you're not required to download it to your home directory). The cluster script stores appropriate kubectl version in ~/.kubeadm-dind-cluster . wget -O ~/dind-cluster-v1.12.sh \\ https://github.com/kubernetes-sigs/kubeadm-dind-cluster/releases/download/v0.1.0/dind-cluster-v1.12.sh chmod +x ~/dind-cluster-v1.12.sh ~/dind-cluster-v1.12.sh up export PATH=\"$HOME/.kubeadm-dind-cluster:$PATH\" Label a node to accept Virtlet pod: kubectl label node kube-node-1 extraRuntime=virtlet Make several mounts shared inside the dind node that will run Virtlet: for p in /dind /dev /boot /sys/fs/cgroup; do docker exec kube-node-1 mount --make-shared $p done Add virtlet image translation configmap: kubectl create configmap -n kube-system virtlet-image-translations \\ --from-file images.yaml Install CRI proxy on the node: CRIPROXY_DEB_URL=\"https://github.com/Mirantis/criproxy/releases/download/v0.14.0/criproxy-nodeps_0.14.0_amd64.deb\" docker exec kube-node-1 /bin/bash -c \\ \"curl -sSL '${CRIPROXY_DEB_URL}' /criproxy.deb dpkg -i /criproxy.deb rm /criproxy.deb\" Download virtletctl binary for virtlet release you need (replace N.N.N in the command below accordingly). You can also use virtletctl from Virtlet image, see below. # for Linux curl -SL -o virtletctl \\ https://github.com/Mirantis/virtlet/releases/download/vN.N.N/virtletctl chmod +x virtletctl # for Mac OS X curl -SL -o virtletctl \\ https://github.com/Mirantis/virtlet/releases/download/vN.N.N/virtletctl.darwin chmod +x virtletctl Deploy Virtlet DaemonSet and related objects: ./virtletctl gen | kubectl apply -f - # or using latest Virtlet image: # (you can replace both occurences of 'latest' with an image tag you need) docker run --rm mirantis/virtlet:latest virtletctl gen --tag latest | kubectl apply -f - Wait for Virtlet pod to activate: kubectl get pods -w -n kube-system Go to examples/ directory and follow the instructions from there.","title":"Deploying Virtlet as a DaemonSet on kubeadm-dind-cluster"},{"location":"user-guide/virtlet-on-kdc/#configuring-virtlet","text":"Virtlet can be customized through the virtlet-config ConfigMap Kuberenetes object. The following keys in the config map are honored by Virtlet when it's deployed using k8s yaml produced by virtletctl gen : disable_kvm - disables KVM support and forces QEMU instead. Use \"1\" as a value. download_protocol - default image download protocol - either http or https . The default is https. loglevel - integer log level value for the virtlet written as a string (e.g. \"3\", \"2\", \"1\"). calico-subnet - netmask width for the Calico CNI. Default is \"24\". image_regexp_translation - enables regexp syntax for the image name translation rules. disable_logging - disables log streaming from VMs. Use \"1\" to disable. It's also possible to set per-node configuration for Virtlet using CRDs.","title":"Configuring Virtlet"},{"location":"user-guide/virtlet-on-kdc/#removing-virtlet","text":"In order to remove Virtlet, first you need to delete all the VM pods. You can remove Virtlet DaemonSet with the following command: kubectl delete daemonset -R -n kube-system virtlet After that you can remove CRI Proxy if you're not going to use the node for Virtlet again by undoing the steps you made to install it (see CRI Proxy documentation ).","title":"Removing Virtlet"}]}